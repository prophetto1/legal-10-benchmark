<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Legal-10 Development Notes - LEGAL-10 BENCHMARK</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link href="../docstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Legal-10 Development Notes";
        var mkdocs_page_input_path = "notes.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]--> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> LEGAL-10 BENCHMARK
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">ABOUT LEGAL-10</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="..">Legal-10</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../mission_statement/">Mission Statement</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../standards/">Standards</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../why_legal_10/">Why Legal-10</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../framework/">Evaluation Architecture</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">LEGAL-10 BENCHMARKS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../introduction/">Introduction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_1_research_planning/">Skill 1: Research Planning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_2_strategic_stopping/">Skill 2: Strategic Stopping</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_3_known_authority/">Skill 3: Known Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_4_unknown_authority/">Skill 4: Unknown Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_5_validate_authority/">Skill 5: Validate Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_6_fact_extraction/">Skill 6: Fact Extraction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_7_distinguish_cases/">Skill 7: Distinguish Cases</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_8_synthesize_results/">Skill 8: Synthesize Results</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_9_citation_integrity/">Skill 9: Citation Integrity</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_10_copyright_compliance/">Skill 10: Copyright Compliance</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EXTENSIONS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../extension_multilingual/">Multilingual</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CONTRIBUTE</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../who_we_need/">Who We Need</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../good_first_issues/">Good First Issues</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">GUIDES</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../user_guide/">User Guide</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../developer_guide/">Developer Guide</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">LEGAL-10 BENCHMARK</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" alt="Docs"></a> &raquo;</li>
      <li>Legal-10 Development Notes</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/prophetto1/legal-10-benchmark/blob/main/docs/notes.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="legal-10-development-notes">Legal-10 Development Notes</h1>
<h2 id="mkdocs-documentation-workflow">MkDocs Documentation Workflow</h2>
<p><strong>Correct workflow for docs changes:</strong>
1. Edit source files (<code>docs/</code>, <code>mkdocs.yml</code>, <code>docstrings.css</code>)
2. Restart local server: <code>python -m mkdocs serve</code>
3. Preview at http://127.0.0.1:8000/
4. Commit and push to <code>main</code>
5. GitHub Action builds and deploys to <code>gh-pages</code>
6. Remote site updates at https://prophetto1.github.io/legal-10-benchmark/</p>
<p><strong>Important:</strong> Never edit the <code>site/</code> folder directly - it's auto-generated and gitignored.</p>
<p><strong>Hidden pages:</strong> Pages not listed in <code>mkdocs.yml</code> nav are still accessible via direct URL (e.g., this page at <code>/notes/</code>).</p>
<hr />
<h2 id="dataset-availability-status">Dataset Availability Status</h2>
<h3 id="available-on-huggingface">✓ Available on HuggingFace</h3>
<ul>
<li><strong>CUAD</strong> - <a href="https://huggingface.co/datasets/theatticusproject/cuad-qa">theatticusproject/cuad-qa</a> (S6: Fact Extraction)</li>
<li><strong>CLERC</strong> - <a href="https://huggingface.co/datasets/jhu-clsp/CLERC">jhu-clsp/CLERC</a> (S3, S4, S5: Citation/Authority tasks)</li>
<li><strong>LEXam</strong> - <a href="https://huggingface.co/datasets/LEXam-Benchmark/LEXam">LEXam-Benchmark/LEXam</a> (S8: Synthesize Results)</li>
<li><strong>FairLex</strong> - <a href="https://huggingface.co/datasets/coastalcph/fairlex">coastalcph/fairlex</a> (Extension: Multilingual)</li>
<li><strong>CaseHOLD</strong> - <a href="https://huggingface.co/datasets/casehold/casehold">casehold/casehold</a> (S7: Distinguish Cases) - Already in HELM</li>
</ul>
<h3 id="available-on-github">⚠ Available on GitHub</h3>
<ul>
<li><strong>Dahl</strong> - <a href="https://github.com/reglab/legal_hallucinations">reglab/legal_hallucinations</a> (S9: Citation Integrity)</li>
<li><strong>LegalAgentBench</strong> - <a href="https://github.com/CSHaitao/LegalAgentBench">CSHaitao/LegalAgentBench</a> (S1, S2: Agentic) - Chinese-focused</li>
</ul>
<h3 id="not-publicly-available">❌ Not Publicly Available</h3>
<ul>
<li><strong>SHIELD</strong> - Dataset not public due to copyright concerns (S10: Copyright Compliance)</li>
<li>Paper: <a href="https://arxiv.org/abs/2406.12975">arXiv:2406.12975</a></li>
</ul>
<h2 id="existing-helm-implementations">Existing HELM Implementations</h2>
<h3 id="already-implemented">Already Implemented</h3>
<ul>
<li><strong>CaseHOLD</strong> - <code>src/helm/benchmark/scenarios/casehold_scenario.py</code></li>
<li>~53k questions, 5-choice MC</li>
<li>
<p>Run spec: <code>enterprise_run_specs.py::get_casehold_spec()</code></p>
</li>
<li>
<p><strong>LegalBench</strong> - <code>src/helm/benchmark/scenarios/legalbench_scenario.py</code></p>
</li>
<li>~100 instances per subset (5 subsets)</li>
<li>Run spec: <code>lite_run_specs.py::get_legalbench_spec(subset)</code></li>
</ul>
<h3 id="to-implement-8-datasets">To Implement (8 datasets)</h3>
<ol>
<li>CLERC (S3, S4, S5)</li>
<li>CUAD (S6)</li>
<li>LEXam (S8)</li>
<li>LegalAgentBench (S1, S2)</li>
<li>KeyCite (S5) - unclear if separate or part of CLERC</li>
<li>SHIELD (S10)</li>
<li>Dahl (S9)</li>
<li>FairLex (Extension)</li>
</ol>
<h2 id="implementation-patterns-from-casehold-legalbench">Implementation Patterns (from CaseHOLD &amp; LegalBench)</h2>
<h3 id="dataset-loading">Dataset Loading</h3>
<ul>
<li>Use HuggingFace <code>load_dataset()</code> with <code>cache_dir</code> parameter</li>
<li>Cache to <code>output_path/data/</code></li>
<li>Support train/test splits</li>
</ul>
<h3 id="instance-structure">Instance Structure</h3>
<pre><code class="language-python">Instance(
    input=Input(text=&quot;...&quot;),
    references=[Reference(Output(text=&quot;...&quot;), tags=[&quot;correct&quot; or []])],
    split=&quot;train&quot; or &quot;test&quot;,
    id=f&quot;id{unique_id}&quot;
)
</code></pre>
<h3 id="run-spec-pattern">Run Spec Pattern</h3>
<ul>
<li>Few-shot: 2-5 examples</li>
<li>Metrics: <code>get_exact_match_metric_specs()</code></li>
<li>Adapt methods: <code>ADAPT_MULTIPLE_CHOICE_JOINT</code> or <code>ADAPT_GENERATION</code></li>
</ul>
<h2 id="computational-estimates">Computational Estimates</h2>
<h3 id="per-dataset-based-on-caseholdlegalbench">Per-Dataset (based on CaseHOLD/LegalBench)</h3>
<ul>
<li><strong>CaseHOLD</strong>: ~1200-1850 tokens/instance, 2 few-shot examples</li>
<li><strong>LegalBench</strong>: ~650-1250 tokens/instance, 5 few-shot examples</li>
<li><strong>Estimated runtime</strong>: 30-60 min for 1 model across all 10 skills (~1000-1500 test instances)</li>
</ul>
<h2 id="branding-changes-completed">Branding Changes Completed</h2>
<ul>
<li>[x] README.md updated with Legal-10 mission statement</li>
<li>[x] CITATION.bib updated (author: Jon Chung)</li>
<li>[ ] pyproject.toml (package name, description, author)</li>
<li>[ ] LICENSE (copyright holder)</li>
<li>[ ] Frontend branding</li>
</ul>
<h2 id="next-steps">Next Steps</h2>
<ol>
<li>Decide on test set sampling strategy (all datasets vary in size)</li>
<li>Create <code>legal10_run_specs.py</code> for all 10 skills</li>
<li>Implement missing scenario files</li>
<li>Handle SHIELD alternative (S10)</li>
<li>Handle LegalAgentBench language issue (S1, S2)</li>
</ol>
<h1 id="helm-framework-architectural-assessment-for-legal-10-integration">HELM Framework Architectural Assessment for Legal-10 Integration</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>The HELM framework provides a <strong>clean extension architecture</strong> that allows Legal-10 to integrate completely without touching core framework code. The system uses:</p>
<ul>
<li><strong>Inheritance-based extension</strong> (scenarios, metrics, adapters)</li>
<li><strong>Decorator-based registration</strong> (run specs via <code>@run_spec_function</code>)</li>
<li><strong>Factory pattern instantiation</strong> (scenarios, metrics via class names)</li>
<li><strong>YAML-based configuration</strong> (models, deployments)</li>
</ul>
<hr />
<h2 id="1-constitutional-boundaries-do-not-touch">1. Constitutional Boundaries - DO NOT TOUCH</h2>
<p>These files form the backbone of HELM's execution engine and <strong>must NOT be modified</strong> when integrating Legal-10.</p>
<h3 id="core-framework-files">Core Framework Files</h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>Files</th>
<th>Purpose</th>
<th>Why Not to Touch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Base Classes</strong></td>
<td><code>scenario.py</code><br><code>adapter.py</code><br><code>metric.py</code><br><code>run_spec.py</code></td>
<td>Define contracts all extensions must follow</td>
<td>Frozen dataclasses and abstract methods form the interface contract</td>
</tr>
<tr>
<td><strong>Execution Engine</strong></td>
<td><code>runner.py</code><br><code>executor.py</code><br><code>scenario_state.py</code><br><code>request_state.py</code></td>
<td>Orchestrate benchmark execution pipeline</td>
<td>Changing flow logic breaks all existing benchmarks</td>
</tr>
<tr>
<td><strong>Registration</strong></td>
<td><code>config_registry.py</code><br><code>model_metadata_registry.py</code><br><code>model_deployment_registry.py</code></td>
<td>Discover and register components</td>
<td>Central registry used by entire framework</td>
</tr>
<tr>
<td><strong>Factory/Discovery</strong></td>
<td><code>adapter_factory.py</code><br><code>run_spec_factory.py</code><br><code>object_spec.py</code></td>
<td>Instantiate components from specs</td>
<td>Routing logic for all adapter/scenario creation</td>
</tr>
</tbody>
</table>
<h3 id="specific-files-absolute-paths">Specific Files (Absolute Paths)</h3>
<p><strong>DO NOT MODIFY:</strong></p>
<ul>
<li><code>/src/helm/benchmark/scenarios/scenario.py</code> - Base <code>Scenario</code> class</li>
<li><code>/src/helm/benchmark/run_spec.py</code> - <code>RunSpec</code> dataclass and decorator</li>
<li><code>/src/helm/benchmark/adaptation/adapter_spec.py</code> - <code>AdapterSpec</code> and constants</li>
<li><code>/src/helm/benchmark/adaptation/adapters/adapter.py</code> - Base <code>Adapter</code> class</li>
<li><code>/src/helm/benchmark/metrics/metric.py</code> - Base <code>Metric</code> class</li>
<li><code>/src/helm/benchmark/runner.py</code> - Execution orchestrator</li>
<li><code>/src/helm/benchmark/executor.py</code> - Request executor</li>
<li><code>/src/helm/benchmark/model_metadata_registry.py</code> - Model registry</li>
<li><code>/src/helm/benchmark/model_deployment_registry.py</code> - Deployment registry</li>
<li><code>/src/helm/benchmark/config_registry.py</code> - Configuration loader</li>
</ul>
<p><strong>Key Principle:</strong> Never modify frozen dataclasses, abstract methods, or execution flow logic.</p>
<hr />
<h2 id="2-safe-extension-points-legal-10-implementation">2. Safe Extension Points - Legal-10 Implementation</h2>
<p>These are the designed interfaces for adding Legal-10 content.</p>
<h3 id="what-to-create-not-modify">What to CREATE (not modify)</h3>
<pre><code>src/helm/benchmark/
├── scenarios/
│   ├── legal_10_clerc_scenario.py          ← S3, S4: Known/Unknown Authority
│   ├── legal_10_keycite_scenario.py        ← S5: Validate Authority
│   ├── legal_10_cuad_scenario.py           ← S6: Fact Extraction
│   ├── legal_10_lexam_scenario.py          ← S8: Synthesize Results
│   ├── legal_10_citation_scenario.py       ← S9: Citation Integrity
│   ├── legal_10_shield_scenario.py         ← S10: Copyright Compliance
│   └── legal_10_agentbench_scenario.py     ← S1, S2: Research Planning/Stopping
│
├── run_specs/
│   └── legal_10_run_specs.py               ← All 10 @run_spec_function defs
│
└── metrics/
    └── legal_10_metrics.py                 ← Custom metrics if needed

docs/
├── legal_10_overview.md                    ← Benchmark introduction
├── legal_10_scenarios.md                   ← Scenario details
└── legal_10_setup.md                       ← Setup guide
</code></pre>
<h3 id="a-adding-custom-scenarios">A. Adding Custom Scenarios</h3>
<p><strong>Pattern:</strong> Create Scenario subclass inheriting from <code>Scenario</code> ABC</p>
<p><strong>Location:</strong> Create new files in <code>/src/helm/benchmark/scenarios/</code></p>
<p><strong>Template:</strong></p>
<pre><code class="language-python">from typing import List
from helm.benchmark.scenarios.scenario import (
    Scenario, Instance, Input, Output, Reference,
    CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT
)
from helm.common.general import ensure_directory_exists

class Legal10CLERCScenario(Scenario):
    &quot;&quot;&quot;
    CLERC: Citation Resolution benchmark for Legal-10
    Tests known authority retrieval (S3)
    &quot;&quot;&quot;
    name = &quot;legal_10_clerc&quot;
    description = &quot;CLERC: Known Authority Retrieval&quot;
    tags = [&quot;legal&quot;, &quot;retrieval&quot;, &quot;legal_10&quot;]

    def get_instances(self, output_path: str) -&gt; List[Instance]:
        # Load CLERC dataset
        # Create Instance objects with Input and References
        instances = []

        # Example instance creation
        instance = Instance(
            input=Input(text=&quot;Find 42 U.S.C. § 1983&quot;),
            references=[
                Reference(Output(text=&quot;correct_citation&quot;), tags=[CORRECT_TAG])
            ],
            split=TEST_SPLIT,
            id=&quot;clerc_001&quot;
        )
        instances.append(instance)

        return instances
</code></pre>
<p><strong>Extension Points:</strong>
- Subclass <code>Scenario</code>
- Implement <code>get_instances(output_path)</code> method
- Set <code>name</code>, <code>description</code>, <code>tags</code> class variables
- Optional: Override <code>get_metadata()</code> for custom display</p>
<h3 id="b-adding-custom-run-specs">B. Adding Custom Run Specs</h3>
<p><strong>Pattern:</strong> Create function decorated with <code>@run_spec_function(name)</code></p>
<p><strong>Location:</strong> Create new file <code>/src/helm/benchmark/run_specs/legal_10_run_specs.py</code></p>
<p><strong>Template:</strong></p>
<pre><code class="language-python">from helm.benchmark.run_spec import RunSpec, run_spec_function
from helm.benchmark.scenarios.scenario import ScenarioSpec
from helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION
from helm.benchmark.adaptation.common_adapter_specs import (
    get_generation_adapter_spec,
    get_multiple_choice_adapter_spec
)
from helm.benchmark.metrics.common_metric_specs import (
    get_exact_match_metric_specs,
    get_f1_metric_specs
)

@run_spec_function(&quot;legal_10_clerc&quot;)
def get_legal_10_clerc_spec() -&gt; RunSpec:
    &quot;&quot;&quot;S3: Known Authority - CLERC dataset&quot;&quot;&quot;
    scenario_spec = ScenarioSpec(
        class_name=&quot;helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario&quot;,
        args={}
    )

    adapter_spec = get_generation_adapter_spec(
        instructions=&quot;Retrieve the legal authority for the given citation.&quot;,
        input_noun=&quot;Citation&quot;,
        output_noun=&quot;Authority&quot;,
        max_tokens=512
    )

    metric_specs = get_exact_match_metric_specs()

    return RunSpec(
        name=&quot;legal_10_clerc&quot;,
        scenario_spec=scenario_spec,
        adapter_spec=adapter_spec,
        metric_specs=metric_specs,
        groups=[&quot;legal_10&quot;, &quot;legal_10_rag&quot;]
    )

@run_spec_function(&quot;legal_10_casehold&quot;)
def get_legal_10_casehold_spec() -&gt; RunSpec:
    &quot;&quot;&quot;S7: Distinguish Cases - CaseHOLD dataset&quot;&quot;&quot;
    scenario_spec = ScenarioSpec(
        class_name=&quot;helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario&quot;,
        args={}
    )

    adapter_spec = get_multiple_choice_adapter_spec(
        method=&quot;ADAPT_MULTIPLE_CHOICE_JOINT&quot;,
        instructions=&quot;Which holding is most relevant?&quot;,
        input_noun=&quot;Passage&quot;,
        output_noun=&quot;Answer&quot;,
        max_train_instances=2
    )

    metric_specs = get_exact_match_metric_specs()

    return RunSpec(
        name=&quot;legal_10_casehold&quot;,
        scenario_spec=scenario_spec,
        adapter_spec=adapter_spec,
        metric_specs=metric_specs,
        groups=[&quot;legal_10&quot;, &quot;legal_10_closed_book&quot;]
    )
</code></pre>
<p><strong>Discovery Mechanism:</strong>
- Decorator <code>@run_spec_function("name")</code> automatically registers the function
- Dynamic discovery via <code>discover_run_spec_functions()</code> scans all <code>run_specs/</code> modules
- Invoked via CLI: <code>helm-run --run-specs legal_10_clerc</code></p>
<h3 id="c-adding-custom-metrics-optional">C. Adding Custom Metrics (Optional)</h3>
<p><strong>Pattern:</strong> Subclass <code>Metric</code> and implement <code>evaluate()</code> method</p>
<p><strong>Location:</strong> Create <code>/src/helm/benchmark/metrics/legal_10_metrics.py</code></p>
<p><strong>Template:</strong></p>
<pre><code class="language-python">from typing import List
from helm.benchmark.metrics.metric import (
    Metric, MetricSpec, MetricResult, PerInstanceStats, Stat
)
from helm.benchmark.metrics.metric_name import MetricName
from helm.benchmark.adaptation.scenario_state import ScenarioState
from helm.benchmark.metrics.metric_service import MetricService

class CitationValidationMetric(Metric):
    &quot;&quot;&quot;Custom metric for citation integrity (S9)&quot;&quot;&quot;

    def evaluate(
        self,
        scenario_state: ScenarioState,
        metric_service: MetricService,
        eval_cache_path: str,
        parallelism: int
    ) -&gt; MetricResult:
        # Custom evaluation logic
        stats = []
        per_instance_stats = []

        for request_state in scenario_state.request_states:
            # Validate citation exists and is correctly formatted
            citation_valid = self._validate_citation(request_state.result.completions[0].text)

            per_instance_stats.append(PerInstanceStats(
                instance_id=request_state.instance.id,
                trial_index=0,
                stats=[Stat(MetricName(&quot;citation_valid&quot;)).add(1 if citation_valid else 0)]
            ))

        # Aggregate stats
        total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1)
        stats.append(Stat(MetricName(&quot;citation_accuracy&quot;)).add(total_valid / len(per_instance_stats)))

        return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats)

    def _validate_citation(self, text: str) -&gt; bool:
        # Implementation for citation validation
        return True  # Placeholder

# Helper function for run specs
def get_legal_10_citation_metric_specs() -&gt; List[MetricSpec]:
    return [
        MetricSpec(
            class_name=&quot;helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric&quot;,
            args={}
        )
    ]
</code></pre>
<p><strong>When to Create Custom Metrics:</strong>
- Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice
- For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping)</p>
<h3 id="d-adapters-usually-not-needed">D. Adapters (Usually NOT Needed)</h3>
<p><strong>Note:</strong> HELM provides standard adapters that handle most use cases:
- <code>GenerationAdapter</code> - For open-ended generation (most Legal-10 tasks)
- <code>MultipleChoiceJointAdapter</code> - For multiple choice (S7: CaseHOLD)
- <code>ChatAdapter</code> - For chat-based models</p>
<p>Only create custom adapters if you need specialized prompt formatting beyond what <code>adapter_spec.instructions</code> provides.</p>
<hr />
<h2 id="3-data-flow-architecture">3. Data Flow Architecture</h2>
<p>Complete flow from input to output (unchanged by Legal-10):</p>
<pre><code>┌─────────────────────────────────────────────────────────────┐
│ Input: RunSpec (name, scenario, adapter, metrics)           │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ SCENARIO CREATION                                            │
│ Runner.run_one() → create_scenario(scenario_spec)           │
│ → Calls Scenario.get_instances(output_path)                │
│ Returns: List[Instance]                                     │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ DATA PREPROCESSING (Optional)                               │
│ DataPreprocessor.preprocess(instances, parallelism)        │
│ Applies data augmentations if specified                     │
│ Returns: List[Instance]                                    │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ ADAPTATION: Instance → Request                              │
│ AdapterFactory.get_adapter(adapter_spec)                    │
│ → Adapter.adapt(instances, parallelism)                     │
│ Converts each Instance to RequestState(s)                   │
│ Returns: ScenarioState (list of RequestState)               │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ EXECUTION: Request → Result                                 │
│ Executor.execute(scenario_state)                            │
│ - Parallel execution (parallelism parameter)                │
│ - Makes API/local calls via client                          │
│ - Populates RequestState.result                             │
│ Returns: ScenarioState (with results filled in)             │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ METRICS EVALUATION                                          │
│ For each MetricSpec:                                        │
│   metric = create_metric(metric_spec)                       │
│   metric.evaluate(scenario_state, metric_service, ...)     │
│ Returns: MetricResult                                       │
│ - aggregated_stats: List[Stat] (global metrics)             │
│ - per_instance_stats: List[PerInstanceStats] (per item)    │
└──────────────────────┬──────────────────────────────────────┘
                       │
                       ▼
┌─────────────────────────────────────────────────────────────┐
│ Output Files (written to benchmark_output/runs/):           │
│ - run_spec.json (RunSpec as JSON)                           │
│ - scenario.json (Scenario instances)                        │
│ - scenario_state.json (RequestState list)                   │
│ - stats.json (aggregated metrics)                           │
│ - per_instance_stats.json (per-item metrics)                │
└─────────────────────────────────────────────────────────────┘
</code></pre>
<p><strong>Key Data Structures:</strong>
1. <strong>Instance:</strong> Single input data point with references
2. <strong>RequestState:</strong> Instance + Request + Result + metadata
3. <strong>ScenarioState:</strong> Collection of RequestStates (immutable)
4. <strong>Stat:</strong> Single metric value with MetricName context
5. <strong>PerInstanceStats:</strong> Stats grouped by (instance_id, trial_index)</p>
<hr />
<h2 id="4-configuration-discovery-systems">4. Configuration &amp; Discovery Systems</h2>
<h3 id="a-run-spec-discovery">A. Run Spec Discovery</h3>
<ul>
<li><strong>Location:</strong> All files in <code>/src/helm/benchmark/run_specs/</code></li>
<li><strong>Mechanism:</strong> Decorator <code>@run_spec_function("name")</code> registers automatically</li>
<li><strong>Discovery:</strong> <code>discover_run_spec_functions()</code> uses <code>pkgutil.iter_modules()</code> to scan package</li>
<li><strong>Registry:</strong> <code>_REGISTERED_RUN_SPEC_FUNCTIONS</code> dict</li>
<li><strong>Access:</strong> <code>get_run_spec_function(name)</code> looks up by name</li>
</ul>
<p><strong>For Legal-10:</strong> Create <code>legal_10_run_specs.py</code> in <code>run_specs/</code> directory</p>
<h3 id="b-model-metadata-discovery">B. Model Metadata Discovery</h3>
<ul>
<li><strong>Built-in:</strong> <code>/src/helm/config/model_metadata.yaml</code></li>
<li><strong>Local Override:</strong> <code>$LOCAL_PATH/model_metadata.yaml</code></li>
<li><strong>Format:</strong> YAML with <code>models</code> list of <code>ModelMetadata</code> objects</li>
<li><strong>Registration:</strong> <code>register_model_metadata(metadata)</code> adds to <code>ALL_MODELS_METADATA</code></li>
<li><strong>Access:</strong> <code>get_model_metadata(name)</code> retrieves by name</li>
</ul>
<h3 id="c-model-deployment-discovery">C. Model Deployment Discovery</h3>
<ul>
<li><strong>Built-in:</strong> <code>/src/helm/config/model_deployments.yaml</code></li>
<li><strong>Local Override:</strong> <code>$LOCAL_PATH/model_deployments.yaml</code></li>
<li><strong>Format:</strong> YAML with <code>model_deployments</code> list + client_spec class_name</li>
<li><strong>Registration:</strong> <code>register_model_deployment(deployment)</code> adds to registry</li>
<li><strong>Access:</strong> <code>get_model_deployment(name)</code> retrieves by name</li>
</ul>
<h3 id="d-scenario-discovery">D. Scenario Discovery</h3>
<ul>
<li><strong>NO central registry</strong> - scenarios found via <code>ScenarioSpec.class_name</code></li>
<li><strong>Pattern:</strong> <code>class_name: "helm.benchmark.scenarios.module.ClassName"</code></li>
<li><strong>Loading:</strong> <code>create_scenario(scenario_spec)</code> → <code>create_object()</code> → import + instantiate</li>
</ul>
<hr />
<h2 id="5-legal-10-specific-implementation-roadmap">5. Legal-10 Specific Implementation Roadmap</h2>
<h3 id="legal-10-skills-mapping">Legal-10 Skills Mapping</h3>
<table>
<thead>
<tr>
<th>Skill</th>
<th>Name</th>
<th>Dataset</th>
<th>Modality</th>
<th>Status</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>S1</strong></td>
<td>Research Planning</td>
<td>LegalAgentBench</td>
<td>AG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S2</strong></td>
<td>Strategic Stopping</td>
<td>LegalAgentBench</td>
<td>AG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S3</strong></td>
<td>Known Authority</td>
<td>CLERC</td>
<td>RAG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S4</strong></td>
<td>Unknown Authority</td>
<td>CLERC (semantic)</td>
<td>RAG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S5</strong></td>
<td>Validate Authority</td>
<td>KeyCite-CLERC</td>
<td>RAG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S6</strong></td>
<td>Fact Extraction</td>
<td>CUAD</td>
<td>RAG</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S7</strong></td>
<td>Distinguish Cases</td>
<td>CaseHOLD</td>
<td>CB</td>
<td>✅ Exists</td>
</tr>
<tr>
<td><strong>S8</strong></td>
<td>Synthesize Results</td>
<td>LEXam</td>
<td>CB</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S9</strong></td>
<td>Citation Integrity</td>
<td>Dahl 10-types</td>
<td>CB</td>
<td>To Implement</td>
</tr>
<tr>
<td><strong>S10</strong></td>
<td>Copyright Compliance</td>
<td>SHIELD</td>
<td>CB</td>
<td>To Implement</td>
</tr>
</tbody>
</table>
<h3 id="implementation-checklist">Implementation Checklist</h3>
<p><strong>Phase 1: Setup</strong>
- [ ] Create <code>/src/helm/benchmark/scenarios/legal_10/</code> subdirectory (optional)
- [ ] Create <code>/src/helm/benchmark/run_specs/legal_10_run_specs.py</code>
- [ ] Create <code>/docs/legal_10_overview.md</code>
- [ ] Update <code>/mkdocs.yml</code> navigation</p>
<p><strong>Phase 2: Closed-Book Scenarios (S7-S10)</strong>
- [ ] S7: Verify existing <code>casehold_scenario.py</code> works
- [ ] S8: Create <code>legal_10_lexam_scenario.py</code>
- [ ] S9: Create <code>legal_10_citation_scenario.py</code>
- [ ] S10: Create <code>legal_10_shield_scenario.py</code></p>
<p><strong>Phase 3: RAG Scenarios (S3-S6)</strong>
- [ ] S3, S4: Create <code>legal_10_clerc_scenario.py</code>
- [ ] S5: Create <code>legal_10_keycite_scenario.py</code>
- [ ] S6: Create <code>legal_10_cuad_scenario.py</code></p>
<p><strong>Phase 4: Agentic Scenarios (S1-S2)</strong>
- [ ] S1, S2: Create <code>legal_10_agentbench_scenario.py</code>
- [ ] Consider custom metrics for research planning quality</p>
<p><strong>Phase 5: Integration</strong>
- [ ] Create all <code>@run_spec_function</code> entries in <code>legal_10_run_specs.py</code>
- [ ] Add Legal-10 group to <code>/src/helm/benchmark/static/schema_legal.yaml</code>
- [ ] Create run entries in <code>/src/helm/benchmark/presentation/run_entries_legal.conf</code>
- [ ] Write unit tests for each scenario</p>
<p><strong>Phase 6: Documentation</strong>
- [ ] Complete <code>/docs/legal_10_setup.md</code>
- [ ] Complete <code>/docs/legal_10_scenarios.md</code>
- [ ] Update skill pages with implementation details</p>
<hr />
<h2 id="6-do-not-touch-vs-safe-to-extend-summary">6. DO NOT TOUCH vs SAFE TO EXTEND Summary</h2>
<h3 id="absolutely-do-not-modify">❌ ABSOLUTELY DO NOT MODIFY</h3>
<ol>
<li><strong>Frozen dataclasses:</strong> <code>RunSpec</code>, <code>AdapterSpec</code>, <code>Instance</code>, <code>Reference</code>, <code>Output</code>, <code>Input</code>, <code>RequestState</code>, <code>ScenarioState</code></li>
<li><strong>Abstract base classes:</strong> <code>Scenario.get_instances()</code>, <code>Adapter.adapt()</code>, <code>Metric.evaluate()</code></li>
<li><strong>Core execution:</strong> <code>Runner.run_one()</code>, <code>Executor.execute()</code></li>
<li><strong>Registries and discovery:</strong> <code>@run_spec_function</code> decorator, <code>discover_run_spec_functions()</code>, config registry</li>
<li><strong>Factory patterns:</strong> <code>AdapterFactory.get_adapter()</code>, <code>create_object()</code>, <code>create_scenario()</code></li>
<li><strong>Core routing:</strong> Model metadata/deployment registries</li>
</ol>
<h3 id="safe-to-extend-create-new-dont-modify-existing">✅ SAFE TO EXTEND (Create new, don't modify existing)</h3>
<ol>
<li><strong>Scenarios:</strong> Create new <code>*_scenario.py</code> files, implement <code>Scenario</code> subclass</li>
<li><strong>Run Specs:</strong> Create new <code>*_run_specs.py</code> files, use <code>@run_spec_function()</code> decorator</li>
<li><strong>Metrics:</strong> Create new <code>*_metrics.py</code> files, subclass <code>Metric</code>, provide <code>MetricSpec</code> helper</li>
<li><strong>Configuration:</strong> Add local YAML files, don't modify built-in config YAML</li>
<li><strong>Documentation:</strong> Add new <code>.md</code> files, update navigation in <code>mkdocs.yml</code></li>
</ol>
<hr />
<h2 id="7-testing-your-integration">7. Testing Your Integration</h2>
<h3 id="verify-run-spec-discovery">Verify Run Spec Discovery</h3>
<pre><code class="language-bash"># List all available run specs (should include legal_10_*)
helm-run --help

# Describe a specific Legal-10 run spec
helm-run --run-specs legal_10_clerc --describe
</code></pre>
<h3 id="run-a-single-scenario">Run a Single Scenario</h3>
<pre><code class="language-bash"># Run Legal-10 CaseHOLD scenario
helm-run \
  --run-specs legal_10_casehold \
  --suite legal_10 \
  --max-eval-instances 10 \
  --output-path benchmark_output/test_run
</code></pre>
<h3 id="run-all-legal-10-scenarios">Run All Legal-10 Scenarios</h3>
<pre><code class="language-bash"># Run complete Legal-10 suite
helm-run \
  --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \
  --suite legal_10 \
  --models-to-run anthropic/claude-3-sonnet-20240229 \
  --output-path benchmark_output/legal_10_full
</code></pre>
<hr />
<h2 id="8-integration-best-practices">8. Integration Best Practices</h2>
<h3 id="naming-conventions">Naming Conventions</h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Convention</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scenario file</td>
<td><code>legal_10_&lt;dataset&gt;_scenario.py</code></td>
<td><code>legal_10_clerc_scenario.py</code></td>
</tr>
<tr>
<td>Scenario class</td>
<td><code>Legal10&lt;Name&gt;Scenario</code></td>
<td><code>Legal10CLERCScenario</code></td>
</tr>
<tr>
<td>Run spec function</td>
<td><code>get_legal_10_&lt;name&gt;_spec()</code></td>
<td><code>get_legal_10_clerc_spec()</code></td>
</tr>
<tr>
<td>Run spec name</td>
<td><code>legal_10_&lt;name&gt;</code></td>
<td><code>"legal_10_clerc"</code></td>
</tr>
<tr>
<td>Groups</td>
<td><code>["legal_10", "legal_10_&lt;modality&gt;"]</code></td>
<td><code>["legal_10", "legal_10_rag"]</code></td>
</tr>
<tr>
<td>Tags</td>
<td>Lowercase, hyphenated</td>
<td><code>["legal", "retrieval"]</code></td>
</tr>
</tbody>
</table>
<h3 id="import-patterns">Import Patterns</h3>
<p><strong>Standard imports for scenarios:</strong></p>
<pre><code class="language-python">from typing import List
from helm.benchmark.scenarios.scenario import (
    Scenario, Instance, Input, Output, Reference,
    TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG
)
from helm.common.general import ensure_directory_exists
</code></pre>
<p><strong>Standard imports for run specs:</strong></p>
<pre><code class="language-python">from helm.benchmark.run_spec import RunSpec, run_spec_function
from helm.benchmark.scenarios.scenario import ScenarioSpec
from helm.benchmark.adaptation.common_adapter_specs import (
    get_generation_adapter_spec,
    get_multiple_choice_adapter_spec
)
from helm.benchmark.metrics.common_metric_specs import (
    get_exact_match_metric_specs,
    get_f1_metric_specs
)
</code></pre>
<h3 id="error-handling">Error Handling</h3>
<ul>
<li>Scenarios should raise <code>ValueError</code> for invalid configuration</li>
<li>Use <code>hlog()</code> from <code>helm.common.hierarchical_logger</code> for logging</li>
<li>Let exceptions propagate - HELM's runner handles them gracefully</li>
</ul>
<h3 id="performance">Performance</h3>
<ul>
<li>Use <code>ensure_directory_exists()</code> before file operations</li>
<li>Cache downloaded datasets in <code>output_path</code></li>
<li>Use <code>parallelism</code> parameter for concurrent processing</li>
<li>Avoid loading entire datasets into memory if possible</li>
</ul>
<hr />
<h2 id="conclusion">Conclusion</h2>
<p>HELM's architecture provides clear separation between:
- <strong>Framework code</strong> (do not touch)
- <strong>Extension points</strong> (safe to add Legal-10 content)</p>
<p>By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are:</p>
<ul>
<li><strong>Modular:</strong> Each skill is an independent scenario</li>
<li><strong>Discoverable:</strong> Run specs auto-register via decorator</li>
<li><strong>Maintainable:</strong> Clear separation from core framework</li>
<li><strong>Upgradeable:</strong> HELM updates won't break Legal-10 code</li>
</ul>
<p>The key is to <strong>extend, never modify</strong> - inherit from base classes, use decorators, and create new files rather than changing existing ones.</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/prophetto1/legal-10-benchmark/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
  </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme_extra.js" defer></script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
