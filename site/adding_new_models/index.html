<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Adding New Models - Legal-10 Benchmark</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link href="../docstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Adding New Models";
        var mkdocs_page_input_path = "adding_new_models.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]--> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Legal-10 Benchmark
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Home</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="..">Legal-10 Benchmark</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../notes/">Legal-10 Development Notes</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">SKILLSETS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_1_research_planning/">Skill 1: Research Planning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_2_strategic_stopping/">Skill 2: Strategic Stopping</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_3_known_authority/">Skill 3: Known Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_4_unknown_authority/">Skill 4: Unknown Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_5_validate_authority/">Skill 5: Validate Authority</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_6_fact_extraction/">Skill 6: Fact Extraction</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_7_distinguish_cases/">Skill 7: Distinguish Cases</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_8_synthesize_results/">Skill 8: Synthesize Results</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_9_citation_integrity/">Skill 9: Citation Integrity</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../skill_10_copyright_compliance/">Skill 10: Copyright Compliance</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EXTENSIONS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../extension_multilingual/">Multilingual</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">User Guide</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../installation/">Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../quick_start/">Quick Start</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tutorial/">Tutorial</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run_entries_configuration_files/">Run Entries Configuration Files</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../run_entries/">Run Entries</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../credentials/">Credentials</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../importing_custom_modules/">Importing Custom Modules</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">Adding New Models</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adding_new_scenarios/">Adding New Scenarios</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../benchmark/">Advanced Benchmarking Guide</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Developer Guide</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../developer_setup/">Developer Setup</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../architecture_assessment/">HELM Framework Architectural Assessment for Legal-10 Integration</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../developer_adding_new_models/">Adding New Clients</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../editing_documentation/">Editing Documentation</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Legal-10 Benchmark</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">User Guide</li>
      <li class="breadcrumb-item active">Adding New Models</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/prophetto1/legal-10-benchmark/blob/main/docs/adding_new_models.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="adding-new-models">Adding New Models</h1>
<p>HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network</p>
<p>HELM comes with many built-in <code>Client</code> classes (i.e. model API clients) and <code>Tokenizer</code> clients. If there is already an existing <code>Client</code> and <code>Tokenizer</code> class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM.</p>
<p>If you wish to evaluate a model not covered by an existing <code>Client</code> and <code>Tokenizer</code>, you can implement your own <code>Client</code> and <code>Tokenizer</code> subclasses. Instructions for adding custom <code>Client</code> and <code>Tokenizer</code> subclasses will be added to the documentation in the future.</p>
<h2 id="adding-a-model-locally">Adding a Model Locally</h2>
<h3 id="model-metadata">Model Metadata</h3>
<p>Create a local model metadata configuration file if it does not already exist. The file should be a <code>prod_env/model_metadata.yaml</code> by default, or at <code>$LOCAL_PATH/model_metadata.yaml</code> if <code>--local-path</code> is set where <code>$LOCAL_FOLDER</code> is the value of the flag.</p>
<p>This file should contain a YAML-formatted <code>ModelMetadataList</code> object. For an example of this format, refer to <code>model_metadata.yaml</code> in the GitHub repository, or follow the example below:</p>
<pre><code class="language-yaml">models:
  - name: eleutherai/pythia-70m
    display_name: Pythia (70M)
    description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.
    creator_organization_name: EleutherAI
    access: open
    num_parameters: 95600000
    release_date: 2023-02-13
    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]
</code></pre>
<h3 id="model-deployment">Model Deployment</h3>
<p>A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model <code>google/gemma-2-9b-it</code> has the model deployments <code>together/gemma-2-9b-it</code> (remote inference using Together AI's API) and <code>google/gemma-2-9b-it</code> (local inference with Hugging Face).</p>
<p>Create a local model deployments configuration file if it does not already exist. The file should be a <code>prod_env/model_metadata.yaml</code> by default, or at <code>$LOCAL_PATH/model_metadata.yaml</code> if <code>--local-path</code> is set where <code>$LOCAL_FOLDER</code> is the value of the flag.</p>
<p>This file should contain a YAML-formatted <code>ModelDeployments</code> object. For an example of this format, refer to <code>model_deployments.yaml</code> in the GitHub repository, or follow an example below for your preferred model platform.</p>
<p>Note that the model deployment name will frequently differ from the model name. The model deployment name should be <code>$HOST_ORGANIZATON/$MODEL_NAME</code>, while the model name should be <code>$CREATOR_ORGANIZATON/$MODEL_NAME</code>.</p>
<h3 id="hugging-face">Hugging Face</h3>
<p>Example:</p>
<pre><code class="language-yaml">model_deployments:
  - name: huggingface/pythia-70m
    model_name: eleutherai/pythia-70m
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    client_spec:
      class_name: &quot;helm.clients.huggingface_client.HuggingFaceClient&quot;
      args:
        pretrained_model_name_or_path: EleutherAI/pythia-70m
</code></pre>
<p>Note: If <code>pretrained_model_name_or_path</code> is omitted, the model will be loaded from Hugging Face Hub using <code>model_name</code> (<em>not</em> <code>name</code>) by default.</p>
<p>Examples of common arguments within <code>args</code>:</p>
<ul>
<li>Loading from local disk: <code>pretrained_model_name_or_path: /path/to/my/model</code></li>
<li>Revision: <code>revision: my_revision</code></li>
<li>Quantization: <code>load_in_8bit: true</code></li>
<li>Model precision: <code>torch_dtype: torch.float16</code></li>
<li>Model device: <code>device: cpu</code> or <code>device: cuda:0</code></li>
<li>Allow running remote code: <code>trust_remote_code: true</code></li>
<li>Multi-GPU: <code>device_map: auto</code></li>
</ul>
<p>Notes:</p>
<ul>
<li>This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise.</li>
<li>Multi-GPU inference can be enabled by setting <code>device_map: auto</code> in the <code>args</code>.</li>
<li>GPU models loaded by <code>helm-run</code> will remain loaded on the GPU for the lifespan of <code>helm-run</code>.</li>
<li>If evaluating multiple models, it is prudent to evaluate each model with a separate <code>helm-run</code> invocation.</li>
<li>If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running <code>helm-run</code>, run <code>huggingface-cli login</code> in your shell. Refer to <a href="https://huggingface.co/docs/huggingface_hub/en/quick-start#login-command">Hugging Face's documentation</a> for more information.</li>
</ul>
<h3 id="vllm">vLLM</h3>
<pre><code class="language-yaml">model_deployments:
  - name: vllm/pythia-70m
    model_name: eleutherai/pythia-70m
    tokenizer_name: EleutherAI/gpt-neox-20b
    max_sequence_length: 2048
    client_spec:
      class_name: &quot;helm.clients.vllm_client.VLLMClient&quot;
      args:
        base_url:  http://mymodelserver:8000/v1/
</code></pre>
<p>For non-chat models, set <code>class_name</code> in <code>client_spec</code> to <code>helm.clients.vllm_client.VLLMClient</code>. For chat models, set <code>class_name</code> in <code>client_spec</code> to <code>helm.clients.vllm_client.VLLMChatClient</code>.</p>
<p>Set <code>base_url</code> to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with:</p>
<pre><code>python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m
</code></pre>
<h3 id="together-ai">Together AI</h3>
<pre><code class="language-yaml">model_deployments:
  - name: together/gemma-2-9b-it
    model_name: google/gemma-2-9b-it
    tokenizer_name: google/gemma-2-9b
    max_sequence_length: 8191
    client_spec:
      class_name: &quot;helm.clients.together_client.TogetherClient&quot;
      args:
        together_model: google/gemma-2-9b-it
</code></pre>
<p>Notes:</p>
<ul>
<li>You will need to add Together AI credentials to your credentials file e.g. add <code>togetherApiKey: your-api-key</code> to <code>./prod_env/credentials.conf</code>.</li>
<li>If <code>together_model</code> is omitted, the Together model with <code>model_name</code> (<em>not</em> <code>name</code>) will be used by default.</li>
<li>This above model may not be currently available on Together AI. Consult <a href="https://docs.together.ai/docs/inference-models">Together AI's Inference Models documentation</a> for a list of currently available models and corresponding model strings.</li>
</ul>
<h2 id="testing-new-models">Testing New Models</h2>
<p>After you've added your model, you can run your model with <code>helm-run</code> using a run entry such as <code>mmlu:subject=anatomy,model=your-org/your-model</code>. It is also recommended to use the <code>--disable-cache</code> flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example:</p>
<pre><code class="language-sh">helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache

helm-summarize --suite my-suite

helm-server
</code></pre>
<h2 id="adding-new-models-to-helm">Adding New Models to HELM</h2>
<p>If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users.</p>
<p>To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at <code>src/helm/config/</code>, rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at <code>prod_env/</code>, you should move those changes to the corresponding configuration files in <code>src/helm/config/</code> - do not add the model to both <code>src/helm/config/</code> and <code>prod_env/</code> simulatenously.</p>
<p>Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../importing_custom_modules/" class="btn btn-neutral float-left" title="Importing Custom Modules"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../adding_new_scenarios/" class="btn btn-neutral float-right" title="Adding New Scenarios">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/prophetto1/legal-10-benchmark/" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../importing_custom_modules/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../adding_new_scenarios/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
