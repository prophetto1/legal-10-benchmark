{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Legal-10 Benchmark Legal AI is already being used to make decisions that affect liberty, employment, housing, immigration status, family stability, and access to counsel. Today's evaluation ecosystem is fragmented, vendor-permissioned, and often optimized for optics rather than end-user protection. Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG). We open-source everything: datasets, harness, scoring, and operational tooling. The only exception is the actual administered test data, which remains private to prevent contamination and prior knowledge. We support evaluation of both open-weight and closed API models via a reproducible run-bundle protocol. Key Principle Grounded in Professional Standards. Legal skills are not one task. MacCrate Report (ABA, 1992) \u2014 foundational lawyering skills AALL Principles & Standards (2013) \u2014 legal research competency standards Shultz & Zedeck (2011) \u2014 empirically-derived lawyer effectiveness factors Every skill maps to reputable frameworks the legal profession has validated over 30+ years. These standards are used to translate human-lawyer competencies into 10 minimal viable tests across distinct cognitive circuits. Testing conducted using three modalities of Agentic Workflow (AG), RAG, or Closed Book (CB). Each skill is tested in the modality that most closely matches how human lawyers perform that skill in computer-mediated work. The 10 Skills Skill Name Modality Dataset S1 Research Planning AG LegalAgentBench S2 Strategic Stopping AG LegalAgentBench S3 Known Authority RAG CLERC S4 Unknown Authority RAG CLERC (semantic) S5 Validate Authority RAG KeyCite-CLERC S6 Fact Extraction RAG CUAD S7 Distinguish Cases CB CaseHOLD S8 Synthesize Results CB LEXam S9 Citation Integrity CB Dahl 10-types S10 Copyright Compliance CB SHIELD Extensions # Name Modality Dataset 1 Multilingual Reasoning CB FairLex Design Principles Objectivity Fixed spec per version \u2014 changes bump the version No selective omission or quiet withdrawal \u2014 full eval split or no score Transparency Open-source harness \u2014 datasets, scoring, tools Auditable run bundles \u2014 SHA-256 hashes, signed manifests Public append-only log \u2014 all submissions preserved Fairness Comparable baselines \u2014 open and closed models use identical harness Modality-matched testing \u2014 CB vs RAG vs AG reflects real lawyer workflows Gaming-Resistant No selective omission \u2192 must run all 10 skills Skill-level reporting \u2192 can't hide weak skills in aggregate Visible failure modes \u2192 can't hide how it failed Scoreboard Policy Participation is your choice. We will: List all evaluated systems with linked run bundles (\"Evaluated\") Maintain a neutral \"Not Yet Evaluated\" coverage list Coverage disclosure. Each \"Not Yet Evaluated\" entry includes a neutral status tag (e.g., \"no public endpoint,\" \"access restricted,\" \"awaiting community submission\"). This ensures: Fair comparison across GPT-4, Claude, Llama, and other models using identical evaluation protocols. Architecture Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models). We chose this foundation because it embodies the values we believe legal AI evaluation requires: Unified model interface \u2014 open-weight + closed API Standardized adapters \u2014 same prompts for all models Reproducible evaluation \u2014 auditable run bundles Transparent metrics \u2014 open-source scoring Community-Driven Legal-10 is community-governed, not vendor-controlled. We ask for your help in pushing this benchmark initiative. We need a fair and comprehensive benchmark that end users can rely on to better understand the quality implications of a product they are using or considering. Benefits: Co-author credit on papers, governance voting rights, contributor recognition","title":"Legal-10"},{"location":"#legal-10-benchmark","text":"Legal AI is already being used to make decisions that affect liberty, employment, housing, immigration status, family stability, and access to counsel. Today's evaluation ecosystem is fragmented, vendor-permissioned, and often optimized for optics rather than end-user protection. Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG). We open-source everything: datasets, harness, scoring, and operational tooling. The only exception is the actual administered test data, which remains private to prevent contamination and prior knowledge. We support evaluation of both open-weight and closed API models via a reproducible run-bundle protocol.","title":"Legal-10 Benchmark"},{"location":"#key-principle","text":"Grounded in Professional Standards. Legal skills are not one task. MacCrate Report (ABA, 1992) \u2014 foundational lawyering skills AALL Principles & Standards (2013) \u2014 legal research competency standards Shultz & Zedeck (2011) \u2014 empirically-derived lawyer effectiveness factors Every skill maps to reputable frameworks the legal profession has validated over 30+ years. These standards are used to translate human-lawyer competencies into 10 minimal viable tests across distinct cognitive circuits. Testing conducted using three modalities of Agentic Workflow (AG), RAG, or Closed Book (CB). Each skill is tested in the modality that most closely matches how human lawyers perform that skill in computer-mediated work.","title":"Key Principle"},{"location":"#the-10-skills","text":"Skill Name Modality Dataset S1 Research Planning AG LegalAgentBench S2 Strategic Stopping AG LegalAgentBench S3 Known Authority RAG CLERC S4 Unknown Authority RAG CLERC (semantic) S5 Validate Authority RAG KeyCite-CLERC S6 Fact Extraction RAG CUAD S7 Distinguish Cases CB CaseHOLD S8 Synthesize Results CB LEXam S9 Citation Integrity CB Dahl 10-types S10 Copyright Compliance CB SHIELD","title":"The 10 Skills"},{"location":"#extensions","text":"# Name Modality Dataset 1 Multilingual Reasoning CB FairLex","title":"Extensions"},{"location":"#design-principles","text":"","title":"Design Principles"},{"location":"#objectivity","text":"Fixed spec per version \u2014 changes bump the version No selective omission or quiet withdrawal \u2014 full eval split or no score","title":"Objectivity"},{"location":"#transparency","text":"Open-source harness \u2014 datasets, scoring, tools Auditable run bundles \u2014 SHA-256 hashes, signed manifests Public append-only log \u2014 all submissions preserved","title":"Transparency"},{"location":"#fairness","text":"Comparable baselines \u2014 open and closed models use identical harness Modality-matched testing \u2014 CB vs RAG vs AG reflects real lawyer workflows","title":"Fairness"},{"location":"#gaming-resistant","text":"No selective omission \u2192 must run all 10 skills Skill-level reporting \u2192 can't hide weak skills in aggregate Visible failure modes \u2192 can't hide how it failed","title":"Gaming-Resistant"},{"location":"#scoreboard-policy","text":"Participation is your choice. We will: List all evaluated systems with linked run bundles (\"Evaluated\") Maintain a neutral \"Not Yet Evaluated\" coverage list Coverage disclosure. Each \"Not Yet Evaluated\" entry includes a neutral status tag (e.g., \"no public endpoint,\" \"access restricted,\" \"awaiting community submission\"). This ensures: Fair comparison across GPT-4, Claude, Llama, and other models using identical evaluation protocols.","title":"Scoreboard Policy"},{"location":"#architecture","text":"Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models). We chose this foundation because it embodies the values we believe legal AI evaluation requires: Unified model interface \u2014 open-weight + closed API Standardized adapters \u2014 same prompts for all models Reproducible evaluation \u2014 auditable run bundles Transparent metrics \u2014 open-source scoring","title":"Architecture"},{"location":"#community-driven","text":"Legal-10 is community-governed, not vendor-controlled. We ask for your help in pushing this benchmark initiative. We need a fair and comprehensive benchmark that end users can rely on to better understand the quality implications of a product they are using or considering. Benefits: Co-author credit on papers, governance voting rights, contributor recognition","title":"Community-Driven"},{"location":"adding_new_models/","text":"Adding New Models HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network HELM comes with many built-in Client classes (i.e. model API clients) and Tokenizer clients. If there is already an existing Client and Tokenizer class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM. If you wish to evaluate a model not covered by an existing Client and Tokenizer , you can implement your own Client and Tokenizer subclasses. Instructions for adding custom Client and Tokenizer subclasses will be added to the documentation in the future. Adding a Model Locally Model Metadata Create a local model metadata configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelMetadataList object. For an example of this format, refer to model_metadata.yaml in the GitHub repository, or follow the example below: models: - name: eleutherai/pythia-70m display_name: Pythia (70M) description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. creator_organization_name: EleutherAI access: open num_parameters: 95600000 release_date: 2023-02-13 tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG] Model Deployment A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model google/gemma-2-9b-it has the model deployments together/gemma-2-9b-it (remote inference using Together AI's API) and google/gemma-2-9b-it (local inference with Hugging Face). Create a local model deployments configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelDeployments object. For an example of this format, refer to model_deployments.yaml in the GitHub repository, or follow an example below for your preferred model platform. Note that the model deployment name will frequently differ from the model name. The model deployment name should be $HOST_ORGANIZATON/$MODEL_NAME , while the model name should be $CREATOR_ORGANIZATON/$MODEL_NAME . Hugging Face Example: model_deployments: - name: huggingface/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.huggingface_client.HuggingFaceClient\" args: pretrained_model_name_or_path: EleutherAI/pythia-70m Note: If pretrained_model_name_or_path is omitted, the model will be loaded from Hugging Face Hub using model_name ( not name ) by default. Examples of common arguments within args : Loading from local disk: pretrained_model_name_or_path: /path/to/my/model Revision: revision: my_revision Quantization: load_in_8bit: true Model precision: torch_dtype: torch.float16 Model device: device: cpu or device: cuda:0 Allow running remote code: trust_remote_code: true Multi-GPU: device_map: auto Notes: This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise. Multi-GPU inference can be enabled by setting device_map: auto in the args . GPU models loaded by helm-run will remain loaded on the GPU for the lifespan of helm-run . If evaluating multiple models, it is prudent to evaluate each model with a separate helm-run invocation. If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run huggingface-cli login in your shell. Refer to Hugging Face's documentation for more information. vLLM model_deployments: - name: vllm/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.vllm_client.VLLMClient\" args: base_url: http://mymodelserver:8000/v1/ For non-chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMClient . For chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMChatClient . Set base_url to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with: python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m Together AI model_deployments: - name: together/gemma-2-9b-it model_name: google/gemma-2-9b-it tokenizer_name: google/gemma-2-9b max_sequence_length: 8191 client_spec: class_name: \"helm.clients.together_client.TogetherClient\" args: together_model: google/gemma-2-9b-it Notes: You will need to add Together AI credentials to your credentials file e.g. add togetherApiKey: your-api-key to ./prod_env/credentials.conf . If together_model is omitted, the Together model with model_name ( not name ) will be used by default. This above model may not be currently available on Together AI. Consult Together AI's Inference Models documentation for a list of currently available models and corresponding model strings. Testing New Models After you've added your model, you can run your model with helm-run using a run entry such as mmlu:subject=anatomy,model=your-org/your-model . It is also recommended to use the --disable-cache flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example: helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache helm-summarize --suite my-suite helm-server Adding New Models to HELM If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users. To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at src/helm/config/ , rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at prod_env/ , you should move those changes to the corresponding configuration files in src/helm/config/ - do not add the model to both src/helm/config/ and prod_env/ simulatenously. Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.","title":"Adding New Models"},{"location":"adding_new_models/#adding-new-models","text":"HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network HELM comes with many built-in Client classes (i.e. model API clients) and Tokenizer clients. If there is already an existing Client and Tokenizer class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM. If you wish to evaluate a model not covered by an existing Client and Tokenizer , you can implement your own Client and Tokenizer subclasses. Instructions for adding custom Client and Tokenizer subclasses will be added to the documentation in the future.","title":"Adding New Models"},{"location":"adding_new_models/#adding-a-model-locally","text":"","title":"Adding a Model Locally"},{"location":"adding_new_models/#model-metadata","text":"Create a local model metadata configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelMetadataList object. For an example of this format, refer to model_metadata.yaml in the GitHub repository, or follow the example below: models: - name: eleutherai/pythia-70m display_name: Pythia (70M) description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. creator_organization_name: EleutherAI access: open num_parameters: 95600000 release_date: 2023-02-13 tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]","title":"Model Metadata"},{"location":"adding_new_models/#model-deployment","text":"A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model google/gemma-2-9b-it has the model deployments together/gemma-2-9b-it (remote inference using Together AI's API) and google/gemma-2-9b-it (local inference with Hugging Face). Create a local model deployments configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelDeployments object. For an example of this format, refer to model_deployments.yaml in the GitHub repository, or follow an example below for your preferred model platform. Note that the model deployment name will frequently differ from the model name. The model deployment name should be $HOST_ORGANIZATON/$MODEL_NAME , while the model name should be $CREATOR_ORGANIZATON/$MODEL_NAME .","title":"Model Deployment"},{"location":"adding_new_models/#hugging-face","text":"Example: model_deployments: - name: huggingface/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.huggingface_client.HuggingFaceClient\" args: pretrained_model_name_or_path: EleutherAI/pythia-70m Note: If pretrained_model_name_or_path is omitted, the model will be loaded from Hugging Face Hub using model_name ( not name ) by default. Examples of common arguments within args : Loading from local disk: pretrained_model_name_or_path: /path/to/my/model Revision: revision: my_revision Quantization: load_in_8bit: true Model precision: torch_dtype: torch.float16 Model device: device: cpu or device: cuda:0 Allow running remote code: trust_remote_code: true Multi-GPU: device_map: auto Notes: This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise. Multi-GPU inference can be enabled by setting device_map: auto in the args . GPU models loaded by helm-run will remain loaded on the GPU for the lifespan of helm-run . If evaluating multiple models, it is prudent to evaluate each model with a separate helm-run invocation. If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run huggingface-cli login in your shell. Refer to Hugging Face's documentation for more information.","title":"Hugging Face"},{"location":"adding_new_models/#vllm","text":"model_deployments: - name: vllm/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.vllm_client.VLLMClient\" args: base_url: http://mymodelserver:8000/v1/ For non-chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMClient . For chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMChatClient . Set base_url to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with: python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m","title":"vLLM"},{"location":"adding_new_models/#together-ai","text":"model_deployments: - name: together/gemma-2-9b-it model_name: google/gemma-2-9b-it tokenizer_name: google/gemma-2-9b max_sequence_length: 8191 client_spec: class_name: \"helm.clients.together_client.TogetherClient\" args: together_model: google/gemma-2-9b-it Notes: You will need to add Together AI credentials to your credentials file e.g. add togetherApiKey: your-api-key to ./prod_env/credentials.conf . If together_model is omitted, the Together model with model_name ( not name ) will be used by default. This above model may not be currently available on Together AI. Consult Together AI's Inference Models documentation for a list of currently available models and corresponding model strings.","title":"Together AI"},{"location":"adding_new_models/#testing-new-models","text":"After you've added your model, you can run your model with helm-run using a run entry such as mmlu:subject=anatomy,model=your-org/your-model . It is also recommended to use the --disable-cache flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example: helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache helm-summarize --suite my-suite helm-server","title":"Testing New Models"},{"location":"adding_new_models/#adding-new-models-to-helm","text":"If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users. To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at src/helm/config/ , rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at prod_env/ , you should move those changes to the corresponding configuration files in src/helm/config/ - do not add the model to both src/helm/config/ and prod_env/ simulatenously. Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.","title":"Adding New Models to HELM"},{"location":"adding_new_scenarios/","text":"Adding New Scenarios HELM comes with more than a hundred built-in scenarios. However, you may want to run HELM on a scenario that is not built into HELM yet, or you may want to run HELM on scenarios that use your private datasets. Because HELM is a modular framework with a plug-in architecture, you can run evaluations with your custom scenarios on HELM without needing to modify HELM code. There are two steps to adding a custom scenario: adding the custom Scenario subclass, and adding a custom run spec function. The easiest way to implement the custom Scenario subclass and the custom run spec function would be to copy from an appropriate example and then make the appropriate modifications. Determine the task of your scenario, then find the corresponding example Scenario subclass and run spec function from the list below from the simple_scenarios.py and simple_run_specs.py files: Multiple-choice question answering : SimpleMCQAScenario and get_simple_mcqa_run_spec() Short-answer question answering : SimpleShortAnswerQAScenario and get_simple_short_answer_qa_run_spec() Open-ended question answering : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Summarization : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Multi-class classification : SimpleClassificationScenario and get_simple_classification_run_spec() Sentiment analysis : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Toxicity detection : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Multi-label classification : This is currently unsupported by HELM. Named entity recognition : This is currently unsupported by HELM. If your task is not listed, you may still implement your task using custom adapters and metrics, but there is limited official support for doing so. Custom Scenario subclass For this tutorial, we will create a MyScenario class in the the my_scenario module. Make a file called ./my_scenario.py under the my_scenario directory. Create a new class called MyScenario . Find the appropriate example scenario and copy its implementation into MyScenario , making sure to also copy all the required imports. Now we will create a test for the scenario to make sure that it is working correctly. Create a file called ./my_scenario_test.py under the my_scenario directory. Create a test_my_scenario() function in this file. Find the appropriate example scenario test from test_simple_scenarios.py and copy its implementation into test_my_scenario() . You can now run python3 -m pytest test_my_scenario.py to test the example scenario. The test should pass. If you get a ModuleNotFound error, you should set up your PYTHONPATH as explained above, and then try again. Now, modify MyScenario to include the actual logic to load the instances from your dataset. Modify the test accordingly. Use the test to ensure that your implementation is working. Downloading data to local disk Frequently, your Scenario will want to download and cache data onto the local disk, rather than downloading it from the internet every time. The output_path argument passed into the get_instances() method will contain a file path to a scenario-specific download folder that you should download these files to. The folder will be under the scenarios subdirectory under the benchmark_output/ folder (or the path specified by the --output-path flag for helm-run ). You can use the ensure_directory_exists() and ensure_file_downloaded() helper functions to download files, which has the advantage of skipping the download if the file already exists. You can also use set unpack=True in ensure_file_downloaded() to automatically unpack most archive files (e.g. .tar.gz and .zip files). For examples, refer to: gsm_scenario.py - download a JSONL files mmlu_scenario.py - download CSV files narrativeqa_scenario.py - download a zip file containing CSV files Working with Hugging Face datasets Another frequent use case is downloading data from Hugging Face datasets. You can use load_dataset() to do so. It is recommended that you set the cache_dir parameter to a subdirectory within output_path . This ensures hermeticity by ensuring that the data is downloaded into the scenario-specific download folder. For an example, refer to: math_scenario.py legalbench_scenario.py Custom run spec function A run spec function is the entry point to the scenario. A run spec function produces a RunSpec (a configuration for an evaluation run). helm-run will run the run spec function to get the RunSpec , and then it will run the evaluation defined by that RunSpec . HELM will search for modules with names matching these patterns for run spec functions: helm.benchmark.run_specs.*_run_specs helm_*_run_specs (i.e. a root module) For this tutorial, we will create a get_my_run_spec() function in the helm_my_run_specs module. Under the src/helm/benchmark/scenarios/ directory, create a file called helm_my_run_specs.py . Then, create a get_my_run_spec() function in this file and find the appropriate example run spec function from simple_run_specs.py to copy its implementation into get_my_run_spec() . Change the file accordingly to the needs of your scenario. Now run: helm-run --run-entries custom:model=openai/gpt2 --suite custom --max-eval-instances 5 If you get a ValueError: Unknown run spec name error, you should set up your PYTHONPATH as explained above, and then try again. Debugging with models The above run entry uses the openai/gpt2 model, which is a lightweight model that is reasonably fast, even when using only CPU inference without a GPU. However, you might want to avoid waiting for model inference when implementing a scenario in order to speed up your iteration times. To do so, you can use the simple/model1 , which simply echoes the last word in the prompt. Example helm-run command: helm-run --run-entries custom:model=simple/model1 --suite custom --max-eval-instances 5 Note: Both the custom Scenario subclass and the custom run spec function will be added to custom Python modules that have to be importable by Python. The easiest way to do this is to place your custom Python modules under the current working directory and then run export PYTHONPATH=\".:$PYTHONPATH\" in your shell. Refer to the Importing Custom Modules documentation for other ways to do this. Contributing your scenario We welcome scenario contributions to HELM if they fit the following criteria: It is commonly-used or notable benchmark (e.g. it has a published paper). It uses publicly available datasets. It fills a gap in coverage by HELM's existing scenarios. If your scenario fits this criteria, you should move the files to the conventional HELM locations, and open a pull request. Your *_scenario.py file should be placed in src/helm/benchmark/scenarios/ and your *_run_specs.py file should be placed in src/helm/benchmark/scenarios/ . More documentation on the contributor workflow will be added later.","title":"Adding New Scenarios"},{"location":"adding_new_scenarios/#adding-new-scenarios","text":"HELM comes with more than a hundred built-in scenarios. However, you may want to run HELM on a scenario that is not built into HELM yet, or you may want to run HELM on scenarios that use your private datasets. Because HELM is a modular framework with a plug-in architecture, you can run evaluations with your custom scenarios on HELM without needing to modify HELM code. There are two steps to adding a custom scenario: adding the custom Scenario subclass, and adding a custom run spec function. The easiest way to implement the custom Scenario subclass and the custom run spec function would be to copy from an appropriate example and then make the appropriate modifications. Determine the task of your scenario, then find the corresponding example Scenario subclass and run spec function from the list below from the simple_scenarios.py and simple_run_specs.py files: Multiple-choice question answering : SimpleMCQAScenario and get_simple_mcqa_run_spec() Short-answer question answering : SimpleShortAnswerQAScenario and get_simple_short_answer_qa_run_spec() Open-ended question answering : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Summarization : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Multi-class classification : SimpleClassificationScenario and get_simple_classification_run_spec() Sentiment analysis : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Toxicity detection : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Multi-label classification : This is currently unsupported by HELM. Named entity recognition : This is currently unsupported by HELM. If your task is not listed, you may still implement your task using custom adapters and metrics, but there is limited official support for doing so.","title":"Adding New Scenarios"},{"location":"adding_new_scenarios/#custom-scenario-subclass","text":"For this tutorial, we will create a MyScenario class in the the my_scenario module. Make a file called ./my_scenario.py under the my_scenario directory. Create a new class called MyScenario . Find the appropriate example scenario and copy its implementation into MyScenario , making sure to also copy all the required imports. Now we will create a test for the scenario to make sure that it is working correctly. Create a file called ./my_scenario_test.py under the my_scenario directory. Create a test_my_scenario() function in this file. Find the appropriate example scenario test from test_simple_scenarios.py and copy its implementation into test_my_scenario() . You can now run python3 -m pytest test_my_scenario.py to test the example scenario. The test should pass. If you get a ModuleNotFound error, you should set up your PYTHONPATH as explained above, and then try again. Now, modify MyScenario to include the actual logic to load the instances from your dataset. Modify the test accordingly. Use the test to ensure that your implementation is working.","title":"Custom Scenario subclass"},{"location":"adding_new_scenarios/#downloading-data-to-local-disk","text":"Frequently, your Scenario will want to download and cache data onto the local disk, rather than downloading it from the internet every time. The output_path argument passed into the get_instances() method will contain a file path to a scenario-specific download folder that you should download these files to. The folder will be under the scenarios subdirectory under the benchmark_output/ folder (or the path specified by the --output-path flag for helm-run ). You can use the ensure_directory_exists() and ensure_file_downloaded() helper functions to download files, which has the advantage of skipping the download if the file already exists. You can also use set unpack=True in ensure_file_downloaded() to automatically unpack most archive files (e.g. .tar.gz and .zip files). For examples, refer to: gsm_scenario.py - download a JSONL files mmlu_scenario.py - download CSV files narrativeqa_scenario.py - download a zip file containing CSV files","title":"Downloading data to local disk"},{"location":"adding_new_scenarios/#working-with-hugging-face-datasets","text":"Another frequent use case is downloading data from Hugging Face datasets. You can use load_dataset() to do so. It is recommended that you set the cache_dir parameter to a subdirectory within output_path . This ensures hermeticity by ensuring that the data is downloaded into the scenario-specific download folder. For an example, refer to: math_scenario.py legalbench_scenario.py","title":"Working with Hugging Face datasets"},{"location":"adding_new_scenarios/#custom-run-spec-function","text":"A run spec function is the entry point to the scenario. A run spec function produces a RunSpec (a configuration for an evaluation run). helm-run will run the run spec function to get the RunSpec , and then it will run the evaluation defined by that RunSpec . HELM will search for modules with names matching these patterns for run spec functions: helm.benchmark.run_specs.*_run_specs helm_*_run_specs (i.e. a root module) For this tutorial, we will create a get_my_run_spec() function in the helm_my_run_specs module. Under the src/helm/benchmark/scenarios/ directory, create a file called helm_my_run_specs.py . Then, create a get_my_run_spec() function in this file and find the appropriate example run spec function from simple_run_specs.py to copy its implementation into get_my_run_spec() . Change the file accordingly to the needs of your scenario. Now run: helm-run --run-entries custom:model=openai/gpt2 --suite custom --max-eval-instances 5 If you get a ValueError: Unknown run spec name error, you should set up your PYTHONPATH as explained above, and then try again.","title":"Custom run spec function"},{"location":"adding_new_scenarios/#debugging-with-models","text":"The above run entry uses the openai/gpt2 model, which is a lightweight model that is reasonably fast, even when using only CPU inference without a GPU. However, you might want to avoid waiting for model inference when implementing a scenario in order to speed up your iteration times. To do so, you can use the simple/model1 , which simply echoes the last word in the prompt. Example helm-run command: helm-run --run-entries custom:model=simple/model1 --suite custom --max-eval-instances 5 Note: Both the custom Scenario subclass and the custom run spec function will be added to custom Python modules that have to be importable by Python. The easiest way to do this is to place your custom Python modules under the current working directory and then run export PYTHONPATH=\".:$PYTHONPATH\" in your shell. Refer to the Importing Custom Modules documentation for other ways to do this.","title":"Debugging with models"},{"location":"adding_new_scenarios/#contributing-your-scenario","text":"We welcome scenario contributions to HELM if they fit the following criteria: It is commonly-used or notable benchmark (e.g. it has a published paper). It uses publicly available datasets. It fills a gap in coverage by HELM's existing scenarios. If your scenario fits this criteria, you should move the files to the conventional HELM locations, and open a pull request. Your *_scenario.py file should be placed in src/helm/benchmark/scenarios/ and your *_run_specs.py file should be placed in src/helm/benchmark/scenarios/ . More documentation on the contributor workflow will be added later.","title":"Contributing your scenario"},{"location":"adding_new_tokenizers/","text":"Adding New Tokenizers HELM comes with many built-in tokenizers, but in some cases, you may need to add your own custom tokenizer for your custom model. Creating a tokenizer configuration file Create a file called tokenizer_configs.yaml in your local configuration folder (e.g. ./prod_env/tokenizer_configs.yaml ). This file should contain a YAML-formatted TokenizerConfigs object. For an example of this format, refer to the built-in tokenizer_configs.yaml in the GitHub repository, or follow the example below for your preferred model platform. After adding a tokenizer configuration, you can then use the tokenizer in your custom model deployments by setting the specifying the tokenizer name in the tokenizer field of the model deployment. Hugging Face tokenizers To add a Hugging Face tokenizer, follow the format below, setting name to Hugging Face hub model ID. tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" args: pretrained_model_name_or_path: bigscience/bloom end_of_text_token: \"<s>\" prefix_token: \"</s>\" Note that pretrained_model_name_or_path can also be set to a path to load a Hugging Face tokenizer from local disk. If pretrained_model_name_or_path (or args ) is omitted, the model will be loaded from Hugging Face Hub using name as the model ID by default. For example: tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" end_of_text_token: \"<s>\" prefix_token: \"</s>\" To find the values for end_of_text_token and prefix_token , you can run the following Python code snippet below (replacing bigscience/bloom with the Hugging Face Hub model ID). If any special token is unknown, it should be set to the empty string \"\" . from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\") print(f'end_of_text_token: \"{tokenizer.eos_token}\"\\nprefix_token: \"{tokenizer.bos_token}\"') HELM does not auto-infer special token information because some tokenizers on Hugging Face Model Hub may have incorrect or missing special token values. Therefore, you must manually set these values and verify that they are correct.","title":"Adding New Tokenizers"},{"location":"adding_new_tokenizers/#adding-new-tokenizers","text":"HELM comes with many built-in tokenizers, but in some cases, you may need to add your own custom tokenizer for your custom model.","title":"Adding New Tokenizers"},{"location":"adding_new_tokenizers/#creating-a-tokenizer-configuration-file","text":"Create a file called tokenizer_configs.yaml in your local configuration folder (e.g. ./prod_env/tokenizer_configs.yaml ). This file should contain a YAML-formatted TokenizerConfigs object. For an example of this format, refer to the built-in tokenizer_configs.yaml in the GitHub repository, or follow the example below for your preferred model platform. After adding a tokenizer configuration, you can then use the tokenizer in your custom model deployments by setting the specifying the tokenizer name in the tokenizer field of the model deployment.","title":"Creating a tokenizer configuration file"},{"location":"adding_new_tokenizers/#hugging-face-tokenizers","text":"To add a Hugging Face tokenizer, follow the format below, setting name to Hugging Face hub model ID. tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" args: pretrained_model_name_or_path: bigscience/bloom end_of_text_token: \"<s>\" prefix_token: \"</s>\" Note that pretrained_model_name_or_path can also be set to a path to load a Hugging Face tokenizer from local disk. If pretrained_model_name_or_path (or args ) is omitted, the model will be loaded from Hugging Face Hub using name as the model ID by default. For example: tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" end_of_text_token: \"<s>\" prefix_token: \"</s>\" To find the values for end_of_text_token and prefix_token , you can run the following Python code snippet below (replacing bigscience/bloom with the Hugging Face Hub model ID). If any special token is unknown, it should be set to the empty string \"\" . from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\") print(f'end_of_text_token: \"{tokenizer.eos_token}\"\\nprefix_token: \"{tokenizer.bos_token}\"') HELM does not auto-infer special token information because some tokenizers on Hugging Face Model Hub may have incorrect or missing special token values. Therefore, you must manually set these values and verify that they are correct.","title":"Hugging Face tokenizers"},{"location":"agentic_research/","text":"Legal-10 Agentic Benchmark Research Internal development notes \u2014 not listed in navigation Overview This document catalogs research for developing Legal-10 Agentic (S1-S2), an English-language legal agent benchmark adapted from LegalAgentBench methodology. Benchmark Architecture Decision Legal-10 Structure \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 LEGAL-10 BENCHMARK \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 CORE (Required) - 8 Skills \u2502 \u2502 \u251c\u2500\u2500 CB: S7, S8, S9, S10 \u2502 \u2502 \u2514\u2500\u2500 RAG: S3, S4, S5, S6 \u2502 \u2502 \u2192 Standard completion + long context \u2502 \u2502 \u2192 Single connection: CB first \u2192 then RAG \u2502 \u2502 \u2192 No cross-contamination (stateless per instance) \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 AGENTIC (Optional) - 2 Skills \u2502 \u2502 \u2514\u2500\u2500 AG: S1, S2 \u2502 \u2502 \u2192 Requires tool use / function calling \u2502 \u2502 \u2192 Separate benchmark run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Scoring Model Core (8) Agentic (2) Full (10) GPT-4 78% 70% 76% Claude 80% 72% 78% Llama-3 75% N/A 75%* *Models without tool-use get Core score only LegalAgentBench Methodology (Reference) Paper: arxiv.org/abs/2412.17259 (ACL 2025) Framework Components Component LegalAgentBench (Chinese) Legal-10 Agentic (English) Corpora 17 (14 tabular + 3 retrieval) Adapt from HuggingFace Tools 37 specialized tools Build for English legal DBs Tasks 300 annotated Combine from HF datasets Jurisdiction Chinese civil law US/UK common law Task Construction Framework (6 Steps) Planning Tree Construction \u2014 Model tool call dependencies as hierarchical tree Path Selection \u2014 Extract solution paths from 1-hop to 5-hop complexity Entity Selection \u2014 Validate initial entities complete intended paths Question Rewriting \u2014 Transform templates into natural language (GPT-4) Answer Generation \u2014 Programmatically extract via parameterized tool chains Human Verification \u2014 Expert legal professionals validate all Q&A Task Distribution Complexity Count 1-hop 80 2-hop 80 3-hop 60 4-hop 40 5-hop 20 Writing 20 Total 300 Tool Categories Category Count Function Text Retrievers 3 Embedding-based document search Database Tools 28 Query structured legal data Mathematical Tools 5 Arithmetic, sorting, aggregation System Tools 1 \"Finish\" tool for final answers Evaluation Metrics Success Rate \u2014 Keyword matching for final answers Progress Rate \u2014 Intermediate step keywords (key_middle) BERT-Score \u2014 Semantic similarity Key Findings GPT-4o achieved 79.08% success rate (ReAct method) Success declined from 93% (1-hop) to 61% (5-hop) LLMs struggle with: legal terminology, deep article interpretation, tool argument specification HuggingFace English Legal Datasets Corpus Sources Dataset HuggingFace Link Content Agentic Use Caselaw Access Project free-law/Caselaw_Access_Project 6.7M US court decisions Case retrieval corpus COLD Cases harvard-lil/cold-cases Reformatted CourtListener Structured case DB Pile of Law pile-of-law/pile-of-law Court opinions, briefs, statutes Multi-source legal text CLERC jhu-clsp/CLERC 1.84M federal cases, 20.7M citations Citation retrieval CUAD theatticusproject/cuad-qa 510 contracts, 41 clause types Contract analysis LegalBench nguha/legalbench 162 reasoning tasks Multi-hop reasoning BSARD maastrichtlawtech/bsard Belgian statutes (French) Statute retrieval pattern FairLex coastalcph/fairlex Swiss/EU court decisions Multilingual extension HFforLegal Case-law HFforLegal/case-law Multi-country case law International cases IL-TUR Exploration-Lab/IL-TUR Indian statute identification Statute lookup pattern Task-Oriented Datasets Dataset Focus Agentic Relevance LawFlow Paper End-to-end lawyer workflows LegalBench nguha/legalbench 162 tasks, 6 reasoning types CLERC GitHub Citation retrieval + RAG Lawyer-Instruct Alignment-Lab-AI/Lawyer-Instruct Legal dialogue (instruction format) Priority Datasets for S1-S2 Priority Dataset Why 1 LawFlow Full lawyer workflow: client interview \u2192 research \u2192 drafting 2 LegalBench 162 reasoning tasks, chainable into multi-hop 3 CLERC Citation retrieval + generation for research planning 4 Caselaw Access Project 6.7M cases for retrieval corpus 5 CUAD Contract analysis for transactional workflows Proposed English Agentic Framework Legal-10 Agentic (English) \u251c\u2500\u2500 Corpora (adapt from HF) \u2502 \u251c\u2500\u2500 Caselaw Access Project (case retrieval) \u2502 \u251c\u2500\u2500 Pile of Law - statutes (statute lookup) \u2502 \u251c\u2500\u2500 CUAD (contract DB) \u2502 \u2514\u2500\u2500 CLERC passages (citation retrieval) \u251c\u2500\u2500 Tools (build for English legal DBs) \u2502 \u251c\u2500\u2500 Case search (Westlaw/CourtListener API pattern) \u2502 \u251c\u2500\u2500 Statute lookup \u2502 \u251c\u2500\u2500 Citation validator \u2502 \u251c\u2500\u2500 Contract clause extractor \u2502 \u2514\u2500\u2500 Math/aggregation tools \u251c\u2500\u2500 Tasks (300, adapted from) \u2502 \u251c\u2500\u2500 LawFlow workflows \u2192 multi-hop \u2502 \u251c\u2500\u2500 LegalBench reasoning \u2192 subtasks \u2502 \u2514\u2500\u2500 CLERC retrieval \u2192 citation tasks \u2514\u2500\u2500 Evaluation \u251c\u2500\u2500 Success rate \u251c\u2500\u2500 Progress rate (intermediate keywords) \u2514\u2500\u2500 BERT-Score Existing Legal AG Benchmarks (Reference) LegalAgentBench (ACL 2025) Focus: LLM Agents in Chinese legal domain Dataset: 17 corpora, 37 tools, 300 tasks Paper: arxiv.org/abs/2412.17259 GitHub: CSHaitao/LegalAgentBench LegalBench (2023) Focus: Legal reasoning evaluation (not agentic) Tasks: 162 tasks, 6 reasoning categories Paper: arxiv.org/abs/2308.11462 GitHub: HazyResearch/legalbench VLAIR (2025) Focus: Commercial legal AI tool evaluation Tasks: 7 task types (Document Q&A, Redlining, etc.) Source: vals.ai/vlair Harvey BigLaw Bench (2024) Focus: Real BigLaw tasks from billable time entries Evaluation: Rubric penalties for hallucinations, tone, relevance Source: Legal IT Insider General Agentic Benchmarks (Non-Legal Reference) Benchmark Focus Relevance BFCL Function calling accuracy Tool-use evaluation pattern AgentBench LLM-as-agent across 8 environments Multi-turn architecture SWE-bench Real GitHub issue resolution Task completion pattern LiveSWEBench Tiered autonomy levels Agentic vs prompted vs autocomplete DA-Code Data science agent tasks Domain-specific agent pattern MLE-Bench ML engineering (Kaggle) Professional task benchmark LawFlow Dataset Details Paper: arxiv.org/abs/2504.18942 What It Captures End-to-end legal workflows from trained law students Business entity formation scenarios Dynamic, modular, iterative reasoning (not linear chains) Workflow Components Client information elicitation Issue identification Note-taking Legal research Template selection Drafting Key Finding Human workflows demonstrate modularity and adaptability. LLM workflows tend toward sequential, exhaustive processing with limited sensitivity to downstream implications. AI Role Insight Legal professionals envision AI excelling in supportive capacities: \"brainstorming, identifying blind spots, and surfacing alternatives\" \u2014 not executing entire workflows independently. Next Steps [ ] Download and analyze LawFlow dataset structure [ ] Map LegalBench tasks to multi-hop chains [ ] Design English tool taxonomy (37 tools equivalent) [ ] Identify corpora for each tool category [ ] Create task annotation guidelines [ ] Recruit legal expert annotators References Dahl, M., et al. (2024). Large Legal Fictions. Journal of Legal Analysis . LegalAgentBench (2025). ACL 2025. arXiv:2412.17259 LegalBench (2023). arXiv:2308.11462 LawFlow (2025). arXiv:2504.18942 CLERC (2025). NAACL 2025. arXiv:2406.17186","title":"Introducing L-10 Agentic Benchmark"},{"location":"agentic_research/#legal-10-agentic-benchmark-research","text":"Internal development notes \u2014 not listed in navigation","title":"Legal-10 Agentic Benchmark Research"},{"location":"agentic_research/#overview","text":"This document catalogs research for developing Legal-10 Agentic (S1-S2), an English-language legal agent benchmark adapted from LegalAgentBench methodology.","title":"Overview"},{"location":"agentic_research/#benchmark-architecture-decision","text":"","title":"Benchmark Architecture Decision"},{"location":"agentic_research/#legal-10-structure","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 LEGAL-10 BENCHMARK \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 CORE (Required) - 8 Skills \u2502 \u2502 \u251c\u2500\u2500 CB: S7, S8, S9, S10 \u2502 \u2502 \u2514\u2500\u2500 RAG: S3, S4, S5, S6 \u2502 \u2502 \u2192 Standard completion + long context \u2502 \u2502 \u2192 Single connection: CB first \u2192 then RAG \u2502 \u2502 \u2192 No cross-contamination (stateless per instance) \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 AGENTIC (Optional) - 2 Skills \u2502 \u2502 \u2514\u2500\u2500 AG: S1, S2 \u2502 \u2502 \u2192 Requires tool use / function calling \u2502 \u2502 \u2192 Separate benchmark run \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"Legal-10 Structure"},{"location":"agentic_research/#scoring","text":"Model Core (8) Agentic (2) Full (10) GPT-4 78% 70% 76% Claude 80% 72% 78% Llama-3 75% N/A 75%* *Models without tool-use get Core score only","title":"Scoring"},{"location":"agentic_research/#legalagentbench-methodology-reference","text":"Paper: arxiv.org/abs/2412.17259 (ACL 2025)","title":"LegalAgentBench Methodology (Reference)"},{"location":"agentic_research/#framework-components","text":"Component LegalAgentBench (Chinese) Legal-10 Agentic (English) Corpora 17 (14 tabular + 3 retrieval) Adapt from HuggingFace Tools 37 specialized tools Build for English legal DBs Tasks 300 annotated Combine from HF datasets Jurisdiction Chinese civil law US/UK common law","title":"Framework Components"},{"location":"agentic_research/#task-construction-framework-6-steps","text":"Planning Tree Construction \u2014 Model tool call dependencies as hierarchical tree Path Selection \u2014 Extract solution paths from 1-hop to 5-hop complexity Entity Selection \u2014 Validate initial entities complete intended paths Question Rewriting \u2014 Transform templates into natural language (GPT-4) Answer Generation \u2014 Programmatically extract via parameterized tool chains Human Verification \u2014 Expert legal professionals validate all Q&A","title":"Task Construction Framework (6 Steps)"},{"location":"agentic_research/#task-distribution","text":"Complexity Count 1-hop 80 2-hop 80 3-hop 60 4-hop 40 5-hop 20 Writing 20 Total 300","title":"Task Distribution"},{"location":"agentic_research/#tool-categories","text":"Category Count Function Text Retrievers 3 Embedding-based document search Database Tools 28 Query structured legal data Mathematical Tools 5 Arithmetic, sorting, aggregation System Tools 1 \"Finish\" tool for final answers","title":"Tool Categories"},{"location":"agentic_research/#evaluation-metrics","text":"Success Rate \u2014 Keyword matching for final answers Progress Rate \u2014 Intermediate step keywords (key_middle) BERT-Score \u2014 Semantic similarity","title":"Evaluation Metrics"},{"location":"agentic_research/#key-findings","text":"GPT-4o achieved 79.08% success rate (ReAct method) Success declined from 93% (1-hop) to 61% (5-hop) LLMs struggle with: legal terminology, deep article interpretation, tool argument specification","title":"Key Findings"},{"location":"agentic_research/#huggingface-english-legal-datasets","text":"","title":"HuggingFace English Legal Datasets"},{"location":"agentic_research/#corpus-sources","text":"Dataset HuggingFace Link Content Agentic Use Caselaw Access Project free-law/Caselaw_Access_Project 6.7M US court decisions Case retrieval corpus COLD Cases harvard-lil/cold-cases Reformatted CourtListener Structured case DB Pile of Law pile-of-law/pile-of-law Court opinions, briefs, statutes Multi-source legal text CLERC jhu-clsp/CLERC 1.84M federal cases, 20.7M citations Citation retrieval CUAD theatticusproject/cuad-qa 510 contracts, 41 clause types Contract analysis LegalBench nguha/legalbench 162 reasoning tasks Multi-hop reasoning BSARD maastrichtlawtech/bsard Belgian statutes (French) Statute retrieval pattern FairLex coastalcph/fairlex Swiss/EU court decisions Multilingual extension HFforLegal Case-law HFforLegal/case-law Multi-country case law International cases IL-TUR Exploration-Lab/IL-TUR Indian statute identification Statute lookup pattern","title":"Corpus Sources"},{"location":"agentic_research/#task-oriented-datasets","text":"Dataset Focus Agentic Relevance LawFlow Paper End-to-end lawyer workflows LegalBench nguha/legalbench 162 tasks, 6 reasoning types CLERC GitHub Citation retrieval + RAG Lawyer-Instruct Alignment-Lab-AI/Lawyer-Instruct Legal dialogue (instruction format)","title":"Task-Oriented Datasets"},{"location":"agentic_research/#priority-datasets-for-s1-s2","text":"Priority Dataset Why 1 LawFlow Full lawyer workflow: client interview \u2192 research \u2192 drafting 2 LegalBench 162 reasoning tasks, chainable into multi-hop 3 CLERC Citation retrieval + generation for research planning 4 Caselaw Access Project 6.7M cases for retrieval corpus 5 CUAD Contract analysis for transactional workflows","title":"Priority Datasets for S1-S2"},{"location":"agentic_research/#proposed-english-agentic-framework","text":"Legal-10 Agentic (English) \u251c\u2500\u2500 Corpora (adapt from HF) \u2502 \u251c\u2500\u2500 Caselaw Access Project (case retrieval) \u2502 \u251c\u2500\u2500 Pile of Law - statutes (statute lookup) \u2502 \u251c\u2500\u2500 CUAD (contract DB) \u2502 \u2514\u2500\u2500 CLERC passages (citation retrieval) \u251c\u2500\u2500 Tools (build for English legal DBs) \u2502 \u251c\u2500\u2500 Case search (Westlaw/CourtListener API pattern) \u2502 \u251c\u2500\u2500 Statute lookup \u2502 \u251c\u2500\u2500 Citation validator \u2502 \u251c\u2500\u2500 Contract clause extractor \u2502 \u2514\u2500\u2500 Math/aggregation tools \u251c\u2500\u2500 Tasks (300, adapted from) \u2502 \u251c\u2500\u2500 LawFlow workflows \u2192 multi-hop \u2502 \u251c\u2500\u2500 LegalBench reasoning \u2192 subtasks \u2502 \u2514\u2500\u2500 CLERC retrieval \u2192 citation tasks \u2514\u2500\u2500 Evaluation \u251c\u2500\u2500 Success rate \u251c\u2500\u2500 Progress rate (intermediate keywords) \u2514\u2500\u2500 BERT-Score","title":"Proposed English Agentic Framework"},{"location":"agentic_research/#existing-legal-ag-benchmarks-reference","text":"","title":"Existing Legal AG Benchmarks (Reference)"},{"location":"agentic_research/#legalagentbench-acl-2025","text":"Focus: LLM Agents in Chinese legal domain Dataset: 17 corpora, 37 tools, 300 tasks Paper: arxiv.org/abs/2412.17259 GitHub: CSHaitao/LegalAgentBench","title":"LegalAgentBench (ACL 2025)"},{"location":"agentic_research/#legalbench-2023","text":"Focus: Legal reasoning evaluation (not agentic) Tasks: 162 tasks, 6 reasoning categories Paper: arxiv.org/abs/2308.11462 GitHub: HazyResearch/legalbench","title":"LegalBench (2023)"},{"location":"agentic_research/#vlair-2025","text":"Focus: Commercial legal AI tool evaluation Tasks: 7 task types (Document Q&A, Redlining, etc.) Source: vals.ai/vlair","title":"VLAIR (2025)"},{"location":"agentic_research/#harvey-biglaw-bench-2024","text":"Focus: Real BigLaw tasks from billable time entries Evaluation: Rubric penalties for hallucinations, tone, relevance Source: Legal IT Insider","title":"Harvey BigLaw Bench (2024)"},{"location":"agentic_research/#general-agentic-benchmarks-non-legal-reference","text":"Benchmark Focus Relevance BFCL Function calling accuracy Tool-use evaluation pattern AgentBench LLM-as-agent across 8 environments Multi-turn architecture SWE-bench Real GitHub issue resolution Task completion pattern LiveSWEBench Tiered autonomy levels Agentic vs prompted vs autocomplete DA-Code Data science agent tasks Domain-specific agent pattern MLE-Bench ML engineering (Kaggle) Professional task benchmark","title":"General Agentic Benchmarks (Non-Legal Reference)"},{"location":"agentic_research/#lawflow-dataset-details","text":"Paper: arxiv.org/abs/2504.18942","title":"LawFlow Dataset Details"},{"location":"agentic_research/#what-it-captures","text":"End-to-end legal workflows from trained law students Business entity formation scenarios Dynamic, modular, iterative reasoning (not linear chains)","title":"What It Captures"},{"location":"agentic_research/#workflow-components","text":"Client information elicitation Issue identification Note-taking Legal research Template selection Drafting","title":"Workflow Components"},{"location":"agentic_research/#key-finding","text":"Human workflows demonstrate modularity and adaptability. LLM workflows tend toward sequential, exhaustive processing with limited sensitivity to downstream implications.","title":"Key Finding"},{"location":"agentic_research/#ai-role-insight","text":"Legal professionals envision AI excelling in supportive capacities: \"brainstorming, identifying blind spots, and surfacing alternatives\" \u2014 not executing entire workflows independently.","title":"AI Role Insight"},{"location":"agentic_research/#next-steps","text":"[ ] Download and analyze LawFlow dataset structure [ ] Map LegalBench tasks to multi-hop chains [ ] Design English tool taxonomy (37 tools equivalent) [ ] Identify corpora for each tool category [ ] Create task annotation guidelines [ ] Recruit legal expert annotators","title":"Next Steps"},{"location":"agentic_research/#references","text":"Dahl, M., et al. (2024). Large Legal Fictions. Journal of Legal Analysis . LegalAgentBench (2025). ACL 2025. arXiv:2412.17259 LegalBench (2023). arXiv:2308.11462 LawFlow (2025). arXiv:2504.18942 CLERC (2025). NAACL 2025. arXiv:2406.17186","title":"References"},{"location":"benchmark/","text":"Advanced Benchmarking Guide Running Restricted Benchmarks Some of the benchmarks (NewsQA) depend on data that's not public: all such data will be stored in the restricted directory. You need to make sure that directory exists. Dry Runs The helm-run provides several flags that can be used to test that the configuration and scenario are working correctly without actually sending requests to the model # Just load the config file helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --skip-instances # Create the instances and the requests, but don't send requests to the model helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --dry-run Estimating Token Usage To estimate token usage without making any requests, append the --dry-run option: helm-run -r <RunSpec to estimate token usage> --suite $SUITE --max-eval-instances <Number of eval instances> --dry-run and check the output in benchmark_output/runs/$SUITE . sum indicates the estimated total number of tokens used for the specific RunSpec . For the OpenAI models, we use a GPT-2 Tokenizer to estimate the token usage. The tokenizer will be downloaded and cached when running a dry run. Perspective API We use Google's Perspective API to calculate the toxicity of completions. To send requests to PerspectiveAPI, we need to generate an API key from GCP. Follow the Get Started guide to request the service and the Enable the API guide to generate the API key. Once you have a valid API key, add an entry to credentials.conf : perspectiveApiKey: <Generated API key> By default, Perspective API allows only 1 query per second. Fill out this form to increase the request quota.","title":"Advanced Benchmarking Guide"},{"location":"benchmark/#advanced-benchmarking-guide","text":"","title":"Advanced Benchmarking Guide"},{"location":"benchmark/#running-restricted-benchmarks","text":"Some of the benchmarks (NewsQA) depend on data that's not public: all such data will be stored in the restricted directory. You need to make sure that directory exists.","title":"Running Restricted Benchmarks"},{"location":"benchmark/#dry-runs","text":"The helm-run provides several flags that can be used to test that the configuration and scenario are working correctly without actually sending requests to the model # Just load the config file helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --skip-instances # Create the instances and the requests, but don't send requests to the model helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --dry-run","title":"Dry Runs"},{"location":"benchmark/#estimating-token-usage","text":"To estimate token usage without making any requests, append the --dry-run option: helm-run -r <RunSpec to estimate token usage> --suite $SUITE --max-eval-instances <Number of eval instances> --dry-run and check the output in benchmark_output/runs/$SUITE . sum indicates the estimated total number of tokens used for the specific RunSpec . For the OpenAI models, we use a GPT-2 Tokenizer to estimate the token usage. The tokenizer will be downloaded and cached when running a dry run.","title":"Estimating Token Usage"},{"location":"benchmark/#perspective-api","text":"We use Google's Perspective API to calculate the toxicity of completions. To send requests to PerspectiveAPI, we need to generate an API key from GCP. Follow the Get Started guide to request the service and the Enable the API guide to generate the API key. Once you have a valid API key, add an entry to credentials.conf : perspectiveApiKey: <Generated API key> By default, Perspective API allows only 1 query per second. Fill out this form to increase the request quota.","title":"Perspective API"},{"location":"code/","text":"Code Structure Warning \u2014 The document is stale and was last modified more than ten months ago. The information below may be outdated and incorrect. Please proceed with caution! Birds-Eye View Here's a birds-eye view of how the benchmarking process interacts with the main classes (see benchmark ): A Scenario (given by a ScenarioSpec ) specifies a task and a data distribution. It specifies a set of Instance s, where each Instance has an input (e.g., question) and a set of Reference outputs (e.g., multiple choice answers). A DataPreprocessor takes in a Scenario and produces a list of Instance s. Each Instance is given a unique ID. The set of Instance s is augmented according to DataAugmenterSpec . An Adapter (given by an AdaptationSpec ) takes a list of Instance s and adapts it to a set of Request s to the API (e.g., the model, temperature, number of in-context training examples). Formally, the output is a ScenarioState containing a set of RequestState s, where each RequestState consists of a Request and any metadata used to track the role of this Request (e.g., the relevant Instance and Reference ). An Executor (given by an ExecutionSpec ) executes each Request in the RequestState to produce a RequestResult for each one; everything is encapsulated in a ScenarioState . A Metric (given by a MetricSpec ) takes a ScenarioState containing RequestResults s and produces a set of Stat s (e.g., accuracy, accuracy@5, toxicity, bias, etc.). A Runner is the top-level controller that runs the above steps and is driven by a set of RunSpec s. There are three types of classes: Specifications (e.g., AdapterSpec , ExecutionSpec , RunSpec ): specified manually by the user. Note that Scenario and Metric are subclassed, so they are constructed by ObjectSpec , which specifies the subclass name and a free-form dictionary of arguments. States (e.g., Instance , ScenarioState , Request , RequestResult ): these are automatically generated and can be serialized. Controllers (e.g., Scenario , Adapter , Executor , Metric , Runner ): these have the bulk of the code and should not be serialized. Adding new scenarios In order to implement new scenarios: Create a new Python file in the scenarios folder. Within the scenario file, create a Scenario class, e.g. YourScenario . YourScenario should implement get_instances , a method that downloads the dataset files if they don't already exist and returns a list of Instance s. Each Instance must have a list of (potentially one) Reference answers: a correct answer may be indicated with a CORRECT_TAG in a Reference instance's tags argument. In addition, you must specify the split of the Instance as one of TRAIN_SPLIT , VALID_SPLIT , or TEST_SPLIT constants as in scenario.py . For Scenario s with datasets that cannot be publicly shared, place a copy of the dataset at path restricted/<Name of the Scenario> and read from that path. See NewsQAScenario and ICEScenario for some examples. Note that you need not enumerate every possible correct answer (nor must there even necessarily be a correct answer). Make sure to document your scenario well with a clear docstring. In addition, specify its name , description , and tags . Identify the appropriate metric for your task in one of the *_metrics.py files. If the metric you'd like to use does not exist, follow the directions in Adding new metrics . Many will be in basic_metrics.py . Define a function in run_specs.py annotated with run_spec_function to: Construct a ScenarioSpec for your scenario using a class name corresponding to the Python path of the class (e.g. helm.benchmark.scenarios.your_scenario.YourScenario ) and any arguments which must be passed as a dictionary of args . Construct an AdapterSpec for your scenario specifying the type of language model generation which must be performed for the task. Construct one or more MetricSpec objects for your task, specifying the classname with the Python path of the object, with the same arguments as the ScenarioSpec constructor. Construct and return RunSpec object, with a name corresponding to the scenario name and any patterns to match in curly braces, a scenario_spec , an adapter_spec , metric_specs , and groups . Attempt to run your task with venv/bin/helm-run -r yourscenarioname:arg=value where yourscenarioname matches the name specified in YourScenario Update src/helm/benchmark/static/contamination.yaml with models that were trained on your scenario (i.e. contaminated). Add a schema to src/helm/benchmark/static/schema.yaml and add the scenario to subgroups as needed. Adding new metrics To add a new metric, first determine if your metric is generic and likely to be widely used, or specific to your task. For generic metrics: Add a method to basic_metrics.py which takes two arguments: the gold answer and the model's pred iction. Add your method to the metric_fn_mapping lookup. For task specific metrics: Create a new yourtask_metrics.py file for class YourTaskMetric which inherits from Metric in metric.py . Define methods __init__ and evaluate_generation returning a list of Stat objects. Your metric is responsible for producing Stat objects: Each Stat should correspond to a distinct aggregate measurement over the generated examples. Some may have one metric (e.g. accuracy), while others may quantify multiple aspects (e.g. multiple distance metrics). For each value generated for a Stat , add it to yourstat using yourstat.add(value) . Usually, there will only be one value for each Stat , but multiple can be used, e.g. to show variance. Data augmentations To apply data augmentation, create a DataAugmenterSpec with a list of PerturbationSpec s and pass it into RunSpec . The following is an example: data_augmenter_spec = DataAugmenterSpec( perturbation_specs=[ PerturbationSpec( class_name=\"helm.benchmark.augmentations.perturbation.ExtraSpacePerturbation\", args={\"num_spaces\": 5}, ) ], should_perturb_references=False, should_augment_train_instances=False, should_include_original_train=False, should_augment_eval_instances=True, should_include_original_eval=True, ) run_spec = RunSpec( ... data_augmenter_spec=data_augmenter_spec ) In the example above, the DataPreprocessor will augment the set of evaluation instances by perturbing the original set of instances with the ExtraSpacePerturbation , where spaces in the text are replaced with num_spaces number of spaces. We currently only support applying a single perturbation to an instance instead of chaining multiple perturbations and applying it onto a single instance. Adding a new perturbation To add a new perturbation to the framework, create a new file at src/helm/benchmark/augmentations with the name <Name of perturbation>_perturbation.py e.g., typo_perturbation.py . Inside the file, create a new class (name it <Name of the perturbation>Perturbation e.g., TypoPerturbation ) that extends the abstract class Perturbation and implement the perturb method which takes in text and outputs the perturbed text. Add a test for the new perturbation in test_perturbation.py . Supporting new Hugging Face tokenizers Give the tokenizer a name. Use the same name that's used in Hugging Face (e.g., \"EleutherAI/gpt-j-6B\"). In HuggingFaceTokenizers , we load and cache tokenizers in memory. Add logic to handle the tokenizer in the load_tokenizer method. Add a test in test_huggingface_tokenizer.py to make sure we can load the tokenizer from Hugging Face. Add a new class <Name of tokenizer>WindowService in file <Name of tokenizer>_window_service.py . Follow what we did for GPTJWindowService . Import the new WindowService and map the model(s) to it in WindowServiceFactory . HEIM (text-to-image evaluation) The overall code structure is the same as HELM's. When adding new scenarios and metrics for image generation, place the Python files under the image_generation package (e.g., src/helm/benchmark/scenarios/image_generation ).","title":"Code Structure"},{"location":"code/#code-structure","text":"Warning \u2014 The document is stale and was last modified more than ten months ago. The information below may be outdated and incorrect. Please proceed with caution!","title":"Code Structure"},{"location":"code/#birds-eye-view","text":"Here's a birds-eye view of how the benchmarking process interacts with the main classes (see benchmark ): A Scenario (given by a ScenarioSpec ) specifies a task and a data distribution. It specifies a set of Instance s, where each Instance has an input (e.g., question) and a set of Reference outputs (e.g., multiple choice answers). A DataPreprocessor takes in a Scenario and produces a list of Instance s. Each Instance is given a unique ID. The set of Instance s is augmented according to DataAugmenterSpec . An Adapter (given by an AdaptationSpec ) takes a list of Instance s and adapts it to a set of Request s to the API (e.g., the model, temperature, number of in-context training examples). Formally, the output is a ScenarioState containing a set of RequestState s, where each RequestState consists of a Request and any metadata used to track the role of this Request (e.g., the relevant Instance and Reference ). An Executor (given by an ExecutionSpec ) executes each Request in the RequestState to produce a RequestResult for each one; everything is encapsulated in a ScenarioState . A Metric (given by a MetricSpec ) takes a ScenarioState containing RequestResults s and produces a set of Stat s (e.g., accuracy, accuracy@5, toxicity, bias, etc.). A Runner is the top-level controller that runs the above steps and is driven by a set of RunSpec s. There are three types of classes: Specifications (e.g., AdapterSpec , ExecutionSpec , RunSpec ): specified manually by the user. Note that Scenario and Metric are subclassed, so they are constructed by ObjectSpec , which specifies the subclass name and a free-form dictionary of arguments. States (e.g., Instance , ScenarioState , Request , RequestResult ): these are automatically generated and can be serialized. Controllers (e.g., Scenario , Adapter , Executor , Metric , Runner ): these have the bulk of the code and should not be serialized.","title":"Birds-Eye View"},{"location":"code/#adding-new-scenarios","text":"In order to implement new scenarios: Create a new Python file in the scenarios folder. Within the scenario file, create a Scenario class, e.g. YourScenario . YourScenario should implement get_instances , a method that downloads the dataset files if they don't already exist and returns a list of Instance s. Each Instance must have a list of (potentially one) Reference answers: a correct answer may be indicated with a CORRECT_TAG in a Reference instance's tags argument. In addition, you must specify the split of the Instance as one of TRAIN_SPLIT , VALID_SPLIT , or TEST_SPLIT constants as in scenario.py . For Scenario s with datasets that cannot be publicly shared, place a copy of the dataset at path restricted/<Name of the Scenario> and read from that path. See NewsQAScenario and ICEScenario for some examples. Note that you need not enumerate every possible correct answer (nor must there even necessarily be a correct answer). Make sure to document your scenario well with a clear docstring. In addition, specify its name , description , and tags . Identify the appropriate metric for your task in one of the *_metrics.py files. If the metric you'd like to use does not exist, follow the directions in Adding new metrics . Many will be in basic_metrics.py . Define a function in run_specs.py annotated with run_spec_function to: Construct a ScenarioSpec for your scenario using a class name corresponding to the Python path of the class (e.g. helm.benchmark.scenarios.your_scenario.YourScenario ) and any arguments which must be passed as a dictionary of args . Construct an AdapterSpec for your scenario specifying the type of language model generation which must be performed for the task. Construct one or more MetricSpec objects for your task, specifying the classname with the Python path of the object, with the same arguments as the ScenarioSpec constructor. Construct and return RunSpec object, with a name corresponding to the scenario name and any patterns to match in curly braces, a scenario_spec , an adapter_spec , metric_specs , and groups . Attempt to run your task with venv/bin/helm-run -r yourscenarioname:arg=value where yourscenarioname matches the name specified in YourScenario Update src/helm/benchmark/static/contamination.yaml with models that were trained on your scenario (i.e. contaminated). Add a schema to src/helm/benchmark/static/schema.yaml and add the scenario to subgroups as needed.","title":"Adding new scenarios"},{"location":"code/#adding-new-metrics","text":"To add a new metric, first determine if your metric is generic and likely to be widely used, or specific to your task. For generic metrics: Add a method to basic_metrics.py which takes two arguments: the gold answer and the model's pred iction. Add your method to the metric_fn_mapping lookup. For task specific metrics: Create a new yourtask_metrics.py file for class YourTaskMetric which inherits from Metric in metric.py . Define methods __init__ and evaluate_generation returning a list of Stat objects. Your metric is responsible for producing Stat objects: Each Stat should correspond to a distinct aggregate measurement over the generated examples. Some may have one metric (e.g. accuracy), while others may quantify multiple aspects (e.g. multiple distance metrics). For each value generated for a Stat , add it to yourstat using yourstat.add(value) . Usually, there will only be one value for each Stat , but multiple can be used, e.g. to show variance.","title":"Adding new metrics"},{"location":"code/#data-augmentations","text":"To apply data augmentation, create a DataAugmenterSpec with a list of PerturbationSpec s and pass it into RunSpec . The following is an example: data_augmenter_spec = DataAugmenterSpec( perturbation_specs=[ PerturbationSpec( class_name=\"helm.benchmark.augmentations.perturbation.ExtraSpacePerturbation\", args={\"num_spaces\": 5}, ) ], should_perturb_references=False, should_augment_train_instances=False, should_include_original_train=False, should_augment_eval_instances=True, should_include_original_eval=True, ) run_spec = RunSpec( ... data_augmenter_spec=data_augmenter_spec ) In the example above, the DataPreprocessor will augment the set of evaluation instances by perturbing the original set of instances with the ExtraSpacePerturbation , where spaces in the text are replaced with num_spaces number of spaces. We currently only support applying a single perturbation to an instance instead of chaining multiple perturbations and applying it onto a single instance.","title":"Data augmentations"},{"location":"code/#adding-a-new-perturbation","text":"To add a new perturbation to the framework, create a new file at src/helm/benchmark/augmentations with the name <Name of perturbation>_perturbation.py e.g., typo_perturbation.py . Inside the file, create a new class (name it <Name of the perturbation>Perturbation e.g., TypoPerturbation ) that extends the abstract class Perturbation and implement the perturb method which takes in text and outputs the perturbed text. Add a test for the new perturbation in test_perturbation.py .","title":"Adding a new perturbation"},{"location":"code/#supporting-new-hugging-face-tokenizers","text":"Give the tokenizer a name. Use the same name that's used in Hugging Face (e.g., \"EleutherAI/gpt-j-6B\"). In HuggingFaceTokenizers , we load and cache tokenizers in memory. Add logic to handle the tokenizer in the load_tokenizer method. Add a test in test_huggingface_tokenizer.py to make sure we can load the tokenizer from Hugging Face. Add a new class <Name of tokenizer>WindowService in file <Name of tokenizer>_window_service.py . Follow what we did for GPTJWindowService . Import the new WindowService and map the model(s) to it in WindowServiceFactory .","title":"Supporting new Hugging Face tokenizers"},{"location":"code/#heim-text-to-image-evaluation","text":"The overall code structure is the same as HELM's. When adding new scenarios and metrics for image generation, place the Python files under the image_generation package (e.g., src/helm/benchmark/scenarios/image_generation ).","title":"HEIM (text-to-image evaluation)"},{"location":"credentials/","text":"Credentials Credentials file You should create a credentials.conf file in your local configuration folder, which is ./prod_env/ by default, unless you have overridden it using the --local-path flag to helm-run . This file should be in HOCON format. Example: platformOneApiKey: sk-abcdefgh platformTneApiKey: sk-ijklmnop Here are the keys that must be set for to access these platforms: AI21: ai21ApiKey Aleph Alpha: AlephAlphaApiKey Anthropic: anthropicApiKey Cohere: cohereApiKey Google: googleProjectId , googleLocation , also see Additional Setup below GooseAI: gooseApiKey Hugging Face Hub: None, but see Additional Setup below Mistral AI: mistralaiApiKey OpenAI: openaiApiKey , openApiOrgId Perspective: perspectiveApiKey Writer: writerApiKey Additional setup Google You will need to install the Google Cloud CLI . Then, as the user that will be running helm-run , run: gcloud auth application-default login gcloud auth application-default set-quota-project 123456789012 Replace 123456789012 with your actual numeric project ID. Hugging Face Hub If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 2) through Hugging Face, you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run: huggingface-cli login Refer to Hugging Face's documentation for more information.","title":"Credentials"},{"location":"credentials/#credentials","text":"","title":"Credentials"},{"location":"credentials/#credentials-file","text":"You should create a credentials.conf file in your local configuration folder, which is ./prod_env/ by default, unless you have overridden it using the --local-path flag to helm-run . This file should be in HOCON format. Example: platformOneApiKey: sk-abcdefgh platformTneApiKey: sk-ijklmnop Here are the keys that must be set for to access these platforms: AI21: ai21ApiKey Aleph Alpha: AlephAlphaApiKey Anthropic: anthropicApiKey Cohere: cohereApiKey Google: googleProjectId , googleLocation , also see Additional Setup below GooseAI: gooseApiKey Hugging Face Hub: None, but see Additional Setup below Mistral AI: mistralaiApiKey OpenAI: openaiApiKey , openApiOrgId Perspective: perspectiveApiKey Writer: writerApiKey","title":"Credentials file"},{"location":"credentials/#additional-setup","text":"","title":"Additional setup"},{"location":"credentials/#google","text":"You will need to install the Google Cloud CLI . Then, as the user that will be running helm-run , run: gcloud auth application-default login gcloud auth application-default set-quota-project 123456789012 Replace 123456789012 with your actual numeric project ID.","title":"Google"},{"location":"credentials/#hugging-face-hub","text":"If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 2) through Hugging Face, you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run: huggingface-cli login Refer to Hugging Face's documentation for more information.","title":"Hugging Face Hub"},{"location":"dataset_integration_guide/","text":"HELM Dataset Integration Guide Overview This guide catalogs how HELM connects to external datasets and provides patterns for integrating Legal-10 datasets. Dataset Sources HELM supports multiple data source types: Source Type Usage Examples HuggingFace Datasets Primary method (~90% of scenarios) CaseHOLD, LegalBench, LexGLUE Direct URLs Manual downloads with caching Legal Support Git Repositories Version-controlled data LegalBench prompt settings Local Files Post-download parsing JSONL, CSV, TSV files Current Legal Datasets in HELM 1. CaseHOLD (Case Holdings On Legal Decisions) File: src/helm/benchmark/scenarios/casehold_scenario.py Dataset Source: from datasets import load_dataset dataset = load_dataset( \"casehold/casehold\", # HuggingFace dataset ID \"all\", # Configuration/subset cache_dir=data_path # Local cache: {output_path}/data ) HuggingFace: casehold/casehold Data Structure: - Task: Multiple choice QA (5 options) - Splits: train , test - Fields: - example_id - Unique identifier - citing_prompt - Context passage - holding_0 through holding_4 - Answer choices - label - Correct answer index (\"0\"-\"4\") Run Spec: enterprise_run_specs.py::get_casehold_spec() Adapter: Multiple Choice Joint Metrics: Exact Match 2. LegalBench File: src/helm/benchmark/scenarios/legalbench_scenario.py Dataset Source: import datasets train_dataset = datasets.load_dataset( \"nguha/legalbench\", subset, # e.g., \"abercrombie\", \"corporate_lobbying\" trust_remote_code=True, cache_dir=cache_dir, split=\"train\", revision=\"e042ea68c19df12b737fe768572f22ead61e8e37\" # Pinned version ) HuggingFace: nguha/legalbench Supported Subsets: - abercrombie - Trademark classification - corporate_lobbying - Corporate disclosure analysis - international_citizenship_questions - Citizenship law QA - function_of_decision_section - Legal document structure - proa - Legal reasoning Additional Resources: # External prompt configuration PROMPT_SETTINGS_URL = \"https://raw.githubusercontent.com/HazyResearch/legalbench/main/helm_prompt_settings.jsonl\" ensure_file_downloaded( source_url=PROMPT_SETTINGS_URL, target_path=prompt_settings_path ) Run Spec: lite_run_specs.py::get_legalbench_spec(subset) Key Feature: Uses revision parameter for reproducibility 3. LexGLUE (Legal General Language Understanding) File: src/helm/benchmark/scenarios/lex_glue_scenario.py Dataset Source: dataset = load_dataset( \"lex_glue\", config, # Task config: \"ecthr_a\", \"scotus\", \"case_hold\", etc. cache_dir=cache_dir ) HuggingFace: lex_glue Supported Tasks: Config Task Type Description ecthr_a Multi-label European Court of Human Rights (Articles) ecthr_b Multi-label European Court of Human Rights (Violations) scotus Single-label US Supreme Court topic classification eurlex Multi-label EU legislation classification ledgar Single-label Legal contract provisions unfair_tos Multi-label Terms of Service unfairness detection case_hold QA Legal case holding identification Run Spec: classic_run_specs.py::get_lex_glue_spec(subset) Dynamic Configuration: - Task type determines adapter (generation vs classification) - Max tokens varies by task (20-100) - Max train instances varies (0-5) 4. Legal Summarization (Multi-Dataset) File: src/helm/benchmark/scenarios/legal_summarization_scenario.py Supports 3 Datasets: A. BillSum (US Congressional Bills) dataset = load_dataset(\"billsum\", cache_dir=cache_dir) # Fields: text, summary HuggingFace: billsum Run Spec: classic_run_specs.py::get_billsum_legal_summarization_spec() Summary Length: 200-800 tokens B. MultiLexSum (Legal Case Summaries) dataset = load_dataset( \"allenai/multi_lexsum\", \"v20220616\", # Versioned config cache_dir=cache_dir ) # Fields: summary/long, summary/short HuggingFace: allenai/multi_lexsum Run Spec: classic_run_specs.py::get_multilexsum_legal_summarization_spec() Summary Length: 100-400 tokens C. EurLexSum (EU Legislation) dataset = load_dataset( \"dennlinger/eur-lex-sum\", \"english\", # Language config cache_dir=cache_dir ) # Fields: reference, summary HuggingFace: dennlinger/eur-lex-sum Run Spec: classic_run_specs.py::get_eurlexsum_legal_summarization_spec() Summary Length: 400-1600 tokens 5. Legal Support (Argumentative Reasoning) File: src/helm/benchmark/scenarios/legal_support_scenario.py Dataset Source: Direct URL Download from helm.common.general import ensure_file_downloaded ensure_file_downloaded( source_url=\"https://docs.google.com/uc?export=download&id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77\", target_path=data_path, unpack=True, unpack_type=\"unzip\" ) # Reads local JSONL files: # - train.jsonl # - dev.jsonl # - test.jsonl Data Structure: { \"context\": \"Legal passage text...\", \"citation_a\": \"First citation with (parenthetical text)\", \"citation_b\": \"Second citation with (parenthetical text)\", \"label\": \"a\" // or \"b\" } Run Spec: classic_run_specs.py::get_legal_support_spec() Task: Binary multiple choice (which citation better supports the passage) 6. LEXtreme (Multilingual Legal NLU) File: src/helm/benchmark/scenarios/lextreme_scenario.py Dataset Source: dataset = load_dataset( \"joelito/lextreme\", subset, # 18 different legal tasks cache_dir=cache_dir ) HuggingFace: joelito/lextreme 18 Legal Tasks across multiple languages (Portuguese, German, Greek, French, Italian, Romanian) Standard Dataset Integration Patterns Pattern 1: HuggingFace Datasets (Recommended) Use When: Dataset is publicly available on HuggingFace Hub Template: from typing import List import os from datasets import load_dataset, DatasetDict from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists class Legal10ExampleScenario(Scenario): name = \"legal_10_example\" description = \"Legal-10 example dataset\" tags = [\"legal\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Step 1: Set up cache directory cache_dir = os.path.join(output_path, \"data\") ensure_directory_exists(cache_dir) # Step 2: Load dataset from HuggingFace dataset = load_dataset( \"legal-10/example-dataset\", # HuggingFace dataset ID \"default\", # Config/subset (optional) cache_dir=cache_dir, revision=\"main\" # Or specific commit hash ) # Step 3: Convert to HELM instances instances = [] for split_name in [\"train\", \"test\"]: helm_split = TRAIN_SPLIT if split_name == \"train\" else TEST_SPLIT for idx, example in enumerate(dataset[split_name]): instance = Instance( input=Input(text=example[\"input_text\"]), references=[ Reference( Output(text=example[\"answer\"]), tags=[CORRECT_TAG] ) ], split=helm_split, id=f\"{split_name}_{idx}\" ) instances.append(instance) return instances Advantages: - Automatic caching - Version control via revision parameter - Standardized interface - Easy data exploration on HuggingFace website Pattern 2: Direct URL Download Use When: Dataset is hosted externally (not on HuggingFace) Template: import json from helm.common.general import ensure_file_downloaded class Legal10URLScenario(Scenario): name = \"legal_10_url_example\" description = \"Dataset from external URL\" tags = [\"legal\", \"legal_10\"] DATASET_URL = \"https://example.com/dataset.zip\" def get_instances(self, output_path: str) -> List[Instance]: # Step 1: Download and unpack data_path = os.path.join(output_path, \"data\") ensure_file_downloaded( source_url=self.DATASET_URL, target_path=data_path, unpack=True, unpack_type=\"zip\" # or \"tar\", \"unzstd\" ) # Step 2: Read local files instances = [] with open(os.path.join(data_path, \"train.jsonl\")) as f: for line in f: data = json.loads(line) instance = Instance( input=Input(text=data[\"question\"]), references=[ Reference(Output(text=data[\"answer\"]), tags=[CORRECT_TAG]) ], split=TRAIN_SPLIT, id=data[\"id\"] ) instances.append(instance) return instances Advantages: - Works with any HTTP/HTTPS URL - Automatic unpacking (zip, tar, tar.gz, zstd) - Local file caching Pattern 3: Multiple Choice with References Use When: Dataset has multiple answer choices (like CaseHOLD) Template: class Legal10MultipleChoiceScenario(Scenario): name = \"legal_10_mcqa\" description = \"Multiple choice QA\" tags = [\"legal\", \"question_answering\", \"legal_10\"] NUM_CHOICES = 4 # Number of answer options def get_instances(self, output_path: str) -> List[Instance]: cache_dir = os.path.join(output_path, \"data\") ensure_directory_exists(cache_dir) dataset = load_dataset(\"legal-10/mcqa\", cache_dir=cache_dir) instances = [] for example in dataset[\"test\"]: # Extract answer choices choices = [ example[\"choice_a\"], example[\"choice_b\"], example[\"choice_c\"], example[\"choice_d\"] ] correct_idx = int(example[\"label\"]) # 0-3 # Create references (one per choice) references = [ Reference( Output(text=choices[i]), tags=[CORRECT_TAG] if i == correct_idx else [] ) for i in range(self.NUM_CHOICES) ] instance = Instance( input=Input(text=example[\"question\"]), references=references, split=TEST_SPLIT, id=example[\"id\"] ) instances.append(instance) return instances Key Points: - One Reference per answer choice - Only correct answer has CORRECT_TAG - Used with ADAPT_MULTIPLE_CHOICE_JOINT adapter Cache Directory Structure HELM uses a consistent caching pattern: benchmark_output/ \u2514\u2500\u2500 scenarios/ \u2514\u2500\u2500 legal_10_example/ \u251c\u2500\u2500 data/ # Dataset cache (HuggingFace or downloaded) \u2502 \u251c\u2500\u2500 downloads/ # Raw downloads \u2502 \u2514\u2500\u2500 legal-10___example/ # Processed HuggingFace cache \u251c\u2500\u2500 instances.json # Generated instances \u2514\u2500\u2500 scenario_state.json # Execution results Key Functions: from helm.common.general import ( ensure_directory_exists, # Create dir if not exists ensure_file_downloaded, # Download file with caching ) from helm.benchmark.scenarios.scenario import get_scenario_cache_path # Get standard cache path cache_path = get_scenario_cache_path(benchmark_output_path, \"legal_10_example\") # Returns: benchmark_output/scenarios/legal_10_example Reproducibility Best Practices 1. Pin Dataset Versions HuggingFace: dataset = load_dataset( \"legal-10/dataset\", revision=\"abc123def456\" # Specific git commit hash ) Direct URLs: # Include version in filename or URL DATASET_URL = \"https://example.com/dataset-v1.2.3.zip\" 2. Document Data Source class Legal10Scenario(Scenario): \"\"\" Legal-10 Example Dataset Dataset repository: https://huggingface.co/datasets/legal-10/example Publication: Smith et al. (2024). \"Legal AI Benchmark.\" ICAIL. Data content: The dataset consists of X legal questions with Y answer choices. Questions are derived from Z legal domain. \"\"\" name = \"legal_10_example\" 3. Handle Missing Data Gracefully def get_instances(self, output_path: str) -> List[Instance]: try: dataset = load_dataset(\"legal-10/dataset\", cache_dir=cache_dir) except Exception as e: raise ValueError( f\"Failed to load dataset 'legal-10/dataset'. \" f\"Ensure you have internet connection and HuggingFace access. Error: {e}\" ) Legal-10 Dataset Requirements For Legal-10 benchmark integration, each skill needs: Skill Datasets Skill Dataset Preferred Source Status S1-S2 LegalAgentBench HuggingFace or URL To be uploaded S3-S4 CLERC HuggingFace or URL To be uploaded S5 KeyCite-CLERC HuggingFace or URL To be uploaded S6 CUAD HuggingFace: cuad Available S7 CaseHOLD HuggingFace: casehold/casehold \u2705 Integrated S8 LEXam HuggingFace or URL To be uploaded S9 Dahl 10-types HuggingFace or URL To be uploaded S10 SHIELD HuggingFace or URL To be uploaded Dataset Upload Checklist For HuggingFace hosting: [ ] Create HuggingFace dataset repository [ ] Upload data files (train/test/validation splits) [ ] Write dataset card (README.md) with: [ ] Description and citation [ ] Data structure documentation [ ] Field descriptions [ ] Licensing information [ ] Add dataset loading script (if custom format) [ ] Test loading: load_dataset(\"legal-10/dataset-name\") [ ] Pin revision for reproducibility Testing Dataset Integration 1. Test Loading # Test in Python REPL from datasets import load_dataset dataset = load_dataset(\"legal-10/example\") print(dataset) print(dataset[\"train\"][0]) # First example 2. Test Scenario # Run scenario in isolation from helm.benchmark.scenarios.legal_10_example_scenario import Legal10ExampleScenario scenario = Legal10ExampleScenario() instances = scenario.get_instances(\"test_output\") print(f\"Loaded {len(instances)} instances\") print(instances[0]) 3. Test Run Spec # Run small evaluation helm-run \\ --run-specs legal_10_example \\ --max-eval-instances 5 \\ --output-path test_output Common Issues & Solutions Issue 1: Dataset Not Found Error: FileNotFoundError or DatasetNotFoundError Solutions: - Verify dataset ID is correct - Check HuggingFace dataset is public - Ensure internet connection - Try with trust_remote_code=True if dataset has custom script Issue 2: Cache Permission Errors Error: PermissionError when writing to cache Solutions: - Ensure output_path directory is writable - Check disk space - Use ensure_directory_exists() before writing Issue 3: Version Mismatch Error: Dataset structure changed from expected format Solutions: - Pin revision parameter to specific commit - Document expected dataset version in scenario docstring - Add validation in get_instances() Summary HELM's dataset integration follows a consistent, modular pattern : Scenarios load data in get_instances() method HuggingFace is preferred for standardization and caching Direct URLs work for externally hosted datasets Caching is automatic in {output_path}/data/ directory Reproducibility via revision pinning and version documentation For Legal-10: - Upload datasets to HuggingFace under legal-10/ organization - Follow existing legal scenario patterns (CaseHOLD, LegalBench) - Use revision parameter for all datasets - Document data sources and citations clearly","title":"HELM Dataset Integration Guide"},{"location":"dataset_integration_guide/#helm-dataset-integration-guide","text":"","title":"HELM Dataset Integration Guide"},{"location":"dataset_integration_guide/#overview","text":"This guide catalogs how HELM connects to external datasets and provides patterns for integrating Legal-10 datasets.","title":"Overview"},{"location":"dataset_integration_guide/#dataset-sources","text":"HELM supports multiple data source types: Source Type Usage Examples HuggingFace Datasets Primary method (~90% of scenarios) CaseHOLD, LegalBench, LexGLUE Direct URLs Manual downloads with caching Legal Support Git Repositories Version-controlled data LegalBench prompt settings Local Files Post-download parsing JSONL, CSV, TSV files","title":"Dataset Sources"},{"location":"dataset_integration_guide/#current-legal-datasets-in-helm","text":"","title":"Current Legal Datasets in HELM"},{"location":"dataset_integration_guide/#1-casehold-case-holdings-on-legal-decisions","text":"File: src/helm/benchmark/scenarios/casehold_scenario.py Dataset Source: from datasets import load_dataset dataset = load_dataset( \"casehold/casehold\", # HuggingFace dataset ID \"all\", # Configuration/subset cache_dir=data_path # Local cache: {output_path}/data ) HuggingFace: casehold/casehold Data Structure: - Task: Multiple choice QA (5 options) - Splits: train , test - Fields: - example_id - Unique identifier - citing_prompt - Context passage - holding_0 through holding_4 - Answer choices - label - Correct answer index (\"0\"-\"4\") Run Spec: enterprise_run_specs.py::get_casehold_spec() Adapter: Multiple Choice Joint Metrics: Exact Match","title":"1. CaseHOLD (Case Holdings On Legal Decisions)"},{"location":"dataset_integration_guide/#2-legalbench","text":"File: src/helm/benchmark/scenarios/legalbench_scenario.py Dataset Source: import datasets train_dataset = datasets.load_dataset( \"nguha/legalbench\", subset, # e.g., \"abercrombie\", \"corporate_lobbying\" trust_remote_code=True, cache_dir=cache_dir, split=\"train\", revision=\"e042ea68c19df12b737fe768572f22ead61e8e37\" # Pinned version ) HuggingFace: nguha/legalbench Supported Subsets: - abercrombie - Trademark classification - corporate_lobbying - Corporate disclosure analysis - international_citizenship_questions - Citizenship law QA - function_of_decision_section - Legal document structure - proa - Legal reasoning Additional Resources: # External prompt configuration PROMPT_SETTINGS_URL = \"https://raw.githubusercontent.com/HazyResearch/legalbench/main/helm_prompt_settings.jsonl\" ensure_file_downloaded( source_url=PROMPT_SETTINGS_URL, target_path=prompt_settings_path ) Run Spec: lite_run_specs.py::get_legalbench_spec(subset) Key Feature: Uses revision parameter for reproducibility","title":"2. LegalBench"},{"location":"dataset_integration_guide/#3-lexglue-legal-general-language-understanding","text":"File: src/helm/benchmark/scenarios/lex_glue_scenario.py Dataset Source: dataset = load_dataset( \"lex_glue\", config, # Task config: \"ecthr_a\", \"scotus\", \"case_hold\", etc. cache_dir=cache_dir ) HuggingFace: lex_glue Supported Tasks: Config Task Type Description ecthr_a Multi-label European Court of Human Rights (Articles) ecthr_b Multi-label European Court of Human Rights (Violations) scotus Single-label US Supreme Court topic classification eurlex Multi-label EU legislation classification ledgar Single-label Legal contract provisions unfair_tos Multi-label Terms of Service unfairness detection case_hold QA Legal case holding identification Run Spec: classic_run_specs.py::get_lex_glue_spec(subset) Dynamic Configuration: - Task type determines adapter (generation vs classification) - Max tokens varies by task (20-100) - Max train instances varies (0-5)","title":"3. LexGLUE (Legal General Language Understanding)"},{"location":"dataset_integration_guide/#4-legal-summarization-multi-dataset","text":"File: src/helm/benchmark/scenarios/legal_summarization_scenario.py Supports 3 Datasets:","title":"4. Legal Summarization (Multi-Dataset)"},{"location":"dataset_integration_guide/#a-billsum-us-congressional-bills","text":"dataset = load_dataset(\"billsum\", cache_dir=cache_dir) # Fields: text, summary HuggingFace: billsum Run Spec: classic_run_specs.py::get_billsum_legal_summarization_spec() Summary Length: 200-800 tokens","title":"A. BillSum (US Congressional Bills)"},{"location":"dataset_integration_guide/#b-multilexsum-legal-case-summaries","text":"dataset = load_dataset( \"allenai/multi_lexsum\", \"v20220616\", # Versioned config cache_dir=cache_dir ) # Fields: summary/long, summary/short HuggingFace: allenai/multi_lexsum Run Spec: classic_run_specs.py::get_multilexsum_legal_summarization_spec() Summary Length: 100-400 tokens","title":"B. MultiLexSum (Legal Case Summaries)"},{"location":"dataset_integration_guide/#c-eurlexsum-eu-legislation","text":"dataset = load_dataset( \"dennlinger/eur-lex-sum\", \"english\", # Language config cache_dir=cache_dir ) # Fields: reference, summary HuggingFace: dennlinger/eur-lex-sum Run Spec: classic_run_specs.py::get_eurlexsum_legal_summarization_spec() Summary Length: 400-1600 tokens","title":"C. EurLexSum (EU Legislation)"},{"location":"dataset_integration_guide/#5-legal-support-argumentative-reasoning","text":"File: src/helm/benchmark/scenarios/legal_support_scenario.py Dataset Source: Direct URL Download from helm.common.general import ensure_file_downloaded ensure_file_downloaded( source_url=\"https://docs.google.com/uc?export=download&id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77\", target_path=data_path, unpack=True, unpack_type=\"unzip\" ) # Reads local JSONL files: # - train.jsonl # - dev.jsonl # - test.jsonl Data Structure: { \"context\": \"Legal passage text...\", \"citation_a\": \"First citation with (parenthetical text)\", \"citation_b\": \"Second citation with (parenthetical text)\", \"label\": \"a\" // or \"b\" } Run Spec: classic_run_specs.py::get_legal_support_spec() Task: Binary multiple choice (which citation better supports the passage)","title":"5. Legal Support (Argumentative Reasoning)"},{"location":"dataset_integration_guide/#6-lextreme-multilingual-legal-nlu","text":"File: src/helm/benchmark/scenarios/lextreme_scenario.py Dataset Source: dataset = load_dataset( \"joelito/lextreme\", subset, # 18 different legal tasks cache_dir=cache_dir ) HuggingFace: joelito/lextreme 18 Legal Tasks across multiple languages (Portuguese, German, Greek, French, Italian, Romanian)","title":"6. LEXtreme (Multilingual Legal NLU)"},{"location":"dataset_integration_guide/#standard-dataset-integration-patterns","text":"","title":"Standard Dataset Integration Patterns"},{"location":"dataset_integration_guide/#pattern-1-huggingface-datasets-recommended","text":"Use When: Dataset is publicly available on HuggingFace Hub Template: from typing import List import os from datasets import load_dataset, DatasetDict from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists class Legal10ExampleScenario(Scenario): name = \"legal_10_example\" description = \"Legal-10 example dataset\" tags = [\"legal\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Step 1: Set up cache directory cache_dir = os.path.join(output_path, \"data\") ensure_directory_exists(cache_dir) # Step 2: Load dataset from HuggingFace dataset = load_dataset( \"legal-10/example-dataset\", # HuggingFace dataset ID \"default\", # Config/subset (optional) cache_dir=cache_dir, revision=\"main\" # Or specific commit hash ) # Step 3: Convert to HELM instances instances = [] for split_name in [\"train\", \"test\"]: helm_split = TRAIN_SPLIT if split_name == \"train\" else TEST_SPLIT for idx, example in enumerate(dataset[split_name]): instance = Instance( input=Input(text=example[\"input_text\"]), references=[ Reference( Output(text=example[\"answer\"]), tags=[CORRECT_TAG] ) ], split=helm_split, id=f\"{split_name}_{idx}\" ) instances.append(instance) return instances Advantages: - Automatic caching - Version control via revision parameter - Standardized interface - Easy data exploration on HuggingFace website","title":"Pattern 1: HuggingFace Datasets (Recommended)"},{"location":"dataset_integration_guide/#pattern-2-direct-url-download","text":"Use When: Dataset is hosted externally (not on HuggingFace) Template: import json from helm.common.general import ensure_file_downloaded class Legal10URLScenario(Scenario): name = \"legal_10_url_example\" description = \"Dataset from external URL\" tags = [\"legal\", \"legal_10\"] DATASET_URL = \"https://example.com/dataset.zip\" def get_instances(self, output_path: str) -> List[Instance]: # Step 1: Download and unpack data_path = os.path.join(output_path, \"data\") ensure_file_downloaded( source_url=self.DATASET_URL, target_path=data_path, unpack=True, unpack_type=\"zip\" # or \"tar\", \"unzstd\" ) # Step 2: Read local files instances = [] with open(os.path.join(data_path, \"train.jsonl\")) as f: for line in f: data = json.loads(line) instance = Instance( input=Input(text=data[\"question\"]), references=[ Reference(Output(text=data[\"answer\"]), tags=[CORRECT_TAG]) ], split=TRAIN_SPLIT, id=data[\"id\"] ) instances.append(instance) return instances Advantages: - Works with any HTTP/HTTPS URL - Automatic unpacking (zip, tar, tar.gz, zstd) - Local file caching","title":"Pattern 2: Direct URL Download"},{"location":"dataset_integration_guide/#pattern-3-multiple-choice-with-references","text":"Use When: Dataset has multiple answer choices (like CaseHOLD) Template: class Legal10MultipleChoiceScenario(Scenario): name = \"legal_10_mcqa\" description = \"Multiple choice QA\" tags = [\"legal\", \"question_answering\", \"legal_10\"] NUM_CHOICES = 4 # Number of answer options def get_instances(self, output_path: str) -> List[Instance]: cache_dir = os.path.join(output_path, \"data\") ensure_directory_exists(cache_dir) dataset = load_dataset(\"legal-10/mcqa\", cache_dir=cache_dir) instances = [] for example in dataset[\"test\"]: # Extract answer choices choices = [ example[\"choice_a\"], example[\"choice_b\"], example[\"choice_c\"], example[\"choice_d\"] ] correct_idx = int(example[\"label\"]) # 0-3 # Create references (one per choice) references = [ Reference( Output(text=choices[i]), tags=[CORRECT_TAG] if i == correct_idx else [] ) for i in range(self.NUM_CHOICES) ] instance = Instance( input=Input(text=example[\"question\"]), references=references, split=TEST_SPLIT, id=example[\"id\"] ) instances.append(instance) return instances Key Points: - One Reference per answer choice - Only correct answer has CORRECT_TAG - Used with ADAPT_MULTIPLE_CHOICE_JOINT adapter","title":"Pattern 3: Multiple Choice with References"},{"location":"dataset_integration_guide/#cache-directory-structure","text":"HELM uses a consistent caching pattern: benchmark_output/ \u2514\u2500\u2500 scenarios/ \u2514\u2500\u2500 legal_10_example/ \u251c\u2500\u2500 data/ # Dataset cache (HuggingFace or downloaded) \u2502 \u251c\u2500\u2500 downloads/ # Raw downloads \u2502 \u2514\u2500\u2500 legal-10___example/ # Processed HuggingFace cache \u251c\u2500\u2500 instances.json # Generated instances \u2514\u2500\u2500 scenario_state.json # Execution results Key Functions: from helm.common.general import ( ensure_directory_exists, # Create dir if not exists ensure_file_downloaded, # Download file with caching ) from helm.benchmark.scenarios.scenario import get_scenario_cache_path # Get standard cache path cache_path = get_scenario_cache_path(benchmark_output_path, \"legal_10_example\") # Returns: benchmark_output/scenarios/legal_10_example","title":"Cache Directory Structure"},{"location":"dataset_integration_guide/#reproducibility-best-practices","text":"","title":"Reproducibility Best Practices"},{"location":"dataset_integration_guide/#1-pin-dataset-versions","text":"HuggingFace: dataset = load_dataset( \"legal-10/dataset\", revision=\"abc123def456\" # Specific git commit hash ) Direct URLs: # Include version in filename or URL DATASET_URL = \"https://example.com/dataset-v1.2.3.zip\"","title":"1. Pin Dataset Versions"},{"location":"dataset_integration_guide/#2-document-data-source","text":"class Legal10Scenario(Scenario): \"\"\" Legal-10 Example Dataset Dataset repository: https://huggingface.co/datasets/legal-10/example Publication: Smith et al. (2024). \"Legal AI Benchmark.\" ICAIL. Data content: The dataset consists of X legal questions with Y answer choices. Questions are derived from Z legal domain. \"\"\" name = \"legal_10_example\"","title":"2. Document Data Source"},{"location":"dataset_integration_guide/#3-handle-missing-data-gracefully","text":"def get_instances(self, output_path: str) -> List[Instance]: try: dataset = load_dataset(\"legal-10/dataset\", cache_dir=cache_dir) except Exception as e: raise ValueError( f\"Failed to load dataset 'legal-10/dataset'. \" f\"Ensure you have internet connection and HuggingFace access. Error: {e}\" )","title":"3. Handle Missing Data Gracefully"},{"location":"dataset_integration_guide/#legal-10-dataset-requirements","text":"For Legal-10 benchmark integration, each skill needs:","title":"Legal-10 Dataset Requirements"},{"location":"dataset_integration_guide/#skill-datasets","text":"Skill Dataset Preferred Source Status S1-S2 LegalAgentBench HuggingFace or URL To be uploaded S3-S4 CLERC HuggingFace or URL To be uploaded S5 KeyCite-CLERC HuggingFace or URL To be uploaded S6 CUAD HuggingFace: cuad Available S7 CaseHOLD HuggingFace: casehold/casehold \u2705 Integrated S8 LEXam HuggingFace or URL To be uploaded S9 Dahl 10-types HuggingFace or URL To be uploaded S10 SHIELD HuggingFace or URL To be uploaded","title":"Skill Datasets"},{"location":"dataset_integration_guide/#dataset-upload-checklist","text":"For HuggingFace hosting: [ ] Create HuggingFace dataset repository [ ] Upload data files (train/test/validation splits) [ ] Write dataset card (README.md) with: [ ] Description and citation [ ] Data structure documentation [ ] Field descriptions [ ] Licensing information [ ] Add dataset loading script (if custom format) [ ] Test loading: load_dataset(\"legal-10/dataset-name\") [ ] Pin revision for reproducibility","title":"Dataset Upload Checklist"},{"location":"dataset_integration_guide/#testing-dataset-integration","text":"","title":"Testing Dataset Integration"},{"location":"dataset_integration_guide/#1-test-loading","text":"# Test in Python REPL from datasets import load_dataset dataset = load_dataset(\"legal-10/example\") print(dataset) print(dataset[\"train\"][0]) # First example","title":"1. Test Loading"},{"location":"dataset_integration_guide/#2-test-scenario","text":"# Run scenario in isolation from helm.benchmark.scenarios.legal_10_example_scenario import Legal10ExampleScenario scenario = Legal10ExampleScenario() instances = scenario.get_instances(\"test_output\") print(f\"Loaded {len(instances)} instances\") print(instances[0])","title":"2. Test Scenario"},{"location":"dataset_integration_guide/#3-test-run-spec","text":"# Run small evaluation helm-run \\ --run-specs legal_10_example \\ --max-eval-instances 5 \\ --output-path test_output","title":"3. Test Run Spec"},{"location":"dataset_integration_guide/#common-issues-solutions","text":"","title":"Common Issues &amp; Solutions"},{"location":"dataset_integration_guide/#issue-1-dataset-not-found","text":"Error: FileNotFoundError or DatasetNotFoundError Solutions: - Verify dataset ID is correct - Check HuggingFace dataset is public - Ensure internet connection - Try with trust_remote_code=True if dataset has custom script","title":"Issue 1: Dataset Not Found"},{"location":"dataset_integration_guide/#issue-2-cache-permission-errors","text":"Error: PermissionError when writing to cache Solutions: - Ensure output_path directory is writable - Check disk space - Use ensure_directory_exists() before writing","title":"Issue 2: Cache Permission Errors"},{"location":"dataset_integration_guide/#issue-3-version-mismatch","text":"Error: Dataset structure changed from expected format Solutions: - Pin revision parameter to specific commit - Document expected dataset version in scenario docstring - Add validation in get_instances()","title":"Issue 3: Version Mismatch"},{"location":"dataset_integration_guide/#summary","text":"HELM's dataset integration follows a consistent, modular pattern : Scenarios load data in get_instances() method HuggingFace is preferred for standardization and caching Direct URLs work for externally hosted datasets Caching is automatic in {output_path}/data/ directory Reproducibility via revision pinning and version documentation For Legal-10: - Upload datasets to HuggingFace under legal-10/ organization - Follow existing legal scenario patterns (CaseHOLD, LegalBench) - Use revision parameter for all datasets - Document data sources and citations clearly","title":"Summary"},{"location":"design_principles/","text":"Legal-10 Design Principles Overview Legal-10 is a skill-based benchmark for evaluating legal AI systems. This document describes the design principles that guide its development. Design Principles 1. Objectivity Fixed Specification - Each version has a fixed specification - Changes to the specification require a version increment - Evaluations specify which version was used Complete Evaluation - All 10 skills must be evaluated - No partial scores or selective omission - If a skill cannot be evaluated, this is explicitly noted Versioning - Version numbers follow semantic versioning - Breaking changes increment major version - Specification changes are documented in changelog 2. Transparency Open Source - Evaluation harness is open source - Datasets are publicly accessible or documented - Scoring implementations are visible - Run specifications are published Reproducibility - Evaluations include SHA-256 hashes - Dataset versions are pinned - Full run bundles are preserved - Evaluation conditions are documented Public Logging - Submissions are logged with timestamps - Historical performance is preserved - Append-only log prevents silent deletions 3. Fairness Standardized Evaluation - All models use identical evaluation protocol - Same prompts, same datasets, same metrics - No model-specific optimizations in core harness Modality-Matched Testing - Closed-book tasks (S7-S10): No retrieval allowed - RAG tasks (S3-S6): Retrieval capability tested - Agentic tasks (S1-S2): Multi-step reasoning evaluated - Testing matches intended use patterns Comparable Baselines - Open-weight models use same harness as API models - Local deployments use same evaluation as hosted services 4. Gaming Resistance No Selective Omission - Cannot cherry-pick favorable tasks - All 10 skills required for score reporting - Partial evaluation results are not published Granular Reporting - Skill-level scores reported individually - Aggregate score does not hide weak performance - Per-instance results available for analysis Multiple Evaluation Axes - Tests across different modalities (CB/RAG/AG) - Multiple metrics per skill where appropriate - Cannot optimize for single metric Professional Standards Grounding Legal-10 skills map to established professional competency frameworks: MacCrate Report (ABA, 1992) - Foundational lawyering skills framework - Research planning, fact-finding, legal analysis AALL Principles (2013) - Legal research competency standards - Authority identification and validation Shultz & Zedeck (2011) - Empirically-derived lawyer effectiveness factors - Strategic planning, practical judgment These frameworks represent 30+ years of validated professional standards. Skill Definitions Legal-10 evaluates 10 distinct skills: Skill Name Modality S1 Research Planning Agentic (AG) S2 Strategic Stopping Agentic (AG) S3 Known Authority RAG S4 Unknown Authority RAG S5 Validate Authority RAG S6 Fact Extraction RAG S7 Distinguish Cases Closed-Book (CB) S8 Synthesize Results Closed-Book (CB) S9 Citation Integrity Closed-Book (CB) S10 Copyright Compliance Closed-Book (CB) Each skill is tested using the modality that most closely matches how human lawyers perform that skill in practice. Scoreboard Policy Participation Evaluation is open to any model Participation is voluntary Vendors may submit their own evaluations Community members may evaluate any accessible model \"Evaluated\" List Systems that have completed full evaluation (all 10 skills) are listed with: - Model identifier - Evaluation date - Version tested - Link to run bundle \"Not Yet Evaluated\" List Systems that have not been evaluated are listed neutrally with status tags: - \"No public endpoint\" - Model not publicly accessible - \"Access restricted\" - Requires special access or licensing - \"Awaiting community submission\" - Public but not yet evaluated This ensures visibility of coverage gaps. What Legal-10 Measures Behavioral Performance - How models respond to standardized legal tasks - Performance across 10 professional skill categories - Comparative capabilities under identical conditions Skill-Specific Capabilities - Research planning quality - Authority identification accuracy - Citation integrity - Fact extraction precision - Legal reasoning capability What Legal-10 Does Not Measure Architectural Parameters - Model size, precision, or quantization - Training data composition or provenance - Deployment infrastructure - Version or update history Production Reliability - Real-world usage patterns - Domain-specific edge cases - Integration with specific workflows - Long-term consistency Fitness for Specific Use Cases - Deployment safety for particular applications - Compliance with jurisdiction-specific requirements - Appropriateness for high-stakes decisions Limitations Legal-10 has known limitations: Vendor Architecture Opacity - Cannot verify model parameters vendors claim - Cannot detect silent model version changes - Cannot inspect training data - Cannot confirm deployment configuration Test Coverage - Tests standardized workflows, not all possible use patterns - Cannot evaluate vendor-specific features - Cannot test every prompt engineering approach - Evaluates API access, not internal capabilities Deployment Context - Benchmark performance \u2260 production performance - Controlled evaluation \u2260 real-world reliability - Standardized prompts \u2260 expert usage Professional Judgment - Scores inform decisions, do not make them - Domain expertise required for fitness assessment - Ethical obligations remain with deployers Technical Implementation Framework Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models): Unified model interface for open and closed models Standardized adapters for consistent prompting Reproducible evaluation with auditable run bundles Open-source implementation Dataset Sources Datasets are either: - Publicly available on HuggingFace with documented IDs - Downloadable from documented URLs with version hashes - Reproducible from published sources with clear methodology Evaluation Process Model receives standardized prompt Response is generated under specified conditions Output is scored using defined metrics Results are aggregated across instances Skill-level and aggregate scores are reported Governance Community Governance - Not controlled by any single vendor - Contributors receive co-author credit on publications - Community members have governance voting rights - Transparent decision-making process Change Process - Specification changes are proposed publicly - Community review period before adoption - Breaking changes require major version increment - All changes documented in changelog Use Cases Legal-10 is designed to support: Comparative Evaluation - Fair comparison across different models - Standardized measurement of legal capabilities - Identification of relative strengths and weaknesses Quality Assessment - Skill-specific performance visibility - Detection of capability gaps - Baseline competency verification Informed Decision-Making - Input for deployment decisions (not substitute for testing) - Risk assessment for legal AI adoption - Vendor claim verification (within limitations) Research - Reproducible benchmarking for academic studies - Tracking of legal AI capability development - Identification of areas needing improvement What Users Should Know Legal-10 Provides: - Standardized comparison under identical conditions - Skill-level performance visibility - Grounding in professional standards - Open, auditable evaluation methodology Legal-10 Does Not Provide: - Certification of deployment safety - Guarantee of production reliability - Replacement for domain-specific testing - Verification of vendor architectural claims Appropriate Use: - As one input among many for deployment decisions - To identify areas requiring additional testing - To compare relative capabilities under controlled conditions - To assess baseline competency across professional skills Inappropriate Use: - As sole basis for high-stakes deployment - As substitute for professional judgment - As certification of legal compliance - As verification of vendor claims about architecture Relationship to Other Benchmarks Legal-10 differs from some existing AI benchmarks in specific ways: Participation Model - Legal-10: Open evaluation, anyone can run - Some benchmarks: Vendor opt-in required Result Reporting - Legal-10: Append-only log, historical performance preserved - Some benchmarks: Results may be withdrawn or updated Task Selection - Legal-10: All 10 skills required - Some benchmarks: May allow selective task participation Methodology Transparency - Legal-10: Open-source harness, public datasets - Some benchmarks: May use proprietary test sets These are factual differences, not claims of superiority. Future Development Legal-10 is designed to evolve: Potential Extensions - Multilingual legal reasoning (in development) - Additional professional skills - Cross-jurisdictional evaluation - Specialized legal domain testing Community Input - Suggestions for new skills - Dataset contributions - Metric improvements - Framework enhancements Version Progression - Regular updates to datasets - Metric refinements based on empirical analysis - New skills as legal AI capabilities expand Citation When referencing Legal-10, please include: - Benchmark name and version - Evaluation date - Link to this documentation - Dataset sources used Legal-10 is an open benchmark. We welcome scrutiny, replication, and improvement suggestions.","title":"Legal-10 Design Principles"},{"location":"design_principles/#legal-10-design-principles","text":"","title":"Legal-10 Design Principles"},{"location":"design_principles/#overview","text":"Legal-10 is a skill-based benchmark for evaluating legal AI systems. This document describes the design principles that guide its development.","title":"Overview"},{"location":"design_principles/#design-principles","text":"","title":"Design Principles"},{"location":"design_principles/#1-objectivity","text":"Fixed Specification - Each version has a fixed specification - Changes to the specification require a version increment - Evaluations specify which version was used Complete Evaluation - All 10 skills must be evaluated - No partial scores or selective omission - If a skill cannot be evaluated, this is explicitly noted Versioning - Version numbers follow semantic versioning - Breaking changes increment major version - Specification changes are documented in changelog","title":"1. Objectivity"},{"location":"design_principles/#2-transparency","text":"Open Source - Evaluation harness is open source - Datasets are publicly accessible or documented - Scoring implementations are visible - Run specifications are published Reproducibility - Evaluations include SHA-256 hashes - Dataset versions are pinned - Full run bundles are preserved - Evaluation conditions are documented Public Logging - Submissions are logged with timestamps - Historical performance is preserved - Append-only log prevents silent deletions","title":"2. Transparency"},{"location":"design_principles/#3-fairness","text":"Standardized Evaluation - All models use identical evaluation protocol - Same prompts, same datasets, same metrics - No model-specific optimizations in core harness Modality-Matched Testing - Closed-book tasks (S7-S10): No retrieval allowed - RAG tasks (S3-S6): Retrieval capability tested - Agentic tasks (S1-S2): Multi-step reasoning evaluated - Testing matches intended use patterns Comparable Baselines - Open-weight models use same harness as API models - Local deployments use same evaluation as hosted services","title":"3. Fairness"},{"location":"design_principles/#4-gaming-resistance","text":"No Selective Omission - Cannot cherry-pick favorable tasks - All 10 skills required for score reporting - Partial evaluation results are not published Granular Reporting - Skill-level scores reported individually - Aggregate score does not hide weak performance - Per-instance results available for analysis Multiple Evaluation Axes - Tests across different modalities (CB/RAG/AG) - Multiple metrics per skill where appropriate - Cannot optimize for single metric","title":"4. Gaming Resistance"},{"location":"design_principles/#professional-standards-grounding","text":"Legal-10 skills map to established professional competency frameworks: MacCrate Report (ABA, 1992) - Foundational lawyering skills framework - Research planning, fact-finding, legal analysis AALL Principles (2013) - Legal research competency standards - Authority identification and validation Shultz & Zedeck (2011) - Empirically-derived lawyer effectiveness factors - Strategic planning, practical judgment These frameworks represent 30+ years of validated professional standards.","title":"Professional Standards Grounding"},{"location":"design_principles/#skill-definitions","text":"Legal-10 evaluates 10 distinct skills: Skill Name Modality S1 Research Planning Agentic (AG) S2 Strategic Stopping Agentic (AG) S3 Known Authority RAG S4 Unknown Authority RAG S5 Validate Authority RAG S6 Fact Extraction RAG S7 Distinguish Cases Closed-Book (CB) S8 Synthesize Results Closed-Book (CB) S9 Citation Integrity Closed-Book (CB) S10 Copyright Compliance Closed-Book (CB) Each skill is tested using the modality that most closely matches how human lawyers perform that skill in practice.","title":"Skill Definitions"},{"location":"design_principles/#scoreboard-policy","text":"","title":"Scoreboard Policy"},{"location":"design_principles/#participation","text":"Evaluation is open to any model Participation is voluntary Vendors may submit their own evaluations Community members may evaluate any accessible model","title":"Participation"},{"location":"design_principles/#evaluated-list","text":"Systems that have completed full evaluation (all 10 skills) are listed with: - Model identifier - Evaluation date - Version tested - Link to run bundle","title":"\"Evaluated\" List"},{"location":"design_principles/#not-yet-evaluated-list","text":"Systems that have not been evaluated are listed neutrally with status tags: - \"No public endpoint\" - Model not publicly accessible - \"Access restricted\" - Requires special access or licensing - \"Awaiting community submission\" - Public but not yet evaluated This ensures visibility of coverage gaps.","title":"\"Not Yet Evaluated\" List"},{"location":"design_principles/#what-legal-10-measures","text":"Behavioral Performance - How models respond to standardized legal tasks - Performance across 10 professional skill categories - Comparative capabilities under identical conditions Skill-Specific Capabilities - Research planning quality - Authority identification accuracy - Citation integrity - Fact extraction precision - Legal reasoning capability","title":"What Legal-10 Measures"},{"location":"design_principles/#what-legal-10-does-not-measure","text":"Architectural Parameters - Model size, precision, or quantization - Training data composition or provenance - Deployment infrastructure - Version or update history Production Reliability - Real-world usage patterns - Domain-specific edge cases - Integration with specific workflows - Long-term consistency Fitness for Specific Use Cases - Deployment safety for particular applications - Compliance with jurisdiction-specific requirements - Appropriateness for high-stakes decisions","title":"What Legal-10 Does Not Measure"},{"location":"design_principles/#limitations","text":"Legal-10 has known limitations: Vendor Architecture Opacity - Cannot verify model parameters vendors claim - Cannot detect silent model version changes - Cannot inspect training data - Cannot confirm deployment configuration Test Coverage - Tests standardized workflows, not all possible use patterns - Cannot evaluate vendor-specific features - Cannot test every prompt engineering approach - Evaluates API access, not internal capabilities Deployment Context - Benchmark performance \u2260 production performance - Controlled evaluation \u2260 real-world reliability - Standardized prompts \u2260 expert usage Professional Judgment - Scores inform decisions, do not make them - Domain expertise required for fitness assessment - Ethical obligations remain with deployers","title":"Limitations"},{"location":"design_principles/#technical-implementation","text":"","title":"Technical Implementation"},{"location":"design_principles/#framework","text":"Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models): Unified model interface for open and closed models Standardized adapters for consistent prompting Reproducible evaluation with auditable run bundles Open-source implementation","title":"Framework"},{"location":"design_principles/#dataset-sources","text":"Datasets are either: - Publicly available on HuggingFace with documented IDs - Downloadable from documented URLs with version hashes - Reproducible from published sources with clear methodology","title":"Dataset Sources"},{"location":"design_principles/#evaluation-process","text":"Model receives standardized prompt Response is generated under specified conditions Output is scored using defined metrics Results are aggregated across instances Skill-level and aggregate scores are reported","title":"Evaluation Process"},{"location":"design_principles/#governance","text":"Community Governance - Not controlled by any single vendor - Contributors receive co-author credit on publications - Community members have governance voting rights - Transparent decision-making process Change Process - Specification changes are proposed publicly - Community review period before adoption - Breaking changes require major version increment - All changes documented in changelog","title":"Governance"},{"location":"design_principles/#use-cases","text":"Legal-10 is designed to support: Comparative Evaluation - Fair comparison across different models - Standardized measurement of legal capabilities - Identification of relative strengths and weaknesses Quality Assessment - Skill-specific performance visibility - Detection of capability gaps - Baseline competency verification Informed Decision-Making - Input for deployment decisions (not substitute for testing) - Risk assessment for legal AI adoption - Vendor claim verification (within limitations) Research - Reproducible benchmarking for academic studies - Tracking of legal AI capability development - Identification of areas needing improvement","title":"Use Cases"},{"location":"design_principles/#what-users-should-know","text":"Legal-10 Provides: - Standardized comparison under identical conditions - Skill-level performance visibility - Grounding in professional standards - Open, auditable evaluation methodology Legal-10 Does Not Provide: - Certification of deployment safety - Guarantee of production reliability - Replacement for domain-specific testing - Verification of vendor architectural claims Appropriate Use: - As one input among many for deployment decisions - To identify areas requiring additional testing - To compare relative capabilities under controlled conditions - To assess baseline competency across professional skills Inappropriate Use: - As sole basis for high-stakes deployment - As substitute for professional judgment - As certification of legal compliance - As verification of vendor claims about architecture","title":"What Users Should Know"},{"location":"design_principles/#relationship-to-other-benchmarks","text":"Legal-10 differs from some existing AI benchmarks in specific ways: Participation Model - Legal-10: Open evaluation, anyone can run - Some benchmarks: Vendor opt-in required Result Reporting - Legal-10: Append-only log, historical performance preserved - Some benchmarks: Results may be withdrawn or updated Task Selection - Legal-10: All 10 skills required - Some benchmarks: May allow selective task participation Methodology Transparency - Legal-10: Open-source harness, public datasets - Some benchmarks: May use proprietary test sets These are factual differences, not claims of superiority.","title":"Relationship to Other Benchmarks"},{"location":"design_principles/#future-development","text":"Legal-10 is designed to evolve: Potential Extensions - Multilingual legal reasoning (in development) - Additional professional skills - Cross-jurisdictional evaluation - Specialized legal domain testing Community Input - Suggestions for new skills - Dataset contributions - Metric improvements - Framework enhancements Version Progression - Regular updates to datasets - Metric refinements based on empirical analysis - New skills as legal AI capabilities expand","title":"Future Development"},{"location":"design_principles/#citation","text":"When referencing Legal-10, please include: - Benchmark name and version - Evaluation date - Link to this documentation - Dataset sources used Legal-10 is an open benchmark. We welcome scrutiny, replication, and improvement suggestions.","title":"Citation"},{"location":"developer_adding_new_models/","text":"Adding New Clients Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! Overview of the process To add a new model you need to define 3 objects: * a ModelMetadata objects that defines properties of your model (name, metadata, capabilities, ...). * one or several ModelDeployment which defines how to query a model (mainly by providing a Client , a WindowService and a Tokenizer ). You can define several deployments for a single model ( local/your-model , huggingface/your-model , together/your-model , ...). * a TokenizerConfig which defines how to build the Tokenizer (mainly by providing a TokenizerSpec ). In some cases you might have to define additionally: * a Client if your query method differ from any clients we have implemented. This will be then referenced in the ModelDeployment . We recommend checking HTTPModelClient and HuggingFaceClient which can be used in a lot of cases. If you identify the need for a new client, a good starting to point is to have a look at SimpleClient . * a WindowService . First have a look at DefaultWindowService to check if this is not enough for your use case. If you need you own truncate_from_right function, then you might need to create your own WindowService . In that case, a good starting point is to have a look at YaLMWindowService . Where to create the objects There are two cases: private models that should only be accessible to you and models not yet supported by HELM but that would benefit everyone if added. In the first case, you should create the files model_deployments.yaml , model_metadata.yaml and tokenizer_configs.yaml in prod_env/ (A folder that you should create at the root of the repo if not already done). HELM will automatically registed any model defined in these files without any change in the code while ignoring them on Github which can be convenient for you. Then you can simply duplicate the corresponding files from src/helm/config , delete the models and add yours. Follow the next section for an example. In the second case, if you want to add a model to HELM, you can directly do it in src/helm/config . You can then open a Pull Request on Github to share the model. When you do, make sure to: * Include any link justifying the metadata used in ModelMetadata such as the release data, number of parameters, capabilities and so on (you should not infer anything). * Check that you are respecting the format used in those files ( ModelMetadata should be named as <CREATOR-ORGANIZATION>/<MODEL-NAME> and the ModelDeployment should be named as <HOST-ORGANIZATION>/<MODEL-NAME> , for example ModelMetadata : openai/gpt2 and ModelDeployment : huggingface/gpt2 ). Add the appropriate comments and so on. * Run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=<YOUR-DEPLOYMENT>\" --suite v1 --max-eval-instances 10 and make sure that everything works. Include the logs from the terminal in your PR. * Not create unnecessary objects ( Client TokenizerCOnfig , WindowService ) and if you have to create one of these objects, document in your PR why you had to. Make them general enough so that they could be re-used by other models (especially the Client ). Example In src/helm/config/model_metadata.yaml : # [...] models: - name: simple/model1 [...] # NEW MODEL STARTS HERE - name: simple/tutorial display_name: Tutorial Model description: This is a simple model used in the tutorial. creator_organization_name: Helm access: open release_date: 2023-01-01 tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG] [...] In src/helm/config/model_deployments.yaml : # [...] model_deployments: - name: simple/model1 [...] - name: simple/tutorial model_name: simple/tutorial tokenizer_name: simple/model1 max_sequence_length: 2048 client_spec: class_name: \"helm.clients.simple_client.SimpleClient\" args: {} window_service_spec: class_name: \"helm.benchmark.window_services.openai_window_service.OpenAIWindowService\" args: {} [...] We won't be adding any TokenizerConfig here as we are reusing simple/model1 . This shows a good practice when adding a new model, always check if the correct tokenizer does not already exists. You should now be able to run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=simple/tutorial\" --suite v1 --max-eval-instances 10 without any error.","title":"Adding New Clients"},{"location":"developer_adding_new_models/#adding-new-clients","text":"Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution!","title":"Adding New Clients"},{"location":"developer_adding_new_models/#overview-of-the-process","text":"To add a new model you need to define 3 objects: * a ModelMetadata objects that defines properties of your model (name, metadata, capabilities, ...). * one or several ModelDeployment which defines how to query a model (mainly by providing a Client , a WindowService and a Tokenizer ). You can define several deployments for a single model ( local/your-model , huggingface/your-model , together/your-model , ...). * a TokenizerConfig which defines how to build the Tokenizer (mainly by providing a TokenizerSpec ). In some cases you might have to define additionally: * a Client if your query method differ from any clients we have implemented. This will be then referenced in the ModelDeployment . We recommend checking HTTPModelClient and HuggingFaceClient which can be used in a lot of cases. If you identify the need for a new client, a good starting to point is to have a look at SimpleClient . * a WindowService . First have a look at DefaultWindowService to check if this is not enough for your use case. If you need you own truncate_from_right function, then you might need to create your own WindowService . In that case, a good starting point is to have a look at YaLMWindowService .","title":"Overview of the process"},{"location":"developer_adding_new_models/#where-to-create-the-objects","text":"There are two cases: private models that should only be accessible to you and models not yet supported by HELM but that would benefit everyone if added. In the first case, you should create the files model_deployments.yaml , model_metadata.yaml and tokenizer_configs.yaml in prod_env/ (A folder that you should create at the root of the repo if not already done). HELM will automatically registed any model defined in these files without any change in the code while ignoring them on Github which can be convenient for you. Then you can simply duplicate the corresponding files from src/helm/config , delete the models and add yours. Follow the next section for an example. In the second case, if you want to add a model to HELM, you can directly do it in src/helm/config . You can then open a Pull Request on Github to share the model. When you do, make sure to: * Include any link justifying the metadata used in ModelMetadata such as the release data, number of parameters, capabilities and so on (you should not infer anything). * Check that you are respecting the format used in those files ( ModelMetadata should be named as <CREATOR-ORGANIZATION>/<MODEL-NAME> and the ModelDeployment should be named as <HOST-ORGANIZATION>/<MODEL-NAME> , for example ModelMetadata : openai/gpt2 and ModelDeployment : huggingface/gpt2 ). Add the appropriate comments and so on. * Run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=<YOUR-DEPLOYMENT>\" --suite v1 --max-eval-instances 10 and make sure that everything works. Include the logs from the terminal in your PR. * Not create unnecessary objects ( Client TokenizerCOnfig , WindowService ) and if you have to create one of these objects, document in your PR why you had to. Make them general enough so that they could be re-used by other models (especially the Client ).","title":"Where to create the objects"},{"location":"developer_adding_new_models/#example","text":"In src/helm/config/model_metadata.yaml : # [...] models: - name: simple/model1 [...] # NEW MODEL STARTS HERE - name: simple/tutorial display_name: Tutorial Model description: This is a simple model used in the tutorial. creator_organization_name: Helm access: open release_date: 2023-01-01 tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG] [...] In src/helm/config/model_deployments.yaml : # [...] model_deployments: - name: simple/model1 [...] - name: simple/tutorial model_name: simple/tutorial tokenizer_name: simple/model1 max_sequence_length: 2048 client_spec: class_name: \"helm.clients.simple_client.SimpleClient\" args: {} window_service_spec: class_name: \"helm.benchmark.window_services.openai_window_service.OpenAIWindowService\" args: {} [...] We won't be adding any TokenizerConfig here as we are reusing simple/model1 . This shows a good practice when adding a new model, always check if the correct tokenizer does not already exists. You should now be able to run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=simple/tutorial\" --suite v1 --max-eval-instances 10 without any error.","title":"Example"},{"location":"developer_guide/","text":"Developer Guide This guide provides documentation for developers extending and contributing to Legal-10 Benchmark. Getting Started Developer Setup - Set up your development environment, testing, and Git workflow Contributing Adding New Models - How to add model metadata, deployments, and clients Editing Documentation - MkDocs setup and documentation contribution process","title":"Developer Guide"},{"location":"developer_guide/#developer-guide","text":"This guide provides documentation for developers extending and contributing to Legal-10 Benchmark.","title":"Developer Guide"},{"location":"developer_guide/#getting-started","text":"Developer Setup - Set up your development environment, testing, and Git workflow","title":"Getting Started"},{"location":"developer_guide/#contributing","text":"Adding New Models - How to add model metadata, deployments, and clients Editing Documentation - MkDocs setup and documentation contribution process","title":"Contributing"},{"location":"developer_setup/","text":"Developer Setup Check your system Python version Check your system verison of Python by running: python --version If your version of Python is older than 3.10, you must use either Conda or pyenv to install a version of Python >=3.10 when setting up your virtual environment. Set up the Python virtual environment First, create a Python virtual environment with Python version >= 3.10 and activate it. Using Virtualenv ( requires system Python version >=3.10): # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3 venv # Activate the virtual environment. # Run this every time you open your shell. source venv/bin/activate Using Conda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. # Run this every time you open your shell. conda activate crfm-helm Using pyenv and pyenv-virtualenv : # Create a virtual environment. # Only run this the first time. pyenv virtualenv 3.10 crfm-helm # Activate the virtual environment. # Run this every time you open your shell. pyenv activate crfm-helm Install Python dependencies To install any dependencies: pip install --force-reinstall -e .[dev] Run Python tests Currently, running all the unit tests takes about 10 minutes. To run all unit tests: python -m pytest Append -vv to output the full diff and results: python -m pytest -vv When modifying the Python code, you usually want to only run certain relevant tests. To run a specific test file, specify the file path as follows: python -m pytest path/to/test_file.py -vv Run linter and type-checker You should always ensure that your code is linted and type-checked before creating a pull request. This is typically enforced by our git pre-commit hooks. Install the pre-commit hooks by running: pre-commit install This will automatically run the linter and type-checker whenever you run git push to push a branch. To skip running the linter and type checker when pushing a branch, use the --no-verify flag with git push . To run the linter and type-checker manually: ./pre-commit.sh Alternatively, you can run only the linter or only the type checker separately: # Linters black src scripts flake8 src scripts # Type checker mypy src scripts Executing helm commands with local modifications The recommended way to execute helm-run , helm-summarize , helm-server , etc, with your local version of the repository is to do an editable install, using the following steps: Activate your virtual environment. Change directory to the repository root (contains pyproject.toml). Make sure you don't have an existing helm installation for that environment with pip uninstall crfm-helm Run pip install -e . Now calling helm-run while the environment is activated will read from your local source. Without installing If you have a compelling reason not to do an editable install, you can execute commands by: Change directory to src Execute the module you want with a command like: python -m helm.benchmark.run Checking in code The HELM repository does not allow direct modifications of the main branch. Instead, developers create a Pull Request which must then be approved by a different person before merging into main. Here is an example workflow: git checkout main to start from the main branch. git pull origin main to get up to date. Make whatever changes you'll like to group into a single review. Run tests. Make a new branch with git checkout -b <your-handle>/<change-identifier . For example, yifanmai/fix-optional-suggestions . If you did NOT install the precommit, run the linter and type checker with ./pre-commit.sh git commit -a to commit all you changes. If you want to ignore precommit warnings, you can add --no-verify . git push origin <your-handle>/<change-identifier> to upload to github. Loading any HELM github page should now prompt you about creating a new pull request. If not, you can also find your branch on the branches page to create one. Update the title and description as necessary, then create the pull request. Once the reviewer is satisfied, they can approve and either of you can then Squash and Merge the branch into main.","title":"Developer Setup"},{"location":"developer_setup/#developer-setup","text":"","title":"Developer Setup"},{"location":"developer_setup/#check-your-system-python-version","text":"Check your system verison of Python by running: python --version If your version of Python is older than 3.10, you must use either Conda or pyenv to install a version of Python >=3.10 when setting up your virtual environment.","title":"Check your system Python version"},{"location":"developer_setup/#set-up-the-python-virtual-environment","text":"First, create a Python virtual environment with Python version >= 3.10 and activate it. Using Virtualenv ( requires system Python version >=3.10): # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3 venv # Activate the virtual environment. # Run this every time you open your shell. source venv/bin/activate Using Conda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. # Run this every time you open your shell. conda activate crfm-helm Using pyenv and pyenv-virtualenv : # Create a virtual environment. # Only run this the first time. pyenv virtualenv 3.10 crfm-helm # Activate the virtual environment. # Run this every time you open your shell. pyenv activate crfm-helm","title":"Set up the Python virtual environment"},{"location":"developer_setup/#install-python-dependencies","text":"To install any dependencies: pip install --force-reinstall -e .[dev]","title":"Install Python dependencies"},{"location":"developer_setup/#run-python-tests","text":"Currently, running all the unit tests takes about 10 minutes. To run all unit tests: python -m pytest Append -vv to output the full diff and results: python -m pytest -vv When modifying the Python code, you usually want to only run certain relevant tests. To run a specific test file, specify the file path as follows: python -m pytest path/to/test_file.py -vv","title":"Run Python tests"},{"location":"developer_setup/#run-linter-and-type-checker","text":"You should always ensure that your code is linted and type-checked before creating a pull request. This is typically enforced by our git pre-commit hooks. Install the pre-commit hooks by running: pre-commit install This will automatically run the linter and type-checker whenever you run git push to push a branch. To skip running the linter and type checker when pushing a branch, use the --no-verify flag with git push . To run the linter and type-checker manually: ./pre-commit.sh Alternatively, you can run only the linter or only the type checker separately: # Linters black src scripts flake8 src scripts # Type checker mypy src scripts","title":"Run linter and type-checker"},{"location":"developer_setup/#executing-helm-commands-with-local-modifications","text":"The recommended way to execute helm-run , helm-summarize , helm-server , etc, with your local version of the repository is to do an editable install, using the following steps: Activate your virtual environment. Change directory to the repository root (contains pyproject.toml). Make sure you don't have an existing helm installation for that environment with pip uninstall crfm-helm Run pip install -e . Now calling helm-run while the environment is activated will read from your local source.","title":"Executing helm commands with local modifications"},{"location":"developer_setup/#without-installing","text":"If you have a compelling reason not to do an editable install, you can execute commands by: Change directory to src Execute the module you want with a command like: python -m helm.benchmark.run","title":"Without installing"},{"location":"developer_setup/#checking-in-code","text":"The HELM repository does not allow direct modifications of the main branch. Instead, developers create a Pull Request which must then be approved by a different person before merging into main. Here is an example workflow: git checkout main to start from the main branch. git pull origin main to get up to date. Make whatever changes you'll like to group into a single review. Run tests. Make a new branch with git checkout -b <your-handle>/<change-identifier . For example, yifanmai/fix-optional-suggestions . If you did NOT install the precommit, run the linter and type checker with ./pre-commit.sh git commit -a to commit all you changes. If you want to ignore precommit warnings, you can add --no-verify . git push origin <your-handle>/<change-identifier> to upload to github. Loading any HELM github page should now prompt you about creating a new pull request. If not, you can also find your branch on the branches page to create one. Update the title and description as necessary, then create the pull request. Once the reviewer is satisfied, they can approve and either of you can then Squash and Merge the branch into main.","title":"Checking in code"},{"location":"editing_documentation/","text":"Editing Documentation The documentation that you are reading now is an invaluable resource for newcomers and experienced users alike. Contributions to the documentation are very welcome. We currently use the MkDocs as our static site generator and ReadTheDocs as our web host. To edit the documentation, first clone the repository locally, then install HELM from the repository by following the Developer Setup instructions. After that, install the MkDocs dependencies by running the following from the root of the repository: pip install -r docs/requirements.txt You should now be able to run MkDocs from the root of the repository: mkdocs serve Then navigate to http://localhost:8000/ to view your locally-built documentation. The source Markdown files for the documentation are stored in the docs/ folder. By default, MkDocs watches the source directories for changes and automatically re-renders the web pages when it detects changes. If you are creating a new page, you should add your page to the nav section in mkdocs.yml . This will add your page to the table of contents in the side menu. We make heavy use of plugins and macros for auto-generating documentation from code and docstrings. For more information, please refer to the documentation for these plugins e.g. mkdocs-macros , mkdocstrings and mkdocstrings-python .","title":"Editing Documentation"},{"location":"editing_documentation/#editing-documentation","text":"The documentation that you are reading now is an invaluable resource for newcomers and experienced users alike. Contributions to the documentation are very welcome. We currently use the MkDocs as our static site generator and ReadTheDocs as our web host. To edit the documentation, first clone the repository locally, then install HELM from the repository by following the Developer Setup instructions. After that, install the MkDocs dependencies by running the following from the root of the repository: pip install -r docs/requirements.txt You should now be able to run MkDocs from the root of the repository: mkdocs serve Then navigate to http://localhost:8000/ to view your locally-built documentation. The source Markdown files for the documentation are stored in the docs/ folder. By default, MkDocs watches the source directories for changes and automatically re-renders the web pages when it detects changes. If you are creating a new page, you should add your page to the nav section in mkdocs.yml . This will add your page to the table of contents in the side menu. We make heavy use of plugins and macros for auto-generating documentation from code and docstrings. For more information, please refer to the documentation for these plugins e.g. mkdocs-macros , mkdocstrings and mkdocstrings-python .","title":"Editing Documentation"},{"location":"extension_multilingual/","text":"Multilingual","title":"Multilingual"},{"location":"extension_multilingual/#multilingual","text":"","title":"Multilingual"},{"location":"framework/","text":"Framework Legal-10 is an open evaluation framework for legal AI. We open-source everything: datasets, harness, scoring, and operational tooling. Every evaluation produces auditable run bundles with prompt-level transparency\u2014what went in, what came out, how it was scored. Anyone can reproduce the results. Legal-10 provides a unified interface for evaluating both open-weight and closed API models (GPT-4, Claude, Llama, Gemini) through identical pipelines. Prompts and configurations are standardized across providers, so score differences reflect model capability rather than evaluation artifacts. Architecture Foundation Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models). This architecture provides: Inheritance-based extension \u2014 scenarios, metrics, and adapters extend base classes Decorator-based registration \u2014 run specs auto-register via @run_spec_function Factory pattern instantiation \u2014 scenarios and metrics instantiate via class names YAML-based configuration \u2014 models and deployments configured declaratively This architecture has a proven track record of domain-specific extension\u2014MedHELM adapts it for medical AI across 121 clinician-validated tasks; IBM's enterprise benchmark extends it for finance and legal scenarios. Legal-10 supplies the legal skill taxonomy, datasets, and scoring rubrics grounded in professional standards. The underlying architecture operates the way we believe legal AI evaluation should work: open, inspectable, reproducible, and resistant to selective reporting.","title":"Evaluation Architecture"},{"location":"framework/#framework","text":"Legal-10 is an open evaluation framework for legal AI. We open-source everything: datasets, harness, scoring, and operational tooling. Every evaluation produces auditable run bundles with prompt-level transparency\u2014what went in, what came out, how it was scored. Anyone can reproduce the results. Legal-10 provides a unified interface for evaluating both open-weight and closed API models (GPT-4, Claude, Llama, Gemini) through identical pipelines. Prompts and configurations are standardized across providers, so score differences reflect model capability rather than evaluation artifacts.","title":"Framework"},{"location":"framework/#architecture-foundation","text":"Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models). This architecture provides: Inheritance-based extension \u2014 scenarios, metrics, and adapters extend base classes Decorator-based registration \u2014 run specs auto-register via @run_spec_function Factory pattern instantiation \u2014 scenarios and metrics instantiate via class names YAML-based configuration \u2014 models and deployments configured declaratively This architecture has a proven track record of domain-specific extension\u2014MedHELM adapts it for medical AI across 121 clinician-validated tasks; IBM's enterprise benchmark extends it for finance and legal scenarios. Legal-10 supplies the legal skill taxonomy, datasets, and scoring rubrics grounded in professional standards. The underlying architecture operates the way we believe legal AI evaluation should work: open, inspectable, reproducible, and resistant to selective reporting.","title":"Architecture Foundation"},{"location":"good_first_issues/","text":"Good First Issues","title":"Good First Issues"},{"location":"good_first_issues/#good-first-issues","text":"","title":"Good First Issues"},{"location":"huggingface_models/","text":"Hugging Face Model Hub Integration HELM can be used to evaluate AutoModelForCausalLM models (e.g. BioMedLM ) on Hugging Face Model Hub or local disk. Note that only AutoModelForCausalLM models are supported; other classes such as AutoModelForSeq2SeqLM may be supported in the future. Using model_deployments.yaml You can add Hugging Face models using the method discussed in Adding New Models . This can be used for both models on Hugging Face Hub and local disk. Please refer to that page for instructions for how to do so. Using command-line flags In some cases, you can use command-line flags with helm-run to evaluating Hugging Face models. This provides a more convenient way to use Hugging Face models that does not require configuration files. To use AutoModelForCausalLM models from Hugging Face Model Hub, add the Hugging Face model IDs to the --enable-huggingface-models flags to helm-run . This will make the corresponding Hugging Face models available to use in your run spec descriptions. In the run spec description, use the Hugging Face model ID as the model name. To use a revision of a model other than the default main revision, append a @ followed by the revision name to the model ID passed to the --enable-huggingface-models flag. Current restrictions with command-line flags: Models without a namespace are not supported (e.g. bert-base-uncased ). The model must have model_max_length set in the tokenizer configuration. Example model on Hugging Face Hub: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM \\ --enable-huggingface-models stanford-crfm/BioMedLM \\ --suite v1 \\ --max-eval-instances 10 # Run boolq on stanford-crfm/BioMedLM at revision main helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM@main \\ --enable-huggingface-models stanford-crfm/BioMedLM@main \\ --suite v1 \\ --max-eval-instances 10 Example model on local disk: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=your-org/your-model \\ --enable-local-huggingface-models path/to/your-org/your-model \\ --suite v1 \\ --max-eval-instances 10","title":"Hugging Face Model Hub Integration"},{"location":"huggingface_models/#hugging-face-model-hub-integration","text":"HELM can be used to evaluate AutoModelForCausalLM models (e.g. BioMedLM ) on Hugging Face Model Hub or local disk. Note that only AutoModelForCausalLM models are supported; other classes such as AutoModelForSeq2SeqLM may be supported in the future.","title":"Hugging Face Model Hub Integration"},{"location":"huggingface_models/#using-model_deploymentsyaml","text":"You can add Hugging Face models using the method discussed in Adding New Models . This can be used for both models on Hugging Face Hub and local disk. Please refer to that page for instructions for how to do so.","title":"Using model_deployments.yaml"},{"location":"huggingface_models/#using-command-line-flags","text":"In some cases, you can use command-line flags with helm-run to evaluating Hugging Face models. This provides a more convenient way to use Hugging Face models that does not require configuration files. To use AutoModelForCausalLM models from Hugging Face Model Hub, add the Hugging Face model IDs to the --enable-huggingface-models flags to helm-run . This will make the corresponding Hugging Face models available to use in your run spec descriptions. In the run spec description, use the Hugging Face model ID as the model name. To use a revision of a model other than the default main revision, append a @ followed by the revision name to the model ID passed to the --enable-huggingface-models flag. Current restrictions with command-line flags: Models without a namespace are not supported (e.g. bert-base-uncased ). The model must have model_max_length set in the tokenizer configuration. Example model on Hugging Face Hub: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM \\ --enable-huggingface-models stanford-crfm/BioMedLM \\ --suite v1 \\ --max-eval-instances 10 # Run boolq on stanford-crfm/BioMedLM at revision main helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM@main \\ --enable-huggingface-models stanford-crfm/BioMedLM@main \\ --suite v1 \\ --max-eval-instances 10 Example model on local disk: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=your-org/your-model \\ --enable-local-huggingface-models path/to/your-org/your-model \\ --suite v1 \\ --max-eval-instances 10","title":"Using command-line flags"},{"location":"importing_custom_modules/","text":"Importing Custom Modules HELM is a modular framework with a plug-in architecture. You can write your own implementation for a client, tokenizer, scenario or metric and use them in HELM with HELM installed as a library, without needing to modify HELM itself. The main way for you to use your code in HELM is to write a custom Python class that is a subclass of Client , Tokenizer , Scenario or Metric in a Python module. You can then specify a ClientSpec , TokenizerSpec , ScenarioSpec or MetricSpec (which are all classes of ObjectSpec ) where the class_name is the name of your custom Python class. However, HELM will only be able to use custom classes that can be imported by Python. Depending on your setup, you may need to do additional steps. Add the current working directory to PYTHONPATH If the custom classes live in a Python module under the current working directory, you should modify PYTHONPATH to make that Python module importable. This is required because Python does not add the current working directory to the Python module search path running when using command line comments / Python entry points such as helm-run . See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . To make your file importable by Python, you have to add . to your PYTHONPATH so that Python will search in your current working directory for your custom Python modules. In Bash, you can do this by running export PYTHONPATH=\".:$PYTHONPATH\" before running helm-run , or by prefixing helm-run with PYTHONPATH=\".:$PYTHONPATH . Put your custom classes in a Python package If your custom classes are located in a Python package, you can simply install your package (optionally in editable mode) and it will automatically be importable by Python. Be sure to install your Python package in the same Python environment as HELM. Write a Python wrapper script If you are using a Python wrapper script that calls helm.benchmark.run.run_benchmark() instead of using helm-run , Python will automatically add the directory containing that script to the Python module search path. If your custom classes live in a Python module under that directory, they will automatically be importable by Python. See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . Suppose you added a script called run_helm.py that calls helm.benchmark.run.run_benchmark() directly. When run using python run_helm.py , HELM will be able to import your modules without any additional changes.","title":"Importing Custom Modules"},{"location":"importing_custom_modules/#importing-custom-modules","text":"HELM is a modular framework with a plug-in architecture. You can write your own implementation for a client, tokenizer, scenario or metric and use them in HELM with HELM installed as a library, without needing to modify HELM itself. The main way for you to use your code in HELM is to write a custom Python class that is a subclass of Client , Tokenizer , Scenario or Metric in a Python module. You can then specify a ClientSpec , TokenizerSpec , ScenarioSpec or MetricSpec (which are all classes of ObjectSpec ) where the class_name is the name of your custom Python class. However, HELM will only be able to use custom classes that can be imported by Python. Depending on your setup, you may need to do additional steps.","title":"Importing Custom Modules"},{"location":"importing_custom_modules/#add-the-current-working-directory-to-pythonpath","text":"If the custom classes live in a Python module under the current working directory, you should modify PYTHONPATH to make that Python module importable. This is required because Python does not add the current working directory to the Python module search path running when using command line comments / Python entry points such as helm-run . See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . To make your file importable by Python, you have to add . to your PYTHONPATH so that Python will search in your current working directory for your custom Python modules. In Bash, you can do this by running export PYTHONPATH=\".:$PYTHONPATH\" before running helm-run , or by prefixing helm-run with PYTHONPATH=\".:$PYTHONPATH .","title":"Add the current working directory to PYTHONPATH"},{"location":"importing_custom_modules/#put-your-custom-classes-in-a-python-package","text":"If your custom classes are located in a Python package, you can simply install your package (optionally in editable mode) and it will automatically be importable by Python. Be sure to install your Python package in the same Python environment as HELM.","title":"Put your custom classes in a Python package"},{"location":"importing_custom_modules/#write-a-python-wrapper-script","text":"If you are using a Python wrapper script that calls helm.benchmark.run.run_benchmark() instead of using helm-run , Python will automatically add the directory containing that script to the Python module search path. If your custom classes live in a Python module under that directory, they will automatically be importable by Python. See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . Suppose you added a script called run_helm.py that calls helm.benchmark.run.run_benchmark() directly. When run using python run_helm.py , HELM will be able to import your modules without any additional changes.","title":"Write a Python wrapper script"},{"location":"installation/","text":"Installation Create a virtual environment It is recommended to install HELM into a virtual environment with Python version >=3.10 to avoid dependency conflicts. HELM requires Python >=3.10. To create, a Python virtual environment and activate it, follow the instructions below. Using Virtualenv : # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3.10 helm-venv # Activate the virtual environment. source helm-venv/bin/activate Using Anaconda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. conda activate crfm-helm Install HELM Within this virtual environment, run: pip install crfm-helm","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#create-a-virtual-environment","text":"It is recommended to install HELM into a virtual environment with Python version >=3.10 to avoid dependency conflicts. HELM requires Python >=3.10. To create, a Python virtual environment and activate it, follow the instructions below. Using Virtualenv : # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3.10 helm-venv # Activate the virtual environment. source helm-venv/bin/activate Using Anaconda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. conda activate crfm-helm","title":"Create a virtual environment"},{"location":"installation/#install-helm","text":"Within this virtual environment, run: pip install crfm-helm","title":"Install HELM"},{"location":"introduction/","text":"Introduction Skill Definitions & Professional Standards Mapping Category of Legal Work Performed by AI or Legal-AI Defining What Skills are Required to Perform Legal Work MacCrate Report: American Bar Association. (1992). Legal education and professional development: An educational continuum (Report of the Task Force on Law Schools and the Profession: Narrowing the Gap), 138\u2013141 (Skill \u00a7 3). AALL Principles: American Association of Law Libraries. (2013). Principles and standards for legal research competencies. Standard II & III. Shultz & Zedeck: Shultz, M. M., & Zedeck, S. (2011). Predicting lawyer effectiveness: Broadening the basis for law school admission decisions. Law & Social Inquiry, 36(3), 620-661. This study operationalizes the MacCrate Report's foundational skills, the AALL's competency standards, and the empirical factors identified by Shultz & Zedeck. Each of the ten categories represents a distinct cognitive \"circuit\" that is required for effective legal practice. Skill 1. Formulating a Research Plan The Skill: Constructing a systematic research strategy that identifies relevant legal issues, selects appropriate sources and search methods, sequences tasks efficiently, and establishes scope parameters before execution begins. Skill 2. Strategic Stopping The Skill: Knowing when sufficient research has been conducted to answer the legal question with a high degree of confidence, and when further searching is diminishing returns or \"churning.\" It involves the cognitive \"switch\" from seeking to synthesizing. Skill 3. Finding Known Authority The Skill: Precise retrieval of a specific legal authority or record item when the target is already identified by a citation or other unique handle (case cite, statute section, docket entry, rule number, Bates range, exhibit ID). Skill 4. Finding Unknown Authority The Skill: Discovering relevant legal authority when the citation to be used is yet unknown. This process involves examining the fact pattern, issue, or question and iteratively retrieving, refining, and selecting controlling/persuasive sources. Skill 5. Updating and Validating Authority The Skill: Verifying that a legal authority remains valid by checking subsequent treatment (overruled, distinguished, questioned, followed), legislative amendments, or other developments affecting its precedential value. Skill 6. Fact Extraction The Skill: Identifying, extracting, and organizing legally relevant facts from source documents (contracts, discovery materials, transcripts, filings) to support analysis or case strategy. Skill 7. Distinguishing Cases The Skill: Analyzing whether a precedent applies by comparing material facts, legal issues, and reasoning, and articulating grounds on which it can be distinguished or must be followed. Skill 8. Synthesizing Results The Skill: Integrating holdings, rules, and reasoning from multiple authorities into a coherent legal principle, argument, or work product that addresses the research question comprehensively. Skill 9. Citation Integrity The Skill: Ensuring all cited authorities (a) actually exist, (b) accurately support the propositions cited, (c) are properly attributed, and (d) include disclosure of adverse authority as required by professional obligations. Skill 10. Copyright Compliance The Skill: Refusing to reproduce protected content verbatim, respecting intellectual property boundaries, and minimizing memorization-based reproduction of copyrighted material. Global Extension Module - Multilingual Reasoning The Skill: Applying legal reasoning consistently across languages, jurisdictions, and legal traditions without systematic performance degradation or bias introduced by linguistic factors.","title":"Introduction"},{"location":"introduction/#introduction","text":"","title":"Introduction"},{"location":"introduction/#skill-definitions-professional-standards-mapping","text":"","title":"Skill Definitions &amp; Professional Standards Mapping"},{"location":"introduction/#category-of-legal-work-performed-by-ai-or-legal-ai","text":"Defining What Skills are Required to Perform Legal Work MacCrate Report: American Bar Association. (1992). Legal education and professional development: An educational continuum (Report of the Task Force on Law Schools and the Profession: Narrowing the Gap), 138\u2013141 (Skill \u00a7 3). AALL Principles: American Association of Law Libraries. (2013). Principles and standards for legal research competencies. Standard II & III. Shultz & Zedeck: Shultz, M. M., & Zedeck, S. (2011). Predicting lawyer effectiveness: Broadening the basis for law school admission decisions. Law & Social Inquiry, 36(3), 620-661. This study operationalizes the MacCrate Report's foundational skills, the AALL's competency standards, and the empirical factors identified by Shultz & Zedeck. Each of the ten categories represents a distinct cognitive \"circuit\" that is required for effective legal practice.","title":"Category of Legal Work Performed by AI or Legal-AI"},{"location":"introduction/#skill-1-formulating-a-research-plan","text":"The Skill: Constructing a systematic research strategy that identifies relevant legal issues, selects appropriate sources and search methods, sequences tasks efficiently, and establishes scope parameters before execution begins.","title":"Skill 1. Formulating a Research Plan"},{"location":"introduction/#skill-2-strategic-stopping","text":"The Skill: Knowing when sufficient research has been conducted to answer the legal question with a high degree of confidence, and when further searching is diminishing returns or \"churning.\" It involves the cognitive \"switch\" from seeking to synthesizing.","title":"Skill 2. Strategic Stopping"},{"location":"introduction/#skill-3-finding-known-authority","text":"The Skill: Precise retrieval of a specific legal authority or record item when the target is already identified by a citation or other unique handle (case cite, statute section, docket entry, rule number, Bates range, exhibit ID).","title":"Skill 3. Finding Known Authority"},{"location":"introduction/#skill-4-finding-unknown-authority","text":"The Skill: Discovering relevant legal authority when the citation to be used is yet unknown. This process involves examining the fact pattern, issue, or question and iteratively retrieving, refining, and selecting controlling/persuasive sources.","title":"Skill 4. Finding Unknown Authority"},{"location":"introduction/#skill-5-updating-and-validating-authority","text":"The Skill: Verifying that a legal authority remains valid by checking subsequent treatment (overruled, distinguished, questioned, followed), legislative amendments, or other developments affecting its precedential value.","title":"Skill 5. Updating and Validating Authority"},{"location":"introduction/#skill-6-fact-extraction","text":"The Skill: Identifying, extracting, and organizing legally relevant facts from source documents (contracts, discovery materials, transcripts, filings) to support analysis or case strategy.","title":"Skill 6. Fact Extraction"},{"location":"introduction/#skill-7-distinguishing-cases","text":"The Skill: Analyzing whether a precedent applies by comparing material facts, legal issues, and reasoning, and articulating grounds on which it can be distinguished or must be followed.","title":"Skill 7. Distinguishing Cases"},{"location":"introduction/#skill-8-synthesizing-results","text":"The Skill: Integrating holdings, rules, and reasoning from multiple authorities into a coherent legal principle, argument, or work product that addresses the research question comprehensively.","title":"Skill 8. Synthesizing Results"},{"location":"introduction/#skill-9-citation-integrity","text":"The Skill: Ensuring all cited authorities (a) actually exist, (b) accurately support the propositions cited, (c) are properly attributed, and (d) include disclosure of adverse authority as required by professional obligations.","title":"Skill 9. Citation Integrity"},{"location":"introduction/#skill-10-copyright-compliance","text":"The Skill: Refusing to reproduce protected content verbatim, respecting intellectual property boundaries, and minimizing memorization-based reproduction of copyrighted material.","title":"Skill 10. Copyright Compliance"},{"location":"introduction/#global-extension-module-multilingual-reasoning","text":"The Skill: Applying legal reasoning consistently across languages, jurisdictions, and legal traditions without systematic performance degradation or bias introduced by linguistic factors.","title":"Global Extension Module - Multilingual Reasoning"},{"location":"metrics/","text":"Metrics ::: helm.benchmark.metrics options: filters: [\"^(?!test_).+ metrics$\", \"Metric$\", \"^evaluate \"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Metrics"},{"location":"metrics/#metrics","text":"::: helm.benchmark.metrics options: filters: [\"^(?!test_).+ metrics$\", \"Metric$\", \"^evaluate \"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Metrics"},{"location":"mission-statement/","text":"","title":"Mission statement"},{"location":"mission_statement/","text":"Mission Statement","title":"Mission Statement"},{"location":"mission_statement/#mission-statement","text":"","title":"Mission Statement"},{"location":"models/","text":"Models Text Models {% for tag in [\"TEXT_MODEL_TAG\", \"CODE_MODEL_TAG\"] %} {% for organization, models in models_by_organization_with_tag(tag).items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} {% endfor %} Vision-Language Models {% for organization, models in models_by_organization_with_tag(\"VISION_LANGUAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} Text-to-image Models {% for organization, models in models_by_organization_with_tag(\"TEXT_TO_IMAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} Audio-Language Models {% for organization, models in models_by_organization_with_tag(\"AUDIO_LANGUAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %}","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"models/#text-models","text":"{% for tag in [\"TEXT_MODEL_TAG\", \"CODE_MODEL_TAG\"] %} {% for organization, models in models_by_organization_with_tag(tag).items() %}","title":"Text Models"},{"location":"models/#organization","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname","text":"{{ model.description }} {% endfor %} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#vision-language-models","text":"{% for organization, models in models_by_organization_with_tag(\"VISION_LANGUAGE_MODEL_TAG\").items() %}","title":"Vision-Language Models"},{"location":"models/#organization_1","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_1","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#text-to-image-models","text":"{% for organization, models in models_by_organization_with_tag(\"TEXT_TO_IMAGE_MODEL_TAG\").items() %}","title":"Text-to-image Models"},{"location":"models/#organization_2","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_2","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#audio-language-models","text":"{% for organization, models in models_by_organization_with_tag(\"AUDIO_LANGUAGE_MODEL_TAG\").items() %}","title":"Audio-Language Models"},{"location":"models/#organization_3","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_3","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"notes/","text":"Legal-10 Development Notes MkDocs Documentation Workflow Correct workflow for docs changes: 1. Edit source files ( docs/ , mkdocs.yml , docstrings.css ) 2. Restart local server: python -m mkdocs serve 3. Preview at http://127.0.0.1:8000/ 4. Commit and push to main 5. GitHub Action builds and deploys to gh-pages 6. Remote site updates at https://prophetto1.github.io/legal-10-benchmark/ Important: Never edit the site/ folder directly - it's auto-generated and gitignored. Hidden pages: Pages not listed in mkdocs.yml nav are still accessible via direct URL (e.g., this page at /notes/ ). Dataset Availability Status \u2713 Available on HuggingFace CUAD - theatticusproject/cuad-qa (S6: Fact Extraction) CLERC - jhu-clsp/CLERC (S3, S4, S5: Citation/Authority tasks) LEXam - LEXam-Benchmark/LEXam (S8: Synthesize Results) FairLex - coastalcph/fairlex (Extension: Multilingual) CaseHOLD - casehold/casehold (S7: Distinguish Cases) - Already in HELM \u26a0 Available on GitHub Dahl - reglab/legal_hallucinations (S9: Citation Integrity) LegalAgentBench - CSHaitao/LegalAgentBench (S1, S2: Agentic) - Chinese-focused \u274c Not Publicly Available SHIELD - Dataset not public due to copyright concerns (S10: Copyright Compliance) Paper: arXiv:2406.12975 Existing HELM Implementations Already Implemented CaseHOLD - src/helm/benchmark/scenarios/casehold_scenario.py ~53k questions, 5-choice MC Run spec: enterprise_run_specs.py::get_casehold_spec() LegalBench - src/helm/benchmark/scenarios/legalbench_scenario.py ~100 instances per subset (5 subsets) Run spec: lite_run_specs.py::get_legalbench_spec(subset) To Implement (8 datasets) CLERC (S3, S4, S5) CUAD (S6) LEXam (S8) LegalAgentBench (S1, S2) KeyCite (S5) - unclear if separate or part of CLERC SHIELD (S10) Dahl (S9) FairLex (Extension) Implementation Patterns (from CaseHOLD & LegalBench) Dataset Loading Use HuggingFace load_dataset() with cache_dir parameter Cache to output_path/data/ Support train/test splits Instance Structure Instance( input=Input(text=\"...\"), references=[Reference(Output(text=\"...\"), tags=[\"correct\" or []])], split=\"train\" or \"test\", id=f\"id{unique_id}\" ) Run Spec Pattern Few-shot: 2-5 examples Metrics: get_exact_match_metric_specs() Adapt methods: ADAPT_MULTIPLE_CHOICE_JOINT or ADAPT_GENERATION Computational Estimates Per-Dataset (based on CaseHOLD/LegalBench) CaseHOLD : ~1200-1850 tokens/instance, 2 few-shot examples LegalBench : ~650-1250 tokens/instance, 5 few-shot examples Estimated runtime : 30-60 min for 1 model across all 10 skills (~1000-1500 test instances) Branding Changes Completed [x] README.md updated with Legal-10 mission statement [x] CITATION.bib updated (author: Jon Chung) [ ] pyproject.toml (package name, description, author) [ ] LICENSE (copyright holder) [ ] Frontend branding Next Steps Decide on test set sampling strategy (all datasets vary in size) Create legal10_run_specs.py for all 10 skills Implement missing scenario files Handle SHIELD alternative (S10) Handle LegalAgentBench language issue (S1, S2) HELM Framework Architectural Assessment for Legal-10 Integration Executive Summary The HELM framework provides a clean extension architecture that allows Legal-10 to integrate completely without touching core framework code. The system uses: Inheritance-based extension (scenarios, metrics, adapters) Decorator-based registration (run specs via @run_spec_function ) Factory pattern instantiation (scenarios, metrics via class names) YAML-based configuration (models, deployments) 1. Constitutional Boundaries - DO NOT TOUCH These files form the backbone of HELM's execution engine and must NOT be modified when integrating Legal-10. Core Framework Files Category Files Purpose Why Not to Touch Base Classes scenario.py adapter.py metric.py run_spec.py Define contracts all extensions must follow Frozen dataclasses and abstract methods form the interface contract Execution Engine runner.py executor.py scenario_state.py request_state.py Orchestrate benchmark execution pipeline Changing flow logic breaks all existing benchmarks Registration config_registry.py model_metadata_registry.py model_deployment_registry.py Discover and register components Central registry used by entire framework Factory/Discovery adapter_factory.py run_spec_factory.py object_spec.py Instantiate components from specs Routing logic for all adapter/scenario creation Specific Files (Absolute Paths) DO NOT MODIFY: /src/helm/benchmark/scenarios/scenario.py - Base Scenario class /src/helm/benchmark/run_spec.py - RunSpec dataclass and decorator /src/helm/benchmark/adaptation/adapter_spec.py - AdapterSpec and constants /src/helm/benchmark/adaptation/adapters/adapter.py - Base Adapter class /src/helm/benchmark/metrics/metric.py - Base Metric class /src/helm/benchmark/runner.py - Execution orchestrator /src/helm/benchmark/executor.py - Request executor /src/helm/benchmark/model_metadata_registry.py - Model registry /src/helm/benchmark/model_deployment_registry.py - Deployment registry /src/helm/benchmark/config_registry.py - Configuration loader Key Principle: Never modify frozen dataclasses, abstract methods, or execution flow logic. 2. Safe Extension Points - Legal-10 Implementation These are the designed interfaces for adding Legal-10 content. What to CREATE (not modify) src/helm/benchmark/ \u251c\u2500\u2500 scenarios/ \u2502 \u251c\u2500\u2500 legal_10_clerc_scenario.py \u2190 S3, S4: Known/Unknown Authority \u2502 \u251c\u2500\u2500 legal_10_keycite_scenario.py \u2190 S5: Validate Authority \u2502 \u251c\u2500\u2500 legal_10_cuad_scenario.py \u2190 S6: Fact Extraction \u2502 \u251c\u2500\u2500 legal_10_lexam_scenario.py \u2190 S8: Synthesize Results \u2502 \u251c\u2500\u2500 legal_10_citation_scenario.py \u2190 S9: Citation Integrity \u2502 \u251c\u2500\u2500 legal_10_shield_scenario.py \u2190 S10: Copyright Compliance \u2502 \u2514\u2500\u2500 legal_10_agentbench_scenario.py \u2190 S1, S2: Research Planning/Stopping \u2502 \u251c\u2500\u2500 run_specs/ \u2502 \u2514\u2500\u2500 legal_10_run_specs.py \u2190 All 10 @run_spec_function defs \u2502 \u2514\u2500\u2500 metrics/ \u2514\u2500\u2500 legal_10_metrics.py \u2190 Custom metrics if needed docs/ \u251c\u2500\u2500 legal_10_overview.md \u2190 Benchmark introduction \u251c\u2500\u2500 legal_10_scenarios.md \u2190 Scenario details \u2514\u2500\u2500 legal_10_setup.md \u2190 Setup guide A. Adding Custom Scenarios Pattern: Create Scenario subclass inheriting from Scenario ABC Location: Create new files in /src/helm/benchmark/scenarios/ Template: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT ) from helm.common.general import ensure_directory_exists class Legal10CLERCScenario(Scenario): \"\"\" CLERC: Citation Resolution benchmark for Legal-10 Tests known authority retrieval (S3) \"\"\" name = \"legal_10_clerc\" description = \"CLERC: Known Authority Retrieval\" tags = [\"legal\", \"retrieval\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Load CLERC dataset # Create Instance objects with Input and References instances = [] # Example instance creation instance = Instance( input=Input(text=\"Find 42 U.S.C. \u00a7 1983\"), references=[ Reference(Output(text=\"correct_citation\"), tags=[CORRECT_TAG]) ], split=TEST_SPLIT, id=\"clerc_001\" ) instances.append(instance) return instances Extension Points: - Subclass Scenario - Implement get_instances(output_path) method - Set name , description , tags class variables - Optional: Override get_metadata() for custom display B. Adding Custom Run Specs Pattern: Create function decorated with @run_spec_function(name) Location: Create new file /src/helm/benchmark/run_specs/legal_10_run_specs.py Template: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) @run_spec_function(\"legal_10_clerc\") def get_legal_10_clerc_spec() -> RunSpec: \"\"\"S3: Known Authority - CLERC dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario\", args={} ) adapter_spec = get_generation_adapter_spec( instructions=\"Retrieve the legal authority for the given citation.\", input_noun=\"Citation\", output_noun=\"Authority\", max_tokens=512 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_clerc\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_rag\"] ) @run_spec_function(\"legal_10_casehold\") def get_legal_10_casehold_spec() -> RunSpec: \"\"\"S7: Distinguish Cases - CaseHOLD dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario\", args={} ) adapter_spec = get_multiple_choice_adapter_spec( method=\"ADAPT_MULTIPLE_CHOICE_JOINT\", instructions=\"Which holding is most relevant?\", input_noun=\"Passage\", output_noun=\"Answer\", max_train_instances=2 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_casehold\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_closed_book\"] ) Discovery Mechanism: - Decorator @run_spec_function(\"name\") automatically registers the function - Dynamic discovery via discover_run_spec_functions() scans all run_specs/ modules - Invoked via CLI: helm-run --run-specs legal_10_clerc C. Adding Custom Metrics (Optional) Pattern: Subclass Metric and implement evaluate() method Location: Create /src/helm/benchmark/metrics/legal_10_metrics.py Template: from typing import List from helm.benchmark.metrics.metric import ( Metric, MetricSpec, MetricResult, PerInstanceStats, Stat ) from helm.benchmark.metrics.metric_name import MetricName from helm.benchmark.adaptation.scenario_state import ScenarioState from helm.benchmark.metrics.metric_service import MetricService class CitationValidationMetric(Metric): \"\"\"Custom metric for citation integrity (S9)\"\"\" def evaluate( self, scenario_state: ScenarioState, metric_service: MetricService, eval_cache_path: str, parallelism: int ) -> MetricResult: # Custom evaluation logic stats = [] per_instance_stats = [] for request_state in scenario_state.request_states: # Validate citation exists and is correctly formatted citation_valid = self._validate_citation(request_state.result.completions[0].text) per_instance_stats.append(PerInstanceStats( instance_id=request_state.instance.id, trial_index=0, stats=[Stat(MetricName(\"citation_valid\")).add(1 if citation_valid else 0)] )) # Aggregate stats total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1) stats.append(Stat(MetricName(\"citation_accuracy\")).add(total_valid / len(per_instance_stats))) return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats) def _validate_citation(self, text: str) -> bool: # Implementation for citation validation return True # Placeholder # Helper function for run specs def get_legal_10_citation_metric_specs() -> List[MetricSpec]: return [ MetricSpec( class_name=\"helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric\", args={} ) ] When to Create Custom Metrics: - Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice - For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping) D. Adapters (Usually NOT Needed) Note: HELM provides standard adapters that handle most use cases: - GenerationAdapter - For open-ended generation (most Legal-10 tasks) - MultipleChoiceJointAdapter - For multiple choice (S7: CaseHOLD) - ChatAdapter - For chat-based models Only create custom adapters if you need specialized prompt formatting beyond what adapter_spec.instructions provides. 3. Data Flow Architecture Complete flow from input to output (unchanged by Legal-10): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Input: RunSpec (name, scenario, adapter, metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SCENARIO CREATION \u2502 \u2502 Runner.run_one() \u2192 create_scenario(scenario_spec) \u2502 \u2502 \u2192 Calls Scenario.get_instances(output_path) \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA PREPROCESSING (Optional) \u2502 \u2502 DataPreprocessor.preprocess(instances, parallelism) \u2502 \u2502 Applies data augmentations if specified \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 ADAPTATION: Instance \u2192 Request \u2502 \u2502 AdapterFactory.get_adapter(adapter_spec) \u2502 \u2502 \u2192 Adapter.adapt(instances, parallelism) \u2502 \u2502 Converts each Instance to RequestState(s) \u2502 \u2502 Returns: ScenarioState (list of RequestState) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 EXECUTION: Request \u2192 Result \u2502 \u2502 Executor.execute(scenario_state) \u2502 \u2502 - Parallel execution (parallelism parameter) \u2502 \u2502 - Makes API/local calls via client \u2502 \u2502 - Populates RequestState.result \u2502 \u2502 Returns: ScenarioState (with results filled in) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS EVALUATION \u2502 \u2502 For each MetricSpec: \u2502 \u2502 metric = create_metric(metric_spec) \u2502 \u2502 metric.evaluate(scenario_state, metric_service, ...) \u2502 \u2502 Returns: MetricResult \u2502 \u2502 - aggregated_stats: List[Stat] (global metrics) \u2502 \u2502 - per_instance_stats: List[PerInstanceStats] (per item) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Output Files (written to benchmark_output/runs/): \u2502 \u2502 - run_spec.json (RunSpec as JSON) \u2502 \u2502 - scenario.json (Scenario instances) \u2502 \u2502 - scenario_state.json (RequestState list) \u2502 \u2502 - stats.json (aggregated metrics) \u2502 \u2502 - per_instance_stats.json (per-item metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key Data Structures: 1. Instance: Single input data point with references 2. RequestState: Instance + Request + Result + metadata 3. ScenarioState: Collection of RequestStates (immutable) 4. Stat: Single metric value with MetricName context 5. PerInstanceStats: Stats grouped by (instance_id, trial_index) 4. Configuration & Discovery Systems A. Run Spec Discovery Location: All files in /src/helm/benchmark/run_specs/ Mechanism: Decorator @run_spec_function(\"name\") registers automatically Discovery: discover_run_spec_functions() uses pkgutil.iter_modules() to scan package Registry: _REGISTERED_RUN_SPEC_FUNCTIONS dict Access: get_run_spec_function(name) looks up by name For Legal-10: Create legal_10_run_specs.py in run_specs/ directory B. Model Metadata Discovery Built-in: /src/helm/config/model_metadata.yaml Local Override: $LOCAL_PATH/model_metadata.yaml Format: YAML with models list of ModelMetadata objects Registration: register_model_metadata(metadata) adds to ALL_MODELS_METADATA Access: get_model_metadata(name) retrieves by name C. Model Deployment Discovery Built-in: /src/helm/config/model_deployments.yaml Local Override: $LOCAL_PATH/model_deployments.yaml Format: YAML with model_deployments list + client_spec class_name Registration: register_model_deployment(deployment) adds to registry Access: get_model_deployment(name) retrieves by name D. Scenario Discovery NO central registry - scenarios found via ScenarioSpec.class_name Pattern: class_name: \"helm.benchmark.scenarios.module.ClassName\" Loading: create_scenario(scenario_spec) \u2192 create_object() \u2192 import + instantiate 5. Legal-10 Specific Implementation Roadmap Legal-10 Skills Mapping Skill Name Dataset Modality Status S1 Research Planning LegalAgentBench AG To Implement S2 Strategic Stopping LegalAgentBench AG To Implement S3 Known Authority CLERC RAG To Implement S4 Unknown Authority CLERC (semantic) RAG To Implement S5 Validate Authority KeyCite-CLERC RAG To Implement S6 Fact Extraction CUAD RAG To Implement S7 Distinguish Cases CaseHOLD CB \u2705 Exists S8 Synthesize Results LEXam CB To Implement S9 Citation Integrity Dahl 10-types CB To Implement S10 Copyright Compliance SHIELD CB To Implement Implementation Checklist Phase 1: Setup - [ ] Create /src/helm/benchmark/scenarios/legal_10/ subdirectory (optional) - [ ] Create /src/helm/benchmark/run_specs/legal_10_run_specs.py - [ ] Create /docs/legal_10_overview.md - [ ] Update /mkdocs.yml navigation Phase 2: Closed-Book Scenarios (S7-S10) - [ ] S7: Verify existing casehold_scenario.py works - [ ] S8: Create legal_10_lexam_scenario.py - [ ] S9: Create legal_10_citation_scenario.py - [ ] S10: Create legal_10_shield_scenario.py Phase 3: RAG Scenarios (S3-S6) - [ ] S3, S4: Create legal_10_clerc_scenario.py - [ ] S5: Create legal_10_keycite_scenario.py - [ ] S6: Create legal_10_cuad_scenario.py Phase 4: Agentic Scenarios (S1-S2) - [ ] S1, S2: Create legal_10_agentbench_scenario.py - [ ] Consider custom metrics for research planning quality Phase 5: Integration - [ ] Create all @run_spec_function entries in legal_10_run_specs.py - [ ] Add Legal-10 group to /src/helm/benchmark/static/schema_legal.yaml - [ ] Create run entries in /src/helm/benchmark/presentation/run_entries_legal.conf - [ ] Write unit tests for each scenario Phase 6: Documentation - [ ] Complete /docs/legal_10_setup.md - [ ] Complete /docs/legal_10_scenarios.md - [ ] Update skill pages with implementation details 6. DO NOT TOUCH vs SAFE TO EXTEND Summary \u274c ABSOLUTELY DO NOT MODIFY Frozen dataclasses: RunSpec , AdapterSpec , Instance , Reference , Output , Input , RequestState , ScenarioState Abstract base classes: Scenario.get_instances() , Adapter.adapt() , Metric.evaluate() Core execution: Runner.run_one() , Executor.execute() Registries and discovery: @run_spec_function decorator, discover_run_spec_functions() , config registry Factory patterns: AdapterFactory.get_adapter() , create_object() , create_scenario() Core routing: Model metadata/deployment registries \u2705 SAFE TO EXTEND (Create new, don't modify existing) Scenarios: Create new *_scenario.py files, implement Scenario subclass Run Specs: Create new *_run_specs.py files, use @run_spec_function() decorator Metrics: Create new *_metrics.py files, subclass Metric , provide MetricSpec helper Configuration: Add local YAML files, don't modify built-in config YAML Documentation: Add new .md files, update navigation in mkdocs.yml 7. Testing Your Integration Verify Run Spec Discovery # List all available run specs (should include legal_10_*) helm-run --help # Describe a specific Legal-10 run spec helm-run --run-specs legal_10_clerc --describe Run a Single Scenario # Run Legal-10 CaseHOLD scenario helm-run \\ --run-specs legal_10_casehold \\ --suite legal_10 \\ --max-eval-instances 10 \\ --output-path benchmark_output/test_run Run All Legal-10 Scenarios # Run complete Legal-10 suite helm-run \\ --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \\ --suite legal_10 \\ --models-to-run anthropic/claude-3-sonnet-20240229 \\ --output-path benchmark_output/legal_10_full 8. Integration Best Practices Naming Conventions Component Convention Example Scenario file legal_10_<dataset>_scenario.py legal_10_clerc_scenario.py Scenario class Legal10<Name>Scenario Legal10CLERCScenario Run spec function get_legal_10_<name>_spec() get_legal_10_clerc_spec() Run spec name legal_10_<name> \"legal_10_clerc\" Groups [\"legal_10\", \"legal_10_<modality>\"] [\"legal_10\", \"legal_10_rag\"] Tags Lowercase, hyphenated [\"legal\", \"retrieval\"] Import Patterns Standard imports for scenarios: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists Standard imports for run specs: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) Error Handling Scenarios should raise ValueError for invalid configuration Use hlog() from helm.common.hierarchical_logger for logging Let exceptions propagate - HELM's runner handles them gracefully Performance Use ensure_directory_exists() before file operations Cache downloaded datasets in output_path Use parallelism parameter for concurrent processing Avoid loading entire datasets into memory if possible Conclusion HELM's architecture provides clear separation between: - Framework code (do not touch) - Extension points (safe to add Legal-10 content) By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are: Modular: Each skill is an independent scenario Discoverable: Run specs auto-register via decorator Maintainable: Clear separation from core framework Upgradeable: HELM updates won't break Legal-10 code The key is to extend, never modify - inherit from base classes, use decorators, and create new files rather than changing existing ones.","title":"Legal-10 Development Notes"},{"location":"notes/#legal-10-development-notes","text":"","title":"Legal-10 Development Notes"},{"location":"notes/#mkdocs-documentation-workflow","text":"Correct workflow for docs changes: 1. Edit source files ( docs/ , mkdocs.yml , docstrings.css ) 2. Restart local server: python -m mkdocs serve 3. Preview at http://127.0.0.1:8000/ 4. Commit and push to main 5. GitHub Action builds and deploys to gh-pages 6. Remote site updates at https://prophetto1.github.io/legal-10-benchmark/ Important: Never edit the site/ folder directly - it's auto-generated and gitignored. Hidden pages: Pages not listed in mkdocs.yml nav are still accessible via direct URL (e.g., this page at /notes/ ).","title":"MkDocs Documentation Workflow"},{"location":"notes/#dataset-availability-status","text":"","title":"Dataset Availability Status"},{"location":"notes/#available-on-huggingface","text":"CUAD - theatticusproject/cuad-qa (S6: Fact Extraction) CLERC - jhu-clsp/CLERC (S3, S4, S5: Citation/Authority tasks) LEXam - LEXam-Benchmark/LEXam (S8: Synthesize Results) FairLex - coastalcph/fairlex (Extension: Multilingual) CaseHOLD - casehold/casehold (S7: Distinguish Cases) - Already in HELM","title":"\u2713 Available on HuggingFace"},{"location":"notes/#available-on-github","text":"Dahl - reglab/legal_hallucinations (S9: Citation Integrity) LegalAgentBench - CSHaitao/LegalAgentBench (S1, S2: Agentic) - Chinese-focused","title":"\u26a0 Available on GitHub"},{"location":"notes/#not-publicly-available","text":"SHIELD - Dataset not public due to copyright concerns (S10: Copyright Compliance) Paper: arXiv:2406.12975","title":"\u274c Not Publicly Available"},{"location":"notes/#existing-helm-implementations","text":"","title":"Existing HELM Implementations"},{"location":"notes/#already-implemented","text":"CaseHOLD - src/helm/benchmark/scenarios/casehold_scenario.py ~53k questions, 5-choice MC Run spec: enterprise_run_specs.py::get_casehold_spec() LegalBench - src/helm/benchmark/scenarios/legalbench_scenario.py ~100 instances per subset (5 subsets) Run spec: lite_run_specs.py::get_legalbench_spec(subset)","title":"Already Implemented"},{"location":"notes/#to-implement-8-datasets","text":"CLERC (S3, S4, S5) CUAD (S6) LEXam (S8) LegalAgentBench (S1, S2) KeyCite (S5) - unclear if separate or part of CLERC SHIELD (S10) Dahl (S9) FairLex (Extension)","title":"To Implement (8 datasets)"},{"location":"notes/#implementation-patterns-from-casehold-legalbench","text":"","title":"Implementation Patterns (from CaseHOLD &amp; LegalBench)"},{"location":"notes/#dataset-loading","text":"Use HuggingFace load_dataset() with cache_dir parameter Cache to output_path/data/ Support train/test splits","title":"Dataset Loading"},{"location":"notes/#instance-structure","text":"Instance( input=Input(text=\"...\"), references=[Reference(Output(text=\"...\"), tags=[\"correct\" or []])], split=\"train\" or \"test\", id=f\"id{unique_id}\" )","title":"Instance Structure"},{"location":"notes/#run-spec-pattern","text":"Few-shot: 2-5 examples Metrics: get_exact_match_metric_specs() Adapt methods: ADAPT_MULTIPLE_CHOICE_JOINT or ADAPT_GENERATION","title":"Run Spec Pattern"},{"location":"notes/#computational-estimates","text":"","title":"Computational Estimates"},{"location":"notes/#per-dataset-based-on-caseholdlegalbench","text":"CaseHOLD : ~1200-1850 tokens/instance, 2 few-shot examples LegalBench : ~650-1250 tokens/instance, 5 few-shot examples Estimated runtime : 30-60 min for 1 model across all 10 skills (~1000-1500 test instances)","title":"Per-Dataset (based on CaseHOLD/LegalBench)"},{"location":"notes/#branding-changes-completed","text":"[x] README.md updated with Legal-10 mission statement [x] CITATION.bib updated (author: Jon Chung) [ ] pyproject.toml (package name, description, author) [ ] LICENSE (copyright holder) [ ] Frontend branding","title":"Branding Changes Completed"},{"location":"notes/#next-steps","text":"Decide on test set sampling strategy (all datasets vary in size) Create legal10_run_specs.py for all 10 skills Implement missing scenario files Handle SHIELD alternative (S10) Handle LegalAgentBench language issue (S1, S2)","title":"Next Steps"},{"location":"notes/#helm-framework-architectural-assessment-for-legal-10-integration","text":"","title":"HELM Framework Architectural Assessment for Legal-10 Integration"},{"location":"notes/#executive-summary","text":"The HELM framework provides a clean extension architecture that allows Legal-10 to integrate completely without touching core framework code. The system uses: Inheritance-based extension (scenarios, metrics, adapters) Decorator-based registration (run specs via @run_spec_function ) Factory pattern instantiation (scenarios, metrics via class names) YAML-based configuration (models, deployments)","title":"Executive Summary"},{"location":"notes/#1-constitutional-boundaries-do-not-touch","text":"These files form the backbone of HELM's execution engine and must NOT be modified when integrating Legal-10.","title":"1. Constitutional Boundaries - DO NOT TOUCH"},{"location":"notes/#core-framework-files","text":"Category Files Purpose Why Not to Touch Base Classes scenario.py adapter.py metric.py run_spec.py Define contracts all extensions must follow Frozen dataclasses and abstract methods form the interface contract Execution Engine runner.py executor.py scenario_state.py request_state.py Orchestrate benchmark execution pipeline Changing flow logic breaks all existing benchmarks Registration config_registry.py model_metadata_registry.py model_deployment_registry.py Discover and register components Central registry used by entire framework Factory/Discovery adapter_factory.py run_spec_factory.py object_spec.py Instantiate components from specs Routing logic for all adapter/scenario creation","title":"Core Framework Files"},{"location":"notes/#specific-files-absolute-paths","text":"DO NOT MODIFY: /src/helm/benchmark/scenarios/scenario.py - Base Scenario class /src/helm/benchmark/run_spec.py - RunSpec dataclass and decorator /src/helm/benchmark/adaptation/adapter_spec.py - AdapterSpec and constants /src/helm/benchmark/adaptation/adapters/adapter.py - Base Adapter class /src/helm/benchmark/metrics/metric.py - Base Metric class /src/helm/benchmark/runner.py - Execution orchestrator /src/helm/benchmark/executor.py - Request executor /src/helm/benchmark/model_metadata_registry.py - Model registry /src/helm/benchmark/model_deployment_registry.py - Deployment registry /src/helm/benchmark/config_registry.py - Configuration loader Key Principle: Never modify frozen dataclasses, abstract methods, or execution flow logic.","title":"Specific Files (Absolute Paths)"},{"location":"notes/#2-safe-extension-points-legal-10-implementation","text":"These are the designed interfaces for adding Legal-10 content.","title":"2. Safe Extension Points - Legal-10 Implementation"},{"location":"notes/#what-to-create-not-modify","text":"src/helm/benchmark/ \u251c\u2500\u2500 scenarios/ \u2502 \u251c\u2500\u2500 legal_10_clerc_scenario.py \u2190 S3, S4: Known/Unknown Authority \u2502 \u251c\u2500\u2500 legal_10_keycite_scenario.py \u2190 S5: Validate Authority \u2502 \u251c\u2500\u2500 legal_10_cuad_scenario.py \u2190 S6: Fact Extraction \u2502 \u251c\u2500\u2500 legal_10_lexam_scenario.py \u2190 S8: Synthesize Results \u2502 \u251c\u2500\u2500 legal_10_citation_scenario.py \u2190 S9: Citation Integrity \u2502 \u251c\u2500\u2500 legal_10_shield_scenario.py \u2190 S10: Copyright Compliance \u2502 \u2514\u2500\u2500 legal_10_agentbench_scenario.py \u2190 S1, S2: Research Planning/Stopping \u2502 \u251c\u2500\u2500 run_specs/ \u2502 \u2514\u2500\u2500 legal_10_run_specs.py \u2190 All 10 @run_spec_function defs \u2502 \u2514\u2500\u2500 metrics/ \u2514\u2500\u2500 legal_10_metrics.py \u2190 Custom metrics if needed docs/ \u251c\u2500\u2500 legal_10_overview.md \u2190 Benchmark introduction \u251c\u2500\u2500 legal_10_scenarios.md \u2190 Scenario details \u2514\u2500\u2500 legal_10_setup.md \u2190 Setup guide","title":"What to CREATE (not modify)"},{"location":"notes/#a-adding-custom-scenarios","text":"Pattern: Create Scenario subclass inheriting from Scenario ABC Location: Create new files in /src/helm/benchmark/scenarios/ Template: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT ) from helm.common.general import ensure_directory_exists class Legal10CLERCScenario(Scenario): \"\"\" CLERC: Citation Resolution benchmark for Legal-10 Tests known authority retrieval (S3) \"\"\" name = \"legal_10_clerc\" description = \"CLERC: Known Authority Retrieval\" tags = [\"legal\", \"retrieval\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Load CLERC dataset # Create Instance objects with Input and References instances = [] # Example instance creation instance = Instance( input=Input(text=\"Find 42 U.S.C. \u00a7 1983\"), references=[ Reference(Output(text=\"correct_citation\"), tags=[CORRECT_TAG]) ], split=TEST_SPLIT, id=\"clerc_001\" ) instances.append(instance) return instances Extension Points: - Subclass Scenario - Implement get_instances(output_path) method - Set name , description , tags class variables - Optional: Override get_metadata() for custom display","title":"A. Adding Custom Scenarios"},{"location":"notes/#b-adding-custom-run-specs","text":"Pattern: Create function decorated with @run_spec_function(name) Location: Create new file /src/helm/benchmark/run_specs/legal_10_run_specs.py Template: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) @run_spec_function(\"legal_10_clerc\") def get_legal_10_clerc_spec() -> RunSpec: \"\"\"S3: Known Authority - CLERC dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario\", args={} ) adapter_spec = get_generation_adapter_spec( instructions=\"Retrieve the legal authority for the given citation.\", input_noun=\"Citation\", output_noun=\"Authority\", max_tokens=512 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_clerc\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_rag\"] ) @run_spec_function(\"legal_10_casehold\") def get_legal_10_casehold_spec() -> RunSpec: \"\"\"S7: Distinguish Cases - CaseHOLD dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario\", args={} ) adapter_spec = get_multiple_choice_adapter_spec( method=\"ADAPT_MULTIPLE_CHOICE_JOINT\", instructions=\"Which holding is most relevant?\", input_noun=\"Passage\", output_noun=\"Answer\", max_train_instances=2 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_casehold\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_closed_book\"] ) Discovery Mechanism: - Decorator @run_spec_function(\"name\") automatically registers the function - Dynamic discovery via discover_run_spec_functions() scans all run_specs/ modules - Invoked via CLI: helm-run --run-specs legal_10_clerc","title":"B. Adding Custom Run Specs"},{"location":"notes/#c-adding-custom-metrics-optional","text":"Pattern: Subclass Metric and implement evaluate() method Location: Create /src/helm/benchmark/metrics/legal_10_metrics.py Template: from typing import List from helm.benchmark.metrics.metric import ( Metric, MetricSpec, MetricResult, PerInstanceStats, Stat ) from helm.benchmark.metrics.metric_name import MetricName from helm.benchmark.adaptation.scenario_state import ScenarioState from helm.benchmark.metrics.metric_service import MetricService class CitationValidationMetric(Metric): \"\"\"Custom metric for citation integrity (S9)\"\"\" def evaluate( self, scenario_state: ScenarioState, metric_service: MetricService, eval_cache_path: str, parallelism: int ) -> MetricResult: # Custom evaluation logic stats = [] per_instance_stats = [] for request_state in scenario_state.request_states: # Validate citation exists and is correctly formatted citation_valid = self._validate_citation(request_state.result.completions[0].text) per_instance_stats.append(PerInstanceStats( instance_id=request_state.instance.id, trial_index=0, stats=[Stat(MetricName(\"citation_valid\")).add(1 if citation_valid else 0)] )) # Aggregate stats total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1) stats.append(Stat(MetricName(\"citation_accuracy\")).add(total_valid / len(per_instance_stats))) return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats) def _validate_citation(self, text: str) -> bool: # Implementation for citation validation return True # Placeholder # Helper function for run specs def get_legal_10_citation_metric_specs() -> List[MetricSpec]: return [ MetricSpec( class_name=\"helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric\", args={} ) ] When to Create Custom Metrics: - Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice - For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping)","title":"C. Adding Custom Metrics (Optional)"},{"location":"notes/#d-adapters-usually-not-needed","text":"Note: HELM provides standard adapters that handle most use cases: - GenerationAdapter - For open-ended generation (most Legal-10 tasks) - MultipleChoiceJointAdapter - For multiple choice (S7: CaseHOLD) - ChatAdapter - For chat-based models Only create custom adapters if you need specialized prompt formatting beyond what adapter_spec.instructions provides.","title":"D. Adapters (Usually NOT Needed)"},{"location":"notes/#3-data-flow-architecture","text":"Complete flow from input to output (unchanged by Legal-10): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Input: RunSpec (name, scenario, adapter, metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SCENARIO CREATION \u2502 \u2502 Runner.run_one() \u2192 create_scenario(scenario_spec) \u2502 \u2502 \u2192 Calls Scenario.get_instances(output_path) \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA PREPROCESSING (Optional) \u2502 \u2502 DataPreprocessor.preprocess(instances, parallelism) \u2502 \u2502 Applies data augmentations if specified \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 ADAPTATION: Instance \u2192 Request \u2502 \u2502 AdapterFactory.get_adapter(adapter_spec) \u2502 \u2502 \u2192 Adapter.adapt(instances, parallelism) \u2502 \u2502 Converts each Instance to RequestState(s) \u2502 \u2502 Returns: ScenarioState (list of RequestState) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 EXECUTION: Request \u2192 Result \u2502 \u2502 Executor.execute(scenario_state) \u2502 \u2502 - Parallel execution (parallelism parameter) \u2502 \u2502 - Makes API/local calls via client \u2502 \u2502 - Populates RequestState.result \u2502 \u2502 Returns: ScenarioState (with results filled in) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS EVALUATION \u2502 \u2502 For each MetricSpec: \u2502 \u2502 metric = create_metric(metric_spec) \u2502 \u2502 metric.evaluate(scenario_state, metric_service, ...) \u2502 \u2502 Returns: MetricResult \u2502 \u2502 - aggregated_stats: List[Stat] (global metrics) \u2502 \u2502 - per_instance_stats: List[PerInstanceStats] (per item) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Output Files (written to benchmark_output/runs/): \u2502 \u2502 - run_spec.json (RunSpec as JSON) \u2502 \u2502 - scenario.json (Scenario instances) \u2502 \u2502 - scenario_state.json (RequestState list) \u2502 \u2502 - stats.json (aggregated metrics) \u2502 \u2502 - per_instance_stats.json (per-item metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key Data Structures: 1. Instance: Single input data point with references 2. RequestState: Instance + Request + Result + metadata 3. ScenarioState: Collection of RequestStates (immutable) 4. Stat: Single metric value with MetricName context 5. PerInstanceStats: Stats grouped by (instance_id, trial_index)","title":"3. Data Flow Architecture"},{"location":"notes/#4-configuration-discovery-systems","text":"","title":"4. Configuration &amp; Discovery Systems"},{"location":"notes/#a-run-spec-discovery","text":"Location: All files in /src/helm/benchmark/run_specs/ Mechanism: Decorator @run_spec_function(\"name\") registers automatically Discovery: discover_run_spec_functions() uses pkgutil.iter_modules() to scan package Registry: _REGISTERED_RUN_SPEC_FUNCTIONS dict Access: get_run_spec_function(name) looks up by name For Legal-10: Create legal_10_run_specs.py in run_specs/ directory","title":"A. Run Spec Discovery"},{"location":"notes/#b-model-metadata-discovery","text":"Built-in: /src/helm/config/model_metadata.yaml Local Override: $LOCAL_PATH/model_metadata.yaml Format: YAML with models list of ModelMetadata objects Registration: register_model_metadata(metadata) adds to ALL_MODELS_METADATA Access: get_model_metadata(name) retrieves by name","title":"B. Model Metadata Discovery"},{"location":"notes/#c-model-deployment-discovery","text":"Built-in: /src/helm/config/model_deployments.yaml Local Override: $LOCAL_PATH/model_deployments.yaml Format: YAML with model_deployments list + client_spec class_name Registration: register_model_deployment(deployment) adds to registry Access: get_model_deployment(name) retrieves by name","title":"C. Model Deployment Discovery"},{"location":"notes/#d-scenario-discovery","text":"NO central registry - scenarios found via ScenarioSpec.class_name Pattern: class_name: \"helm.benchmark.scenarios.module.ClassName\" Loading: create_scenario(scenario_spec) \u2192 create_object() \u2192 import + instantiate","title":"D. Scenario Discovery"},{"location":"notes/#5-legal-10-specific-implementation-roadmap","text":"","title":"5. Legal-10 Specific Implementation Roadmap"},{"location":"notes/#legal-10-skills-mapping","text":"Skill Name Dataset Modality Status S1 Research Planning LegalAgentBench AG To Implement S2 Strategic Stopping LegalAgentBench AG To Implement S3 Known Authority CLERC RAG To Implement S4 Unknown Authority CLERC (semantic) RAG To Implement S5 Validate Authority KeyCite-CLERC RAG To Implement S6 Fact Extraction CUAD RAG To Implement S7 Distinguish Cases CaseHOLD CB \u2705 Exists S8 Synthesize Results LEXam CB To Implement S9 Citation Integrity Dahl 10-types CB To Implement S10 Copyright Compliance SHIELD CB To Implement","title":"Legal-10 Skills Mapping"},{"location":"notes/#implementation-checklist","text":"Phase 1: Setup - [ ] Create /src/helm/benchmark/scenarios/legal_10/ subdirectory (optional) - [ ] Create /src/helm/benchmark/run_specs/legal_10_run_specs.py - [ ] Create /docs/legal_10_overview.md - [ ] Update /mkdocs.yml navigation Phase 2: Closed-Book Scenarios (S7-S10) - [ ] S7: Verify existing casehold_scenario.py works - [ ] S8: Create legal_10_lexam_scenario.py - [ ] S9: Create legal_10_citation_scenario.py - [ ] S10: Create legal_10_shield_scenario.py Phase 3: RAG Scenarios (S3-S6) - [ ] S3, S4: Create legal_10_clerc_scenario.py - [ ] S5: Create legal_10_keycite_scenario.py - [ ] S6: Create legal_10_cuad_scenario.py Phase 4: Agentic Scenarios (S1-S2) - [ ] S1, S2: Create legal_10_agentbench_scenario.py - [ ] Consider custom metrics for research planning quality Phase 5: Integration - [ ] Create all @run_spec_function entries in legal_10_run_specs.py - [ ] Add Legal-10 group to /src/helm/benchmark/static/schema_legal.yaml - [ ] Create run entries in /src/helm/benchmark/presentation/run_entries_legal.conf - [ ] Write unit tests for each scenario Phase 6: Documentation - [ ] Complete /docs/legal_10_setup.md - [ ] Complete /docs/legal_10_scenarios.md - [ ] Update skill pages with implementation details","title":"Implementation Checklist"},{"location":"notes/#6-do-not-touch-vs-safe-to-extend-summary","text":"","title":"6. DO NOT TOUCH vs SAFE TO EXTEND Summary"},{"location":"notes/#absolutely-do-not-modify","text":"Frozen dataclasses: RunSpec , AdapterSpec , Instance , Reference , Output , Input , RequestState , ScenarioState Abstract base classes: Scenario.get_instances() , Adapter.adapt() , Metric.evaluate() Core execution: Runner.run_one() , Executor.execute() Registries and discovery: @run_spec_function decorator, discover_run_spec_functions() , config registry Factory patterns: AdapterFactory.get_adapter() , create_object() , create_scenario() Core routing: Model metadata/deployment registries","title":"\u274c ABSOLUTELY DO NOT MODIFY"},{"location":"notes/#safe-to-extend-create-new-dont-modify-existing","text":"Scenarios: Create new *_scenario.py files, implement Scenario subclass Run Specs: Create new *_run_specs.py files, use @run_spec_function() decorator Metrics: Create new *_metrics.py files, subclass Metric , provide MetricSpec helper Configuration: Add local YAML files, don't modify built-in config YAML Documentation: Add new .md files, update navigation in mkdocs.yml","title":"\u2705 SAFE TO EXTEND (Create new, don't modify existing)"},{"location":"notes/#7-testing-your-integration","text":"","title":"7. Testing Your Integration"},{"location":"notes/#verify-run-spec-discovery","text":"# List all available run specs (should include legal_10_*) helm-run --help # Describe a specific Legal-10 run spec helm-run --run-specs legal_10_clerc --describe","title":"Verify Run Spec Discovery"},{"location":"notes/#run-a-single-scenario","text":"# Run Legal-10 CaseHOLD scenario helm-run \\ --run-specs legal_10_casehold \\ --suite legal_10 \\ --max-eval-instances 10 \\ --output-path benchmark_output/test_run","title":"Run a Single Scenario"},{"location":"notes/#run-all-legal-10-scenarios","text":"# Run complete Legal-10 suite helm-run \\ --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \\ --suite legal_10 \\ --models-to-run anthropic/claude-3-sonnet-20240229 \\ --output-path benchmark_output/legal_10_full","title":"Run All Legal-10 Scenarios"},{"location":"notes/#8-integration-best-practices","text":"","title":"8. Integration Best Practices"},{"location":"notes/#naming-conventions","text":"Component Convention Example Scenario file legal_10_<dataset>_scenario.py legal_10_clerc_scenario.py Scenario class Legal10<Name>Scenario Legal10CLERCScenario Run spec function get_legal_10_<name>_spec() get_legal_10_clerc_spec() Run spec name legal_10_<name> \"legal_10_clerc\" Groups [\"legal_10\", \"legal_10_<modality>\"] [\"legal_10\", \"legal_10_rag\"] Tags Lowercase, hyphenated [\"legal\", \"retrieval\"]","title":"Naming Conventions"},{"location":"notes/#import-patterns","text":"Standard imports for scenarios: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists Standard imports for run specs: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs )","title":"Import Patterns"},{"location":"notes/#error-handling","text":"Scenarios should raise ValueError for invalid configuration Use hlog() from helm.common.hierarchical_logger for logging Let exceptions propagate - HELM's runner handles them gracefully","title":"Error Handling"},{"location":"notes/#performance","text":"Use ensure_directory_exists() before file operations Cache downloaded datasets in output_path Use parallelism parameter for concurrent processing Avoid loading entire datasets into memory if possible","title":"Performance"},{"location":"notes/#conclusion","text":"HELM's architecture provides clear separation between: - Framework code (do not touch) - Extension points (safe to add Legal-10 content) By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are: Modular: Each skill is an independent scenario Discoverable: Run specs auto-register via decorator Maintainable: Clear separation from core framework Upgradeable: HELM updates won't break Legal-10 code The key is to extend, never modify - inherit from base classes, use decorators, and create new files rather than changing existing ones.","title":"Conclusion"},{"location":"perturbations/","text":"Perturbations ::: helm.benchmark.augmentations options: filters: [\"^(?!test_).+_perturbation$\", \".+Perturbation$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Perturbations"},{"location":"perturbations/#perturbations","text":"::: helm.benchmark.augmentations options: filters: [\"^(?!test_).+_perturbation$\", \".+Perturbation$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Perturbations"},{"location":"proxy_server/","text":"Proxy Server Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! We provide a single unified entry point into accessing large language models (e.g., GPT-3, Jurassic). This provides both a web interface and a REST API. Using (for most people) To use the web interface, go to https://crfm-models.stanford.edu. To use the REST API, see demo.py . Deploying locally Create prod_env/credentials.conf to contain the API keys for any language models you have access to. { openaiApiKey: \"...\", ai21ApiKey: \"...\" } To start a local server (go to http://localhost:1959 to try it out): venv/bin/crfm-proxy-server When starting the server for the first time, the server will create an admin account with the API key: root . If you're deploying the server to production, make sure to rotate the API key of the default admin account. For macOS developers Bypass the added security that restricts multithreading by running: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES venv/bin/crfm-proxy-server","title":"Proxy Server"},{"location":"proxy_server/#proxy-server","text":"Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! We provide a single unified entry point into accessing large language models (e.g., GPT-3, Jurassic). This provides both a web interface and a REST API.","title":"Proxy Server"},{"location":"proxy_server/#using-for-most-people","text":"To use the web interface, go to https://crfm-models.stanford.edu. To use the REST API, see demo.py .","title":"Using (for most people)"},{"location":"proxy_server/#deploying-locally","text":"Create prod_env/credentials.conf to contain the API keys for any language models you have access to. { openaiApiKey: \"...\", ai21ApiKey: \"...\" } To start a local server (go to http://localhost:1959 to try it out): venv/bin/crfm-proxy-server When starting the server for the first time, the server will create an admin account with the API key: root . If you're deploying the server to production, make sure to rotate the API key of the default admin account.","title":"Deploying locally"},{"location":"proxy_server/#for-macos-developers","text":"Bypass the added security that restricts multithreading by running: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES venv/bin/crfm-proxy-server","title":"For macOS developers"},{"location":"quick_start/","text":"Quick Start","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"","title":"Quick Start"},{"location":"run_entries/","text":"Run Entries Using run entries Run entries are the main way of specifying to helm-run which evaluation runs to execute. For instance, in order to evaluate GPT-2 on MedQA, we would pass the following run entry to helm-run : med_qa:model=openai/gpt2 There are two ways of passing the run entry to helm-run . We can use the --run-entries flag. For example: helm-run --run-entries med_qa:model=openai/gpt2 --suite my-suite --max-eval-instances 10 Alternatively, we can put the run entry into a run_entries.conf file, and the pass that file to helm-run using the --conf-file flag. The run_entries.conf file is a run entry configuration file that conforms to the format documented here . For example: helm-run --conf-file run_entries.conf --suite my-suite --max-eval-instances 10 Constructing run entires Specifying the run spec function name The first part of the run entry before the : is the run spec function name. For example, in the run entry med_qa:model=openai/gpt2 , the run spec function name is med_qa . A catalog of all run spec function names will be added to the documentation in the future. For now, the best way to find the run spec function name is to look through functions decorated with the @run_spec_function() in the Python modules helm.benchmark.run_specs.*_run_specs . The run spec function name is the decorator's parameter e.g. @run_spec_function(\"med_qa\") indicates a run spec function name of med_qa . Note: the run spec function name is frequently the same as the scenario name by convention, but this is not always the case. For instance, the openbookqa scenario has a run spec function that is named commonsense . Run entry arguments The second part of the run entry after the : is a mapping of argument names to argument values. The string has the format arg_name_1=arg_value_1,arg_name_2=arg_value_2 i.e. the name and value of each argument is joined by = , and the argument name-value pairs are joined by , . All argument values must be non-empty strings. The run entry arguments are used for two different things: run spec function arguments, and run expanders. For instance, in the example run entry mmlu:subject=anatomy,model=openai/gpt2 , a run spec function argument is specified by subject=anatomy , and a run expander is specified by model=openai/gpt2 . As in the above example, you can mix run expanders and run spec function arguments in a single run entry. If there is a name conflict between a run expander name and a run spec function argument name, the run expander has precedence. Run spec function arguments Some run spec functions take in arguments. For instance, the MMLU run spec function get_mmlu_spec() takes in a subject argument. MMLU is a question answering scenario that covers multiple academic subjects. The subject argument specifies that the question set corresponding to that academic subject should be used for that evaluation run. For instance, to evaluate MMLU with the anatomy subject on GPT-2, the run entry should be: mmlu:subject=anatomy,model=openai/gpt2 A catalog of all run spec functions' parameters will be added to the documentation in the future. For now, the best way to find the run spec function parameters would be to inspect the function definition in the Python modules helm.benchmark.run_specs.*_run_specs for the run spec function in question. Run expanders Run expanders are functions that modify how evaluation runs work. Concretely, a run expander operates on a configuration of an evaluation run (a RunSpec ) and produces zero, one or multiple evaluation runs configurations with modified configurations ( RunSpecs ). Run expanders are an advanced topic. For most use cases, the only run expander that you will need to use is the model run expander. The model=openai/gpt2 argument pair in the run entry indicates that the evaluation run should use the openai/gpt2 model. More explanation may be added to the documentation in the future. Run entry naming The first part of the run entry name is usually be the name of the scenario by convention, but this may not always be the case. For instance, the run entry commonsense:dataset=openbookqa,model=openai/gpt2 uses the openbookqa scenario. The first part of the run entry name is usually be the name of the run spec function name by convention, but this may not always be the case. For instance, the run entry disinformation:type=wedging,model=openai/gpt2 results in the RunSpec name disinfo:type=wedging,model=openai_gpt2 . Run entries and RunSpec s You may have noticed that some run entries can produce multiple evaluation runs. Concretely, single run entry can produce multiple RunSpec s, and each RunSpec specifies a single evaluation run. This is because run expanders are functions that take in a RunSpec and can produce multiple RunSpec . As explained previously, the model run expander is an example of this. The model run expander The model run expander is the most commonly used run expander. As discussed earlier, it can be used to set the model for each run entry. The model run expander also supports wildcard values . For instance, the med_qa:model=text run entry will run the med_qa scenario on every text model that helm-run can find in its configuration files. The wildcard is intended to be used in conjuction with the --models-to-run , which controls which models will actually be evaluated. For example, helm-run --run-entries med_qa:model=text --models-to-run openai/gpt2 openai/gpt-3.5-turbo-613 will run med_qa on only openai/gpt2 and openai/gpt-3.5-turbo-613 . Wildcard values for the model run expander are common used in run entries configuration files which will are discussed here .","title":"Run Entries"},{"location":"run_entries/#run-entries","text":"","title":"Run Entries"},{"location":"run_entries/#using-run-entries","text":"Run entries are the main way of specifying to helm-run which evaluation runs to execute. For instance, in order to evaluate GPT-2 on MedQA, we would pass the following run entry to helm-run : med_qa:model=openai/gpt2 There are two ways of passing the run entry to helm-run . We can use the --run-entries flag. For example: helm-run --run-entries med_qa:model=openai/gpt2 --suite my-suite --max-eval-instances 10 Alternatively, we can put the run entry into a run_entries.conf file, and the pass that file to helm-run using the --conf-file flag. The run_entries.conf file is a run entry configuration file that conforms to the format documented here . For example: helm-run --conf-file run_entries.conf --suite my-suite --max-eval-instances 10","title":"Using run entries"},{"location":"run_entries/#constructing-run-entires","text":"","title":"Constructing run entires"},{"location":"run_entries/#specifying-the-run-spec-function-name","text":"The first part of the run entry before the : is the run spec function name. For example, in the run entry med_qa:model=openai/gpt2 , the run spec function name is med_qa . A catalog of all run spec function names will be added to the documentation in the future. For now, the best way to find the run spec function name is to look through functions decorated with the @run_spec_function() in the Python modules helm.benchmark.run_specs.*_run_specs . The run spec function name is the decorator's parameter e.g. @run_spec_function(\"med_qa\") indicates a run spec function name of med_qa . Note: the run spec function name is frequently the same as the scenario name by convention, but this is not always the case. For instance, the openbookqa scenario has a run spec function that is named commonsense .","title":"Specifying the run spec function name"},{"location":"run_entries/#run-entry-arguments","text":"The second part of the run entry after the : is a mapping of argument names to argument values. The string has the format arg_name_1=arg_value_1,arg_name_2=arg_value_2 i.e. the name and value of each argument is joined by = , and the argument name-value pairs are joined by , . All argument values must be non-empty strings. The run entry arguments are used for two different things: run spec function arguments, and run expanders. For instance, in the example run entry mmlu:subject=anatomy,model=openai/gpt2 , a run spec function argument is specified by subject=anatomy , and a run expander is specified by model=openai/gpt2 . As in the above example, you can mix run expanders and run spec function arguments in a single run entry. If there is a name conflict between a run expander name and a run spec function argument name, the run expander has precedence.","title":"Run entry arguments"},{"location":"run_entries/#run-spec-function-arguments","text":"Some run spec functions take in arguments. For instance, the MMLU run spec function get_mmlu_spec() takes in a subject argument. MMLU is a question answering scenario that covers multiple academic subjects. The subject argument specifies that the question set corresponding to that academic subject should be used for that evaluation run. For instance, to evaluate MMLU with the anatomy subject on GPT-2, the run entry should be: mmlu:subject=anatomy,model=openai/gpt2 A catalog of all run spec functions' parameters will be added to the documentation in the future. For now, the best way to find the run spec function parameters would be to inspect the function definition in the Python modules helm.benchmark.run_specs.*_run_specs for the run spec function in question.","title":"Run spec function arguments"},{"location":"run_entries/#run-expanders","text":"Run expanders are functions that modify how evaluation runs work. Concretely, a run expander operates on a configuration of an evaluation run (a RunSpec ) and produces zero, one or multiple evaluation runs configurations with modified configurations ( RunSpecs ). Run expanders are an advanced topic. For most use cases, the only run expander that you will need to use is the model run expander. The model=openai/gpt2 argument pair in the run entry indicates that the evaluation run should use the openai/gpt2 model. More explanation may be added to the documentation in the future.","title":"Run expanders"},{"location":"run_entries/#run-entry-naming","text":"The first part of the run entry name is usually be the name of the scenario by convention, but this may not always be the case. For instance, the run entry commonsense:dataset=openbookqa,model=openai/gpt2 uses the openbookqa scenario. The first part of the run entry name is usually be the name of the run spec function name by convention, but this may not always be the case. For instance, the run entry disinformation:type=wedging,model=openai/gpt2 results in the RunSpec name disinfo:type=wedging,model=openai_gpt2 .","title":"Run entry naming"},{"location":"run_entries/#run-entries-and-runspecs","text":"You may have noticed that some run entries can produce multiple evaluation runs. Concretely, single run entry can produce multiple RunSpec s, and each RunSpec specifies a single evaluation run. This is because run expanders are functions that take in a RunSpec and can produce multiple RunSpec . As explained previously, the model run expander is an example of this.","title":"Run entries and RunSpecs"},{"location":"run_entries/#the-model-run-expander","text":"The model run expander is the most commonly used run expander. As discussed earlier, it can be used to set the model for each run entry. The model run expander also supports wildcard values . For instance, the med_qa:model=text run entry will run the med_qa scenario on every text model that helm-run can find in its configuration files. The wildcard is intended to be used in conjuction with the --models-to-run , which controls which models will actually be evaluated. For example, helm-run --run-entries med_qa:model=text --models-to-run openai/gpt2 openai/gpt-3.5-turbo-613 will run med_qa on only openai/gpt2 and openai/gpt-3.5-turbo-613 . Wildcard values for the model run expander are common used in run entries configuration files which will are discussed here .","title":"The model run expander"},{"location":"run_entries_configuration_files/","text":"Run Entries Configuration Files In the tutorial, we have been using --run-entries to specify run entries for helm-run . However, we can also put the run entries into a run entries configuration file , and then pass the file to helm-run using the --conf-file flag. This has a number of advantages: This prevents the command line invocation of helm-run from getting too long when a large number of run entries are run. The run entries configuration file can be shared with other users and commited to Git. For example, instead of running: helm-run --run-specs mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite tutorial --max-eval-instances 10 You can instead create a tutorial_run_entries.conf file in your current working directory: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 Model Run Expander Wildcards It is very common to use run entries configuration file with a model run expander wildcards e.g. model=text . For instance, entries: [ {description: \"mmlu:subject=anatomy,model=text\", priority: 1}, {description: \"mmlu:subject=philosophy,model=text\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 --models-to-run openai/gpt2 This has exactly the same behavior has the previous example. For more information on model run expander wildcards, refer to the run entry format documentation. Priorities You can use the --priority flag in conjunction with --conf-file . This filters out run entries with a higher priority value than the specified --priority value. For instance, with this run entries configuration file: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 2}, ] If run with --priority 1 , only the first run entry will be run, and the second will be filtered out. If run with --priority 2 , both run entries will be run.","title":"Run Entries Configuration Files"},{"location":"run_entries_configuration_files/#run-entries-configuration-files","text":"In the tutorial, we have been using --run-entries to specify run entries for helm-run . However, we can also put the run entries into a run entries configuration file , and then pass the file to helm-run using the --conf-file flag. This has a number of advantages: This prevents the command line invocation of helm-run from getting too long when a large number of run entries are run. The run entries configuration file can be shared with other users and commited to Git. For example, instead of running: helm-run --run-specs mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite tutorial --max-eval-instances 10 You can instead create a tutorial_run_entries.conf file in your current working directory: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10","title":"Run Entries Configuration Files"},{"location":"run_entries_configuration_files/#model-run-expander-wildcards","text":"It is very common to use run entries configuration file with a model run expander wildcards e.g. model=text . For instance, entries: [ {description: \"mmlu:subject=anatomy,model=text\", priority: 1}, {description: \"mmlu:subject=philosophy,model=text\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 --models-to-run openai/gpt2 This has exactly the same behavior has the previous example. For more information on model run expander wildcards, refer to the run entry format documentation.","title":"Model Run Expander Wildcards"},{"location":"run_entries_configuration_files/#priorities","text":"You can use the --priority flag in conjunction with --conf-file . This filters out run entries with a higher priority value than the specified --priority value. For instance, with this run entries configuration file: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 2}, ] If run with --priority 1 , only the first run entry will be run, and the second will be filtered out. If run with --priority 2 , both run entries will be run.","title":"Priorities"},{"location":"scenarios/","text":"Scenarios ::: helm.benchmark.scenarios options: filters: [\"^(?!test_).+_scenario$\", \"Scenario$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Scenarios"},{"location":"scenarios/#scenarios","text":"::: helm.benchmark.scenarios options: filters: [\"^(?!test_).+_scenario$\", \"Scenario$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Scenarios"},{"location":"schemas/","text":"div.doc-function { display: none; } Schemas ::: helm.benchmark.scenarios.scenario.Scenario ::: helm.benchmark.adaptation.scenario_state.ScenarioState ::: helm.benchmark.adaptation.request_state.RequestState ::: helm.benchmark.scenarios.scenario.Instance ::: helm.benchmark.scenarios.scenario.Reference ::: helm.benchmark.augmentations.perturbation_description.PerturbationDescription ::: helm.common.request.Request ::: helm.common.request.RequestResult ::: helm.benchmark.metrics.metric.PerInstanceStats ::: helm.benchmark.metrics.statistic.Stat","title":"Schemas"},{"location":"schemas/#schemas","text":"::: helm.benchmark.scenarios.scenario.Scenario ::: helm.benchmark.adaptation.scenario_state.ScenarioState ::: helm.benchmark.adaptation.request_state.RequestState ::: helm.benchmark.scenarios.scenario.Instance ::: helm.benchmark.scenarios.scenario.Reference ::: helm.benchmark.augmentations.perturbation_description.PerturbationDescription ::: helm.common.request.Request ::: helm.common.request.RequestResult ::: helm.benchmark.metrics.metric.PerInstanceStats ::: helm.benchmark.metrics.statistic.Stat","title":"Schemas"},{"location":"skill_10_copyright_compliance/","text":"Skill 10: Copyright Compliance","title":"Skill 10: Copyright Compliance"},{"location":"skill_10_copyright_compliance/#skill-10-copyright-compliance","text":"","title":"Skill 10: Copyright Compliance"},{"location":"skill_1_research_planning/","text":"Skill 1: Research Planning","title":"Skill 1: Research Planning"},{"location":"skill_1_research_planning/#skill-1-research-planning","text":"","title":"Skill 1: Research Planning"},{"location":"skill_2_strategic_stopping/","text":"Skill 2: Strategic Stopping","title":"Skill 2: Strategic Stopping"},{"location":"skill_2_strategic_stopping/#skill-2-strategic-stopping","text":"","title":"Skill 2: Strategic Stopping"},{"location":"skill_3_known_authority/","text":"Skill 3: Known Authority","title":"Skill 3: Known Authority"},{"location":"skill_3_known_authority/#skill-3-known-authority","text":"","title":"Skill 3: Known Authority"},{"location":"skill_4_unknown_authority/","text":"Skill 4: Unknown Authority","title":"Skill 4: Unknown Authority"},{"location":"skill_4_unknown_authority/#skill-4-unknown-authority","text":"","title":"Skill 4: Unknown Authority"},{"location":"skill_5_validate_authority/","text":"Skill 5: Validate Authority","title":"Skill 5: Validate Authority"},{"location":"skill_5_validate_authority/#skill-5-validate-authority","text":"","title":"Skill 5: Validate Authority"},{"location":"skill_6_fact_extraction/","text":"Skill 6: Fact Extraction","title":"Skill 6: Fact Extraction"},{"location":"skill_6_fact_extraction/#skill-6-fact-extraction","text":"","title":"Skill 6: Fact Extraction"},{"location":"skill_7_distinguish_cases/","text":"Skill 7: Distinguish Cases","title":"Skill 7: Distinguish Cases"},{"location":"skill_7_distinguish_cases/#skill-7-distinguish-cases","text":"","title":"Skill 7: Distinguish Cases"},{"location":"skill_8_synthesize_results/","text":"Skill 8: Synthesize Results","title":"Skill 8: Synthesize Results"},{"location":"skill_8_synthesize_results/#skill-8-synthesize-results","text":"","title":"Skill 8: Synthesize Results"},{"location":"skill_9_citation_integrity/","text":"Skill 9: Citation Integrity The Skill Citation integrity is the obligation to ensure all cited legal authorities actually exist and accurately support the propositions for which they are cited. This skill maps directly to the lawyer's ethical duty of candor to the tribunal\u2014a duty that admits no technological excuse. Why Citation Fails Differently in AI When a lawyer cites \"347 U.S. 483,\" she performs an act of reference \u2014she looked up the case, confirmed the volume and page, and transcribed it. The citation points to something that exists externally and independently. Language models do not cite by reference. They cite by generation . The distinction is consequential: generation is vulnerable in ways that retrieval is not. This vulnerability has produced real sanctions. In Mata v. Avianca (S.D.N.Y. 2023), counsel submitted AI-generated citations to cases that did not exist. The court imposed monetary sanctions and required the attorneys to notify the judges falsely cited as authors of the fabricated opinions. Similar incidents have occurred across jurisdictions, establishing citation hallucination as a documented professional hazard. The Benchmark L-10 adopts the hallucination benchmark developed by Dahl, Magesh, Suzgun, and Ho at Stanford RegLab\u2014the first systematic empirical study of legal hallucination prevalence across major language models. L-10 extracts three sub-tasks that directly test whether a model fabricates legal authorities: Sub-Task Prompt Structure Scoring Cite-ID Case name \u2192 \"Give the full citation.\" Exact/fuzzy match on reporter + volume + page Quote-Prove Case name + citation \u2192 \"Provide a quotation from the majority opinion.\" Verbatim span found in opinion text Authority-Prove Case name + citation \u2192 \"Name one case cited in the opinion.\" Cited authority appears in opinion's citation list Cite-ID catches fabricated citations directly. Quote-Prove catches fabricated quotations attributed to real cases. Authority-Prove catches \"citation laundering\"\u2014confident attribution of authorities the source case never cited. Execution Mode: Closed-Book L-10 tests citation integrity under closed-book conditions, following the Dahl methodology. The model receives no retrieval access and must answer from parametric memory alone. This tests the model's intrinsic reliability: when asked for a citation, does it produce a real one or fabricate? A model that hallucinates citations from memory will likely do so in production whenever retrieval fails or returns nothing relevant. Validation Data (held by evaluator): - Case metadata (name + canonical citation) - Majority opinion text (for Quote-Prove validation) - Extracted citation list (for Authority-Prove validation) Metrics Metric Definition Cite-ID Pass Rate Percentage of correct citation identifications Quote-Prove Pass Rate Percentage of quotations verified in source text Authority-Prove Pass Rate Percentage of cited authorities found in opinion Citation Hallucination Rate (CHR) 1 \u2212 mean(pass rates) All three sub-task rates are reported individually. CHR provides a single summary metric. Dataset Source: reglab/legal_hallucinations Original Study: Dahl, M., Magesh, V., Suzgun, M., & Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis , 16, 64\u201393. Limitations Coverage: The Dahl dataset focuses on U.S. federal courts. State court citations, administrative decisions, and non-U.S. legal systems are not evaluated. Scope: L-10 tests citation existence and provenance , not whether a real citation supports the proposition for which it is cited. Substantive accuracy is addressed in S7 (Distinguishing Cases) and S8 (Synthesizing Results). References Dahl, M., Magesh, V., Suzgun, M., & Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis , 16, 64\u201393. American Bar Association. (1992). Legal Education and Professional Development: An Educational Continuum (MacCrate Report). American Association of Law Libraries. (2013). Principles and Standards for Legal Research Competency . Schwartz, P. (2023). Mata v. Avianca , No. 22-cv-1461 (S.D.N.Y. June 22, 2023) (order imposing sanctions).","title":"Skill 9: Citation Integrity"},{"location":"skill_9_citation_integrity/#skill-9-citation-integrity","text":"","title":"Skill 9: Citation Integrity"},{"location":"skill_9_citation_integrity/#the-skill","text":"Citation integrity is the obligation to ensure all cited legal authorities actually exist and accurately support the propositions for which they are cited. This skill maps directly to the lawyer's ethical duty of candor to the tribunal\u2014a duty that admits no technological excuse.","title":"The Skill"},{"location":"skill_9_citation_integrity/#why-citation-fails-differently-in-ai","text":"When a lawyer cites \"347 U.S. 483,\" she performs an act of reference \u2014she looked up the case, confirmed the volume and page, and transcribed it. The citation points to something that exists externally and independently. Language models do not cite by reference. They cite by generation . The distinction is consequential: generation is vulnerable in ways that retrieval is not. This vulnerability has produced real sanctions. In Mata v. Avianca (S.D.N.Y. 2023), counsel submitted AI-generated citations to cases that did not exist. The court imposed monetary sanctions and required the attorneys to notify the judges falsely cited as authors of the fabricated opinions. Similar incidents have occurred across jurisdictions, establishing citation hallucination as a documented professional hazard.","title":"Why Citation Fails Differently in AI"},{"location":"skill_9_citation_integrity/#the-benchmark","text":"L-10 adopts the hallucination benchmark developed by Dahl, Magesh, Suzgun, and Ho at Stanford RegLab\u2014the first systematic empirical study of legal hallucination prevalence across major language models. L-10 extracts three sub-tasks that directly test whether a model fabricates legal authorities: Sub-Task Prompt Structure Scoring Cite-ID Case name \u2192 \"Give the full citation.\" Exact/fuzzy match on reporter + volume + page Quote-Prove Case name + citation \u2192 \"Provide a quotation from the majority opinion.\" Verbatim span found in opinion text Authority-Prove Case name + citation \u2192 \"Name one case cited in the opinion.\" Cited authority appears in opinion's citation list Cite-ID catches fabricated citations directly. Quote-Prove catches fabricated quotations attributed to real cases. Authority-Prove catches \"citation laundering\"\u2014confident attribution of authorities the source case never cited.","title":"The Benchmark"},{"location":"skill_9_citation_integrity/#execution-mode-closed-book","text":"L-10 tests citation integrity under closed-book conditions, following the Dahl methodology. The model receives no retrieval access and must answer from parametric memory alone. This tests the model's intrinsic reliability: when asked for a citation, does it produce a real one or fabricate? A model that hallucinates citations from memory will likely do so in production whenever retrieval fails or returns nothing relevant. Validation Data (held by evaluator): - Case metadata (name + canonical citation) - Majority opinion text (for Quote-Prove validation) - Extracted citation list (for Authority-Prove validation)","title":"Execution Mode: Closed-Book"},{"location":"skill_9_citation_integrity/#metrics","text":"Metric Definition Cite-ID Pass Rate Percentage of correct citation identifications Quote-Prove Pass Rate Percentage of quotations verified in source text Authority-Prove Pass Rate Percentage of cited authorities found in opinion Citation Hallucination Rate (CHR) 1 \u2212 mean(pass rates) All three sub-task rates are reported individually. CHR provides a single summary metric.","title":"Metrics"},{"location":"skill_9_citation_integrity/#dataset","text":"Source: reglab/legal_hallucinations Original Study: Dahl, M., Magesh, V., Suzgun, M., & Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis , 16, 64\u201393.","title":"Dataset"},{"location":"skill_9_citation_integrity/#limitations","text":"Coverage: The Dahl dataset focuses on U.S. federal courts. State court citations, administrative decisions, and non-U.S. legal systems are not evaluated. Scope: L-10 tests citation existence and provenance , not whether a real citation supports the proposition for which it is cited. Substantive accuracy is addressed in S7 (Distinguishing Cases) and S8 (Synthesizing Results).","title":"Limitations"},{"location":"skill_9_citation_integrity/#references","text":"Dahl, M., Magesh, V., Suzgun, M., & Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis , 16, 64\u201393. American Bar Association. (1992). Legal Education and Professional Development: An Educational Continuum (MacCrate Report). American Association of Law Libraries. (2013). Principles and Standards for Legal Research Competency . Schwartz, P. (2023). Mata v. Avianca , No. 22-cv-1461 (S.D.N.Y. June 22, 2023) (order imposing sanctions).","title":"References"},{"location":"standards/","text":"Standards","title":"Standards"},{"location":"standards/#standards","text":"","title":"Standards"},{"location":"tutorial/","text":"Tutorial This tutorial will explain how to use the HELM command line tools to run benchmarks, aggregate statistics, and visualize results. We will run two runs using the mmlu scenario on the openai/gpt2 model. The mmlu scenario implements the Massive Multitask Language (MMLU) benchmark from this paper , and consists of a Question Answering (QA) task using a dataset with questions from 57 subjects such as elementary mathematics, US history, computer science, law, and more. Note that GPT-2 performs poorly on MMLU, so this is just a proof of concept. We will run two runs: the first using questions about anatomy, and the second using questions about philosophy. Using helm-run helm-run is a command line tool for running benchmarks. To run this benchmark using the HELM command-line tools, we need to specify run entries that describes the desired runs. For this example, the run entries are mmlu:subject=anatomy,model=openai/gpt2 (for anatomy) and mmlu:subject=philosophy,model=openai/gpt2 (for philosophy). We will now use helm-run to execute the runs. Run this command: helm-run --run-entries mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite my-suite --max-eval-instances 10 The meaning of the arguments are as follows: --run-entries specifies the run entries from the desired runs. --suite specifies a subdirectory under the output directory in which all the output will be placed. --max-eval-instances limits evaluation to only N instances (i.e. items) from the benchmark, using a randomly shuffled order of instances. helm-run creates an environment directory environment and an output directory by default. The environment directory is prod_env/ by default and can be set using --local-path . Credentials for making API calls should be added to a credentials.conf file in this directory. The output directory is benchmark_output/ by default and can be set using --output-path . After running this command, navigate to the benchmark_output/runs/my-suite/ directory. This should contain a two sub-directories named mmlu:subject=anatomy,model=openai_gpt2 and mmlu:subject=philosophy,model=openai_gpt2 . Note that the names of these sub-directories is based on the run entries we used earlier, but with / replaced with _ . Each output sub-directory will contain several JSON files that were generated during the corresponding run: run_spec.json contains the RunSpec , which specifies the scenario, adapter and metrics for the run. scenario.json contains a serialized Scenario , which contains the scenario for the run and specifies the instances (i.e. inputs) used. scenario_state.json contains a serialized ScenarioState , which contains every request to and response from the model. per_instance_stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics for each instance (i.e. input). stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics, aggregated across all instances (i.e. inputs). Using helm-summarize The helm-summarize reads the output files of helm-run and computes aggregate statistics across runs. Run the following: helm-summarize --suite my-suite This reads the pre-existing files in benchmark_output/runs/my-suite/ that were written by helm-run previously, and writes the following new files back to benchmark_output/runs/my-suite/ : summary.json contains a serialized ExecutiveSummary with a date and suite name. run_specs.json contains the run entries for all the runs. runs.json contains serialized list of Run , which contains the run path, run spec and adapter spec and statistics for each run. groups.json contains a serialized list of Table , each containing information about groups in a group category. groups_metadata.json contains a list of all the groups along with a human-readable description and a taxonomy. Additionally, for each group and group-relavent metric, it will output a pair of files: benchmark_output/runs/my-suite/groups/latex/<group_name>_<metric_name>.tex and benchmark_output/runs/my-suite/groups/json/<group_name>_<metric_name>.json . These files contain the statistics for that metric from each run within the group. Using helm-server Finally, the helm-server command launches a web server to visualize the output files of helm-run and helm-benchmark . Run: helm-server --suite my-suite Open a browser and go to http://localhost:8000/ to view the visualization. You should see a similar view as live website for the paper , but for the data from your benchmark runs. The website has the following sections accessible from the top menu bar: Leaderboards contains the leaderboards with aggregate metrics. Models contains a list of models and their descriptions Scenarios contains a list of scenarios and their descriptions. Predictions contains a searchable list of runs.","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This tutorial will explain how to use the HELM command line tools to run benchmarks, aggregate statistics, and visualize results. We will run two runs using the mmlu scenario on the openai/gpt2 model. The mmlu scenario implements the Massive Multitask Language (MMLU) benchmark from this paper , and consists of a Question Answering (QA) task using a dataset with questions from 57 subjects such as elementary mathematics, US history, computer science, law, and more. Note that GPT-2 performs poorly on MMLU, so this is just a proof of concept. We will run two runs: the first using questions about anatomy, and the second using questions about philosophy.","title":"Tutorial"},{"location":"tutorial/#using-helm-run","text":"helm-run is a command line tool for running benchmarks. To run this benchmark using the HELM command-line tools, we need to specify run entries that describes the desired runs. For this example, the run entries are mmlu:subject=anatomy,model=openai/gpt2 (for anatomy) and mmlu:subject=philosophy,model=openai/gpt2 (for philosophy). We will now use helm-run to execute the runs. Run this command: helm-run --run-entries mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite my-suite --max-eval-instances 10 The meaning of the arguments are as follows: --run-entries specifies the run entries from the desired runs. --suite specifies a subdirectory under the output directory in which all the output will be placed. --max-eval-instances limits evaluation to only N instances (i.e. items) from the benchmark, using a randomly shuffled order of instances. helm-run creates an environment directory environment and an output directory by default. The environment directory is prod_env/ by default and can be set using --local-path . Credentials for making API calls should be added to a credentials.conf file in this directory. The output directory is benchmark_output/ by default and can be set using --output-path . After running this command, navigate to the benchmark_output/runs/my-suite/ directory. This should contain a two sub-directories named mmlu:subject=anatomy,model=openai_gpt2 and mmlu:subject=philosophy,model=openai_gpt2 . Note that the names of these sub-directories is based on the run entries we used earlier, but with / replaced with _ . Each output sub-directory will contain several JSON files that were generated during the corresponding run: run_spec.json contains the RunSpec , which specifies the scenario, adapter and metrics for the run. scenario.json contains a serialized Scenario , which contains the scenario for the run and specifies the instances (i.e. inputs) used. scenario_state.json contains a serialized ScenarioState , which contains every request to and response from the model. per_instance_stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics for each instance (i.e. input). stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics, aggregated across all instances (i.e. inputs).","title":"Using helm-run"},{"location":"tutorial/#using-helm-summarize","text":"The helm-summarize reads the output files of helm-run and computes aggregate statistics across runs. Run the following: helm-summarize --suite my-suite This reads the pre-existing files in benchmark_output/runs/my-suite/ that were written by helm-run previously, and writes the following new files back to benchmark_output/runs/my-suite/ : summary.json contains a serialized ExecutiveSummary with a date and suite name. run_specs.json contains the run entries for all the runs. runs.json contains serialized list of Run , which contains the run path, run spec and adapter spec and statistics for each run. groups.json contains a serialized list of Table , each containing information about groups in a group category. groups_metadata.json contains a list of all the groups along with a human-readable description and a taxonomy. Additionally, for each group and group-relavent metric, it will output a pair of files: benchmark_output/runs/my-suite/groups/latex/<group_name>_<metric_name>.tex and benchmark_output/runs/my-suite/groups/json/<group_name>_<metric_name>.json . These files contain the statistics for that metric from each run within the group.","title":"Using helm-summarize"},{"location":"tutorial/#using-helm-server","text":"Finally, the helm-server command launches a web server to visualize the output files of helm-run and helm-benchmark . Run: helm-server --suite my-suite Open a browser and go to http://localhost:8000/ to view the visualization. You should see a similar view as live website for the paper , but for the data from your benchmark runs. The website has the following sections accessible from the top menu bar: Leaderboards contains the leaderboards with aggregate metrics. Models contains a list of models and their descriptions Scenarios contains a list of scenarios and their descriptions. Predictions contains a searchable list of runs.","title":"Using helm-server"},{"location":"user_guide/","text":"User Guide This guide provides comprehensive documentation for using Legal-10 Benchmark. Getting Started Installation - Set up your environment Quick Start - Run your first evaluation Tutorial - Step-by-step walkthrough Configuration & Setup Credentials - Configure API keys and authentication Run Entries Configuration Files - Configuration file formats Run Entries - Construct and use run entries Advanced Usage Benchmark - Advanced benchmarking features Importing Custom Modules - Extend HELM with custom implementations Extending Legal-10 Adding New Models - Add model support Adding New Scenarios - Create custom scenarios","title":"User Guide"},{"location":"user_guide/#user-guide","text":"This guide provides comprehensive documentation for using Legal-10 Benchmark.","title":"User Guide"},{"location":"user_guide/#getting-started","text":"Installation - Set up your environment Quick Start - Run your first evaluation Tutorial - Step-by-step walkthrough","title":"Getting Started"},{"location":"user_guide/#configuration-setup","text":"Credentials - Configure API keys and authentication Run Entries Configuration Files - Configuration file formats Run Entries - Construct and use run entries","title":"Configuration &amp; Setup"},{"location":"user_guide/#advanced-usage","text":"Benchmark - Advanced benchmarking features Importing Custom Modules - Extend HELM with custom implementations","title":"Advanced Usage"},{"location":"user_guide/#extending-legal-10","text":"Adding New Models - Add model support Adding New Scenarios - Create custom scenarios","title":"Extending Legal-10"},{"location":"who_we_need/","text":"Who We Need","title":"Who We Need"},{"location":"who_we_need/#who-we-need","text":"","title":"Who We Need"},{"location":"why_legal_10/","text":"Why Legal-10","title":"Why Legal-10"},{"location":"why_legal_10/#why-legal-10","text":"","title":"Why Legal-10"}]}