{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Legal-10 Benchmark Introduction Legal AI is already being used to make decisions that affect liberty, employment, housing, immigration status, family stability, and access to counsel. Today's evaluation ecosystem is fragmented, vendor-permissioned, and often optimized for optics rather than end-user protection. Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG). We open-source everything: datasets, harness, scoring, and operational tooling. The only exception is the actual administered test data, which remains private to prevent contamination and prior knowledge. We support evaluation of both open-weight and closed API models via a reproducible run-bundle protocol. Key Principle Grounded in Professional Standards. Legal skills are not one task. MacCrate Report (ABA, 1992) \u2014 foundational lawyering skills AALL Principles & Standards (2013) \u2014 legal research competency standards Shultz & Zedeck (2011) \u2014 empirically-derived lawyer effectiveness factors Every skill maps to reputable frameworks the legal profession has validated over 30+ years. These standards are used to translate human-lawyer competencies into 10 minimal viable tests across distinct cognitive circuits. Testing conducted using three modalities of Agentic Workflow (AG), RAG, or Closed Book (CB). Each skill is tested in the modality that most closely matches how human lawyers perform that skill in computer-mediated work. The 10 Skills Skill Name Dataset Description Modality S1 Research Planning LegalAgentBench Plan and execute multi-step research strategy under budget with clear stop rule Agentic Workflows (AG) S2 Strategic Stopping LegalAgentBench Terminate research at the right time given diminishing returns and uncertainty Agentic Workflows (AG) S3 Known Authority CLERC Resolve a known citation/identifier to the correct authority quickly and exactly Retrieval-Augmented Generation (RAG) S4 Unknown Authority CLERC (semantic) Retrieve relevant law from a fact pattern and rank/select the best authority Retrieval-Augmented Generation (RAG) S5 Validate Authority KeyCite-CLERC Determine whether authority remains good law and summarize negative treatment Retrieval-Augmented Generation (RAG) S6 Fact Extraction CUAD Extract specific facts from long contracts (needle-in-haystack retrieval) Retrieval-Augmented Generation (RAG) S7 Distinguish Cases CaseHOLD Decide whether a case applies or can be distinguished based on facts/reasoning Closed-Book (CB) S8 Synthesize Results LEXam Combine multiple authorities into a cohesive principle, memo, or argument Closed-Book (CB) S9 Citation Integrity Dahl 10-types Avoid fabricated authority; ensure grounding and adverse authority obligations Closed-Book (CB) S10 Copyright Compliance SHIELD Refuse protected content and minimize verbatim memorization Closed-Book (CB) Extensions # Name Dataset Description 1 Multilingual Reasoning FairLex Evaluate legal reasoning across jurisdictions/languages; test quantization transparency Design Principles Objectivity Fixed spec per version \u2014 changes bump the version No selective omission or quiet withdrawal \u2014 full eval split or no score Transparency Open-source harness \u2014 datasets, scoring, tools Auditable run bundles \u2014 SHA-256 hashes, signed manifests Public append-only log \u2014 all submissions preserved Fairness Comparable baselines \u2014 open and closed models use identical harness Modality-matched testing \u2014 CB vs RAG vs AG reflects real lawyer workflows Gaming-Resistant No selective omission \u2192 must run all 10 skills Skill-level reporting \u2192 can't hide weak skills in aggregate Visible failure modes \u2192 can't hide how it failed Scoreboard Policy Participation is your choice. We will: List all evaluated systems with linked run bundles (\"Evaluated\") Maintain a neutral \"Not Yet Evaluated\" coverage list Coverage disclosure. Each \"Not Yet Evaluated\" entry includes a neutral status tag (e.g., \"no public endpoint,\" \"access restricted,\" \"awaiting community submission\"). This ensures: Fair comparison across GPT-4, Claude, Llama, and other models using identical evaluation protocols. Development Architecture - HELM Legal-10 uses Stanford CRFM's Holistic Evaluation of Language Models (HELM) framework because it embodies many of the values we strive to establish moving forward: Unified model interface \u2014 open-weight + closed API Standardized adapters \u2014 same prompts for all models Reproducible evaluation \u2014 auditable run bundles Transparent metrics \u2014 open-source scoring Community-Driven Legal-10 is community-governed, not vendor-controlled. We ask for your help in pushing this benchmark initiative. We need a fair and comprehensive benchmark that end users can rely on to better understand the quality implications of a product they are using or considering. Benefits: Co-author credit on papers, governance voting rights, contributor recognition Skill Definitions & Professional Standards Mapping Category of Legal Work Performed by AI or Legal-AI Defining What Skills are Required to Perform Legal Work MacCrate Report: American Bar Association. (1992). Legal education and professional development: An educational continuum (Report of the Task Force on Law Schools and the Profession: Narrowing the Gap), 138\u2013141 (Skill \u00a7 3). AALL Principles: American Association of Law Libraries. (2013). Principles and standards for legal research competencies. Standard II & III. Shultz & Zedeck: Shultz, M. M., & Zedeck, S. (2011). Predicting lawyer effectiveness: Broadening the basis for law school admission decisions. Law & Social Inquiry, 36(3), 620-661. This study operationalizes the MacCrate Report's foundational skills, the AALL's competency standards, and the empirical factors identified by Shultz & Zedeck. Each of the ten categories represents a distinct cognitive \"circuit\" that is required for effective legal practice. Skill 1. Formulating a Research Plan MacCrate Citation: Skill \u00a7 3.3(a): Requiring the ability to \"identify the full range of search strategies\" and \"devising and implementing a coherent and effective research design.\" AALL Citation: Principle II, Standard 1: \"A successful legal researcher... identifies the best search strategy for the research need.\" Shultz & Zedeck: Factor 14: Strategic Planning. \"Plans and strategizes to address present and future issues and goals.\" The Skill: Constructing a systematic research strategy that identifies relevant legal issues, selects appropriate sources and search methods, sequences tasks efficiently, and establishes scope parameters before execution begins. Skill 2. Strategic Stopping MacCrate Citation: Skill \u00a7 3.3(a)(iii): Explicitly requires \"assessing the feasibility of research strategies\" in terms of \"time and financial constraints.\" Skill \u00a7 3.3(b) requires the lawyer to \"monitor the results\" and \"modify the plan,\" which implicitly includes the decision to terminate the plan. AALL Citation: Principle II, Standard 4: \"Recognizes when sufficient research has been done to adequately address the legal issue.\" (This is the literal \"stopping rule\" standard). Shultz & Zedeck: Factor 13: Organizing and Managing One's Own Work. \"Uses time efficiently... allocates time in proportion to importance of task.\" Factor 12 (Strategic Planning) also covers \"allocating resources\" effectively. The Skill: Knowing when sufficient research has been conducted to answer the legal question with a high degree of confidence, and when further searching is diminishing returns or \"churning.\" It involves the cognitive \"switch\" from seeking to synthesizing. Skill 3. Finding Known Authority MacCrate Citation: Skill \u00a7 3.2: \"Knowledge of the Fundamental Tools of Legal Research,\" including the ability to utilize \"primary and secondary sources.\" AALL Citation: Principle II, Standard 2: \"Demonstrates the ability to find the full text of the case or statute... given a legal citation.\" Shultz & Zedeck: Factor 6: Fact Finding. \"Able to identify relevant facts and issues in case.\" The Skill: Precise retrieval of a specific legal authority or record item when the target is already identified by a citation or other unique handle (case cite, statute section, docket entry, rule number, Bates range, exhibit ID). Skill 4. Finding Unknown Authority MacCrate Citation: Skill \u00a7 3.2(a): The ability to use \"secondary sources\" as \"tools for finding primary authority.\" AALL Citation: Principle II, Standard 3: \"Given a legal topic, demonstrate the ability to identify secondary sources... to find relevant primary law.\" Shultz & Zedeck: Factor 7: Researching the Law. \"Utilizes appropriate sources and strategies to identify issues and derive solutions.\" The Skill: Discovering relevant legal authority when the citation to be used is yet unknown. This process involves examining the fact pattern, issue, or question and iteratively retrieving, refining, and selecting controlling/persuasive sources. Skill 5. Updating and Validating Authority MacCrate Citation: Skill \u00a7 3.1: \"Knowledge of the Nature of Legal Rules,\" including understanding \"the hierarchy of authority.\" AALL Citation: Principle III, Standard 2: \"Checks the currency of the law... [and] verifies that the authority is current and still good law.\" Shultz & Zedeck: Factor 5: Researching the Law. (Implicit in identifying current solutions). The Skill: Verifying that a legal authority remains valid by checking subsequent treatment (overruled, distinguished, questioned, followed), legislative amendments, or other developments affecting its precedential value. Skill 6. Fact Extraction MacCrate Citation: Skill \u00a7 4.1: \"Factual Investigation,\" including the ability to \"identify factual issues\" and \"determine the nature of the needed facts.\" AALL Citation: [Gap \u2014 AALL focuses on legal research, not document review. Consider noting this as an extension.] Shultz & Zedeck: Factor 6: Fact Finding. \"Able to identify relevant facts and issues in case.\" The Skill: Identifying, extracting, and organizing legally relevant facts from source documents (contracts, discovery materials, transcripts, filings) to support analysis or case strategy. Skill 7. Distinguishing Cases MacCrate Citation: Skill \u00a7 3.3(c): \"Evaluating information... distinguishing cases on their facts and reasoning.\" AALL Citation: Principle IV, Standard 1: \"Analyzes information to determine its relevance to the specific research need.\" Shultz & Zedeck: Factor 1: Analysis and Reasoning. \"Uses analytical skills, logic, and reasoning to approach problems and to formulate conclusions.\" The Skill: Analyzing whether a precedent applies by comparing material facts, legal issues, and reasoning, and articulating grounds on which it can be distinguished or must be followed. Skill 8. Synthesizing Results MacCrate Citation: Skill \u00a7 2.2: \"Legal Analysis and Reasoning... synthesizing the holdings of multiple cases.\" AALL Citation: Principle IV, Standard 2: \"Integrates the results of the research into a cohesive argument or product.\" Shultz & Zedeck: Factor 9: Writing. \"Writes clearly, efficiently and persuasively.\" The Skill: Integrating holdings, rules, and reasoning from multiple authorities into a coherent legal principle, argument, or work product that addresses the research question comprehensively. Skill 9. Citation Integrity MacCrate Citation: Skill \u00a7 10: \"Recognizing and Resolving Ethical Dilemmas,\" specifically the duty of candor to the tribunal. AALL Citation: Principle V, Standard 1: \"Distinguishes between ethical and unethical uses of information... correctly attributes ideas and text to original sources.\" Shultz & Zedeck: Factor 21: Integrity & Honesty. \"Has core values and beliefs; acts with integrity and honesty.\" The Skill: Ensuring all cited authorities (a) actually exist, (b) accurately support the propositions cited, (c) are properly attributed, and (d) include disclosure of adverse authority as required by professional obligations. Skill 10. Copyright Compliance MacCrate Citation: Skill \u00a7 10: \"Recognizing and Resolving Ethical Dilemmas,\" including compliance with applicable law. AALL Citation: Principle V, Standard 1: \"Distinguishes between ethical and unethical uses of information... respects intellectual property rights.\" Shultz & Zedeck: Factor 21: Integrity & Honesty. \"Acts with integrity and honesty.\" The Skill: Refusing to reproduce protected content verbatim, respecting intellectual property boundaries, and minimizing memorization-based reproduction of copyrighted material. Global Extension Module - Multilingual Reasoning MacCrate Citation: [Gap \u2014 1992 report is US-centric. Consider framing as extension for global legal AI deployment.] AALL Citation: [Gap \u2014 2013 standards are US-focused.] Shultz & Zedeck: Factor 26: \"Able to See the World Through the Eyes of Others.\" Factor 4: Practical Judgment \u2014 applicable across contexts. The Skill: Applying legal reasoning consistently across languages, jurisdictions, and legal traditions without systematic performance degradation or bias introduced by linguistic factors.","title":"Index"},{"location":"#legal-10-benchmark","text":"","title":"Legal-10 Benchmark"},{"location":"#introduction","text":"Legal AI is already being used to make decisions that affect liberty, employment, housing, immigration status, family stability, and access to counsel. Today's evaluation ecosystem is fragmented, vendor-permissioned, and often optimized for optics rather than end-user protection. Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG). We open-source everything: datasets, harness, scoring, and operational tooling. The only exception is the actual administered test data, which remains private to prevent contamination and prior knowledge. We support evaluation of both open-weight and closed API models via a reproducible run-bundle protocol.","title":"Introduction"},{"location":"#key-principle","text":"Grounded in Professional Standards. Legal skills are not one task. MacCrate Report (ABA, 1992) \u2014 foundational lawyering skills AALL Principles & Standards (2013) \u2014 legal research competency standards Shultz & Zedeck (2011) \u2014 empirically-derived lawyer effectiveness factors Every skill maps to reputable frameworks the legal profession has validated over 30+ years. These standards are used to translate human-lawyer competencies into 10 minimal viable tests across distinct cognitive circuits. Testing conducted using three modalities of Agentic Workflow (AG), RAG, or Closed Book (CB). Each skill is tested in the modality that most closely matches how human lawyers perform that skill in computer-mediated work.","title":"Key Principle"},{"location":"#the-10-skills","text":"Skill Name Dataset Description Modality S1 Research Planning LegalAgentBench Plan and execute multi-step research strategy under budget with clear stop rule Agentic Workflows (AG) S2 Strategic Stopping LegalAgentBench Terminate research at the right time given diminishing returns and uncertainty Agentic Workflows (AG) S3 Known Authority CLERC Resolve a known citation/identifier to the correct authority quickly and exactly Retrieval-Augmented Generation (RAG) S4 Unknown Authority CLERC (semantic) Retrieve relevant law from a fact pattern and rank/select the best authority Retrieval-Augmented Generation (RAG) S5 Validate Authority KeyCite-CLERC Determine whether authority remains good law and summarize negative treatment Retrieval-Augmented Generation (RAG) S6 Fact Extraction CUAD Extract specific facts from long contracts (needle-in-haystack retrieval) Retrieval-Augmented Generation (RAG) S7 Distinguish Cases CaseHOLD Decide whether a case applies or can be distinguished based on facts/reasoning Closed-Book (CB) S8 Synthesize Results LEXam Combine multiple authorities into a cohesive principle, memo, or argument Closed-Book (CB) S9 Citation Integrity Dahl 10-types Avoid fabricated authority; ensure grounding and adverse authority obligations Closed-Book (CB) S10 Copyright Compliance SHIELD Refuse protected content and minimize verbatim memorization Closed-Book (CB)","title":"The 10 Skills"},{"location":"#extensions","text":"# Name Dataset Description 1 Multilingual Reasoning FairLex Evaluate legal reasoning across jurisdictions/languages; test quantization transparency","title":"Extensions"},{"location":"#design-principles","text":"","title":"Design Principles"},{"location":"#objectivity","text":"Fixed spec per version \u2014 changes bump the version No selective omission or quiet withdrawal \u2014 full eval split or no score","title":"Objectivity"},{"location":"#transparency","text":"Open-source harness \u2014 datasets, scoring, tools Auditable run bundles \u2014 SHA-256 hashes, signed manifests Public append-only log \u2014 all submissions preserved","title":"Transparency"},{"location":"#fairness","text":"Comparable baselines \u2014 open and closed models use identical harness Modality-matched testing \u2014 CB vs RAG vs AG reflects real lawyer workflows","title":"Fairness"},{"location":"#gaming-resistant","text":"No selective omission \u2192 must run all 10 skills Skill-level reporting \u2192 can't hide weak skills in aggregate Visible failure modes \u2192 can't hide how it failed","title":"Gaming-Resistant"},{"location":"#scoreboard-policy","text":"Participation is your choice. We will: List all evaluated systems with linked run bundles (\"Evaluated\") Maintain a neutral \"Not Yet Evaluated\" coverage list Coverage disclosure. Each \"Not Yet Evaluated\" entry includes a neutral status tag (e.g., \"no public endpoint,\" \"access restricted,\" \"awaiting community submission\"). This ensures: Fair comparison across GPT-4, Claude, Llama, and other models using identical evaluation protocols.","title":"Scoreboard Policy"},{"location":"#development-architecture-helm","text":"Legal-10 uses Stanford CRFM's Holistic Evaluation of Language Models (HELM) framework because it embodies many of the values we strive to establish moving forward: Unified model interface \u2014 open-weight + closed API Standardized adapters \u2014 same prompts for all models Reproducible evaluation \u2014 auditable run bundles Transparent metrics \u2014 open-source scoring","title":"Development Architecture - HELM"},{"location":"#community-driven","text":"Legal-10 is community-governed, not vendor-controlled. We ask for your help in pushing this benchmark initiative. We need a fair and comprehensive benchmark that end users can rely on to better understand the quality implications of a product they are using or considering. Benefits: Co-author credit on papers, governance voting rights, contributor recognition","title":"Community-Driven"},{"location":"#skill-definitions-professional-standards-mapping","text":"","title":"Skill Definitions &amp; Professional Standards Mapping"},{"location":"#category-of-legal-work-performed-by-ai-or-legal-ai","text":"Defining What Skills are Required to Perform Legal Work MacCrate Report: American Bar Association. (1992). Legal education and professional development: An educational continuum (Report of the Task Force on Law Schools and the Profession: Narrowing the Gap), 138\u2013141 (Skill \u00a7 3). AALL Principles: American Association of Law Libraries. (2013). Principles and standards for legal research competencies. Standard II & III. Shultz & Zedeck: Shultz, M. M., & Zedeck, S. (2011). Predicting lawyer effectiveness: Broadening the basis for law school admission decisions. Law & Social Inquiry, 36(3), 620-661. This study operationalizes the MacCrate Report's foundational skills, the AALL's competency standards, and the empirical factors identified by Shultz & Zedeck. Each of the ten categories represents a distinct cognitive \"circuit\" that is required for effective legal practice.","title":"Category of Legal Work Performed by AI or Legal-AI"},{"location":"#skill-1-formulating-a-research-plan","text":"MacCrate Citation: Skill \u00a7 3.3(a): Requiring the ability to \"identify the full range of search strategies\" and \"devising and implementing a coherent and effective research design.\" AALL Citation: Principle II, Standard 1: \"A successful legal researcher... identifies the best search strategy for the research need.\" Shultz & Zedeck: Factor 14: Strategic Planning. \"Plans and strategizes to address present and future issues and goals.\" The Skill: Constructing a systematic research strategy that identifies relevant legal issues, selects appropriate sources and search methods, sequences tasks efficiently, and establishes scope parameters before execution begins.","title":"Skill 1. Formulating a Research Plan"},{"location":"#skill-2-strategic-stopping","text":"MacCrate Citation: Skill \u00a7 3.3(a)(iii): Explicitly requires \"assessing the feasibility of research strategies\" in terms of \"time and financial constraints.\" Skill \u00a7 3.3(b) requires the lawyer to \"monitor the results\" and \"modify the plan,\" which implicitly includes the decision to terminate the plan. AALL Citation: Principle II, Standard 4: \"Recognizes when sufficient research has been done to adequately address the legal issue.\" (This is the literal \"stopping rule\" standard). Shultz & Zedeck: Factor 13: Organizing and Managing One's Own Work. \"Uses time efficiently... allocates time in proportion to importance of task.\" Factor 12 (Strategic Planning) also covers \"allocating resources\" effectively. The Skill: Knowing when sufficient research has been conducted to answer the legal question with a high degree of confidence, and when further searching is diminishing returns or \"churning.\" It involves the cognitive \"switch\" from seeking to synthesizing.","title":"Skill 2. Strategic Stopping"},{"location":"#skill-3-finding-known-authority","text":"MacCrate Citation: Skill \u00a7 3.2: \"Knowledge of the Fundamental Tools of Legal Research,\" including the ability to utilize \"primary and secondary sources.\" AALL Citation: Principle II, Standard 2: \"Demonstrates the ability to find the full text of the case or statute... given a legal citation.\" Shultz & Zedeck: Factor 6: Fact Finding. \"Able to identify relevant facts and issues in case.\" The Skill: Precise retrieval of a specific legal authority or record item when the target is already identified by a citation or other unique handle (case cite, statute section, docket entry, rule number, Bates range, exhibit ID).","title":"Skill 3. Finding Known Authority"},{"location":"#skill-4-finding-unknown-authority","text":"MacCrate Citation: Skill \u00a7 3.2(a): The ability to use \"secondary sources\" as \"tools for finding primary authority.\" AALL Citation: Principle II, Standard 3: \"Given a legal topic, demonstrate the ability to identify secondary sources... to find relevant primary law.\" Shultz & Zedeck: Factor 7: Researching the Law. \"Utilizes appropriate sources and strategies to identify issues and derive solutions.\" The Skill: Discovering relevant legal authority when the citation to be used is yet unknown. This process involves examining the fact pattern, issue, or question and iteratively retrieving, refining, and selecting controlling/persuasive sources.","title":"Skill 4. Finding Unknown Authority"},{"location":"#skill-5-updating-and-validating-authority","text":"MacCrate Citation: Skill \u00a7 3.1: \"Knowledge of the Nature of Legal Rules,\" including understanding \"the hierarchy of authority.\" AALL Citation: Principle III, Standard 2: \"Checks the currency of the law... [and] verifies that the authority is current and still good law.\" Shultz & Zedeck: Factor 5: Researching the Law. (Implicit in identifying current solutions). The Skill: Verifying that a legal authority remains valid by checking subsequent treatment (overruled, distinguished, questioned, followed), legislative amendments, or other developments affecting its precedential value.","title":"Skill 5. Updating and Validating Authority"},{"location":"#skill-6-fact-extraction","text":"MacCrate Citation: Skill \u00a7 4.1: \"Factual Investigation,\" including the ability to \"identify factual issues\" and \"determine the nature of the needed facts.\" AALL Citation: [Gap \u2014 AALL focuses on legal research, not document review. Consider noting this as an extension.] Shultz & Zedeck: Factor 6: Fact Finding. \"Able to identify relevant facts and issues in case.\" The Skill: Identifying, extracting, and organizing legally relevant facts from source documents (contracts, discovery materials, transcripts, filings) to support analysis or case strategy.","title":"Skill 6. Fact Extraction"},{"location":"#skill-7-distinguishing-cases","text":"MacCrate Citation: Skill \u00a7 3.3(c): \"Evaluating information... distinguishing cases on their facts and reasoning.\" AALL Citation: Principle IV, Standard 1: \"Analyzes information to determine its relevance to the specific research need.\" Shultz & Zedeck: Factor 1: Analysis and Reasoning. \"Uses analytical skills, logic, and reasoning to approach problems and to formulate conclusions.\" The Skill: Analyzing whether a precedent applies by comparing material facts, legal issues, and reasoning, and articulating grounds on which it can be distinguished or must be followed.","title":"Skill 7. Distinguishing Cases"},{"location":"#skill-8-synthesizing-results","text":"MacCrate Citation: Skill \u00a7 2.2: \"Legal Analysis and Reasoning... synthesizing the holdings of multiple cases.\" AALL Citation: Principle IV, Standard 2: \"Integrates the results of the research into a cohesive argument or product.\" Shultz & Zedeck: Factor 9: Writing. \"Writes clearly, efficiently and persuasively.\" The Skill: Integrating holdings, rules, and reasoning from multiple authorities into a coherent legal principle, argument, or work product that addresses the research question comprehensively.","title":"Skill 8. Synthesizing Results"},{"location":"#skill-9-citation-integrity","text":"MacCrate Citation: Skill \u00a7 10: \"Recognizing and Resolving Ethical Dilemmas,\" specifically the duty of candor to the tribunal. AALL Citation: Principle V, Standard 1: \"Distinguishes between ethical and unethical uses of information... correctly attributes ideas and text to original sources.\" Shultz & Zedeck: Factor 21: Integrity & Honesty. \"Has core values and beliefs; acts with integrity and honesty.\" The Skill: Ensuring all cited authorities (a) actually exist, (b) accurately support the propositions cited, (c) are properly attributed, and (d) include disclosure of adverse authority as required by professional obligations.","title":"Skill 9. Citation Integrity"},{"location":"#skill-10-copyright-compliance","text":"MacCrate Citation: Skill \u00a7 10: \"Recognizing and Resolving Ethical Dilemmas,\" including compliance with applicable law. AALL Citation: Principle V, Standard 1: \"Distinguishes between ethical and unethical uses of information... respects intellectual property rights.\" Shultz & Zedeck: Factor 21: Integrity & Honesty. \"Acts with integrity and honesty.\" The Skill: Refusing to reproduce protected content verbatim, respecting intellectual property boundaries, and minimizing memorization-based reproduction of copyrighted material.","title":"Skill 10. Copyright Compliance"},{"location":"#global-extension-module-multilingual-reasoning","text":"MacCrate Citation: [Gap \u2014 1992 report is US-centric. Consider framing as extension for global legal AI deployment.] AALL Citation: [Gap \u2014 2013 standards are US-focused.] Shultz & Zedeck: Factor 26: \"Able to See the World Through the Eyes of Others.\" Factor 4: Practical Judgment \u2014 applicable across contexts. The Skill: Applying legal reasoning consistently across languages, jurisdictions, and legal traditions without systematic performance degradation or bias introduced by linguistic factors.","title":"Global Extension Module - Multilingual Reasoning"},{"location":"adding_new_models/","text":"Adding New Models HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network HELM comes with many built-in Client classes (i.e. model API clients) and Tokenizer clients. If there is already an existing Client and Tokenizer class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM. If you wish to evaluate a model not covered by an existing Client and Tokenizer , you can implement your own Client and Tokenizer subclasses. Instructions for adding custom Client and Tokenizer subclasses will be added to the documentation in the future. Adding a Model Locally Model Metadata Create a local model metadata configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelMetadataList object. For an example of this format, refer to model_metadata.yaml in the GitHub repository, or follow the example below: models: - name: eleutherai/pythia-70m display_name: Pythia (70M) description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. creator_organization_name: EleutherAI access: open num_parameters: 95600000 release_date: 2023-02-13 tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG] Model Deployment A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model google/gemma-2-9b-it has the model deployments together/gemma-2-9b-it (remote inference using Together AI's API) and google/gemma-2-9b-it (local inference with Hugging Face). Create a local model deployments configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelDeployments object. For an example of this format, refer to model_deployments.yaml in the GitHub repository, or follow an example below for your preferred model platform. Note that the model deployment name will frequently differ from the model name. The model deployment name should be $HOST_ORGANIZATON/$MODEL_NAME , while the model name should be $CREATOR_ORGANIZATON/$MODEL_NAME . Hugging Face Example: model_deployments: - name: huggingface/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.huggingface_client.HuggingFaceClient\" args: pretrained_model_name_or_path: EleutherAI/pythia-70m Note: If pretrained_model_name_or_path is omitted, the model will be loaded from Hugging Face Hub using model_name ( not name ) by default. Examples of common arguments within args : Loading from local disk: pretrained_model_name_or_path: /path/to/my/model Revision: revision: my_revision Quantization: load_in_8bit: true Model precision: torch_dtype: torch.float16 Model device: device: cpu or device: cuda:0 Allow running remote code: trust_remote_code: true Multi-GPU: device_map: auto Notes: This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise. Multi-GPU inference can be enabled by setting device_map: auto in the args . GPU models loaded by helm-run will remain loaded on the GPU for the lifespan of helm-run . If evaluating multiple models, it is prudent to evaluate each model with a separate helm-run invocation. If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run huggingface-cli login in your shell. Refer to Hugging Face's documentation for more information. vLLM model_deployments: - name: vllm/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.vllm_client.VLLMClient\" args: base_url: http://mymodelserver:8000/v1/ For non-chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMClient . For chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMChatClient . Set base_url to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with: python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m Together AI model_deployments: - name: together/gemma-2-9b-it model_name: google/gemma-2-9b-it tokenizer_name: google/gemma-2-9b max_sequence_length: 8191 client_spec: class_name: \"helm.clients.together_client.TogetherClient\" args: together_model: google/gemma-2-9b-it Notes: You will need to add Together AI credentials to your credentials file e.g. add togetherApiKey: your-api-key to ./prod_env/credentials.conf . If together_model is omitted, the Together model with model_name ( not name ) will be used by default. This above model may not be currently available on Together AI. Consult Together AI's Inference Models documentation for a list of currently available models and corresponding model strings. Testing New Models After you've added your model, you can run your model with helm-run using a run entry such as mmlu:subject=anatomy,model=your-org/your-model . It is also recommended to use the --disable-cache flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example: helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache helm-summarize --suite my-suite helm-server Adding New Models to HELM If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users. To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at src/helm/config/ , rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at prod_env/ , you should move those changes to the corresponding configuration files in src/helm/config/ - do not add the model to both src/helm/config/ and prod_env/ simulatenously. Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.","title":"Adding New Models"},{"location":"adding_new_models/#adding-new-models","text":"HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network HELM comes with many built-in Client classes (i.e. model API clients) and Tokenizer clients. If there is already an existing Client and Tokenizer class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM. If you wish to evaluate a model not covered by an existing Client and Tokenizer , you can implement your own Client and Tokenizer subclasses. Instructions for adding custom Client and Tokenizer subclasses will be added to the documentation in the future.","title":"Adding New Models"},{"location":"adding_new_models/#adding-a-model-locally","text":"","title":"Adding a Model Locally"},{"location":"adding_new_models/#model-metadata","text":"Create a local model metadata configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelMetadataList object. For an example of this format, refer to model_metadata.yaml in the GitHub repository, or follow the example below: models: - name: eleutherai/pythia-70m display_name: Pythia (70M) description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers. creator_organization_name: EleutherAI access: open num_parameters: 95600000 release_date: 2023-02-13 tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]","title":"Model Metadata"},{"location":"adding_new_models/#model-deployment","text":"A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model google/gemma-2-9b-it has the model deployments together/gemma-2-9b-it (remote inference using Together AI's API) and google/gemma-2-9b-it (local inference with Hugging Face). Create a local model deployments configuration file if it does not already exist. The file should be a prod_env/model_metadata.yaml by default, or at $LOCAL_PATH/model_metadata.yaml if --local-path is set where $LOCAL_FOLDER is the value of the flag. This file should contain a YAML-formatted ModelDeployments object. For an example of this format, refer to model_deployments.yaml in the GitHub repository, or follow an example below for your preferred model platform. Note that the model deployment name will frequently differ from the model name. The model deployment name should be $HOST_ORGANIZATON/$MODEL_NAME , while the model name should be $CREATOR_ORGANIZATON/$MODEL_NAME .","title":"Model Deployment"},{"location":"adding_new_models/#hugging-face","text":"Example: model_deployments: - name: huggingface/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.huggingface_client.HuggingFaceClient\" args: pretrained_model_name_or_path: EleutherAI/pythia-70m Note: If pretrained_model_name_or_path is omitted, the model will be loaded from Hugging Face Hub using model_name ( not name ) by default. Examples of common arguments within args : Loading from local disk: pretrained_model_name_or_path: /path/to/my/model Revision: revision: my_revision Quantization: load_in_8bit: true Model precision: torch_dtype: torch.float16 Model device: device: cpu or device: cuda:0 Allow running remote code: trust_remote_code: true Multi-GPU: device_map: auto Notes: This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise. Multi-GPU inference can be enabled by setting device_map: auto in the args . GPU models loaded by helm-run will remain loaded on the GPU for the lifespan of helm-run . If evaluating multiple models, it is prudent to evaluate each model with a separate helm-run invocation. If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run huggingface-cli login in your shell. Refer to Hugging Face's documentation for more information.","title":"Hugging Face"},{"location":"adding_new_models/#vllm","text":"model_deployments: - name: vllm/pythia-70m model_name: eleutherai/pythia-70m tokenizer_name: EleutherAI/gpt-neox-20b max_sequence_length: 2048 client_spec: class_name: \"helm.clients.vllm_client.VLLMClient\" args: base_url: http://mymodelserver:8000/v1/ For non-chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMClient . For chat models, set class_name in client_spec to helm.clients.vllm_client.VLLMChatClient . Set base_url to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with: python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m","title":"vLLM"},{"location":"adding_new_models/#together-ai","text":"model_deployments: - name: together/gemma-2-9b-it model_name: google/gemma-2-9b-it tokenizer_name: google/gemma-2-9b max_sequence_length: 8191 client_spec: class_name: \"helm.clients.together_client.TogetherClient\" args: together_model: google/gemma-2-9b-it Notes: You will need to add Together AI credentials to your credentials file e.g. add togetherApiKey: your-api-key to ./prod_env/credentials.conf . If together_model is omitted, the Together model with model_name ( not name ) will be used by default. This above model may not be currently available on Together AI. Consult Together AI's Inference Models documentation for a list of currently available models and corresponding model strings.","title":"Together AI"},{"location":"adding_new_models/#testing-new-models","text":"After you've added your model, you can run your model with helm-run using a run entry such as mmlu:subject=anatomy,model=your-org/your-model . It is also recommended to use the --disable-cache flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example: helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache helm-summarize --suite my-suite helm-server","title":"Testing New Models"},{"location":"adding_new_models/#adding-new-models-to-helm","text":"If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users. To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at src/helm/config/ , rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at prod_env/ , you should move those changes to the corresponding configuration files in src/helm/config/ - do not add the model to both src/helm/config/ and prod_env/ simulatenously. Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.","title":"Adding New Models to HELM"},{"location":"adding_new_scenarios/","text":"Adding New Scenarios HELM comes with more than a hundred built-in scenarios. However, you may want to run HELM on a scenario that is not built into HELM yet, or you may want to run HELM on scenarios that use your private datasets. Because HELM is a modular framework with a plug-in architecture, you can run evaluations with your custom scenarios on HELM without needing to modify HELM code. There are two steps to adding a custom scenario: adding the custom Scenario subclass, and adding a custom run spec function. The easiest way to implement the custom Scenario subclass and the custom run spec function would be to copy from an appropriate example and then make the appropriate modifications. Determine the task of your scenario, then find the corresponding example Scenario subclass and run spec function from the list below from the simple_scenarios.py and simple_run_specs.py files: Multiple-choice question answering : SimpleMCQAScenario and get_simple_mcqa_run_spec() Short-answer question answering : SimpleShortAnswerQAScenario and get_simple_short_answer_qa_run_spec() Open-ended question answering : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Summarization : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Multi-class classification : SimpleClassificationScenario and get_simple_classification_run_spec() Sentiment analysis : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Toxicity detection : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Multi-label classification : This is currently unsupported by HELM. Named entity recognition : This is currently unsupported by HELM. If your task is not listed, you may still implement your task using custom adapters and metrics, but there is limited official support for doing so. Custom Scenario subclass For this tutorial, we will create a MyScenario class in the the my_scenario module. Make a file called ./my_scenario.py under the my_scenario directory. Create a new class called MyScenario . Find the appropriate example scenario and copy its implementation into MyScenario , making sure to also copy all the required imports. Now we will create a test for the scenario to make sure that it is working correctly. Create a file called ./my_scenario_test.py under the my_scenario directory. Create a test_my_scenario() function in this file. Find the appropriate example scenario test from test_simple_scenarios.py and copy its implementation into test_my_scenario() . You can now run python3 -m pytest test_my_scenario.py to test the example scenario. The test should pass. If you get a ModuleNotFound error, you should set up your PYTHONPATH as explained above, and then try again. Now, modify MyScenario to include the actual logic to load the instances from your dataset. Modify the test accordingly. Use the test to ensure that your implementation is working. Downloading data to local disk Frequently, your Scenario will want to download and cache data onto the local disk, rather than downloading it from the internet every time. The output_path argument passed into the get_instances() method will contain a file path to a scenario-specific download folder that you should download these files to. The folder will be under the scenarios subdirectory under the benchmark_output/ folder (or the path specified by the --output-path flag for helm-run ). You can use the ensure_directory_exists() and ensure_file_downloaded() helper functions to download files, which has the advantage of skipping the download if the file already exists. You can also use set unpack=True in ensure_file_downloaded() to automatically unpack most archive files (e.g. .tar.gz and .zip files). For examples, refer to: gsm_scenario.py - download a JSONL files mmlu_scenario.py - download CSV files narrativeqa_scenario.py - download a zip file containing CSV files Working with Hugging Face datasets Another frequent use case is downloading data from Hugging Face datasets. You can use load_dataset() to do so. It is recommended that you set the cache_dir parameter to a subdirectory within output_path . This ensures hermeticity by ensuring that the data is downloaded into the scenario-specific download folder. For an example, refer to: math_scenario.py legalbench_scenario.py Custom run spec function A run spec function is the entry point to the scenario. A run spec function produces a RunSpec (a configuration for an evaluation run). helm-run will run the run spec function to get the RunSpec , and then it will run the evaluation defined by that RunSpec . HELM will search for modules with names matching these patterns for run spec functions: helm.benchmark.run_specs.*_run_specs helm_*_run_specs (i.e. a root module) For this tutorial, we will create a get_my_run_spec() function in the helm_my_run_specs module. Under the src/helm/benchmark/scenarios/ directory, create a file called helm_my_run_specs.py . Then, create a get_my_run_spec() function in this file and find the appropriate example run spec function from simple_run_specs.py to copy its implementation into get_my_run_spec() . Change the file accordingly to the needs of your scenario. Now run: helm-run --run-entries custom:model=openai/gpt2 --suite custom --max-eval-instances 5 If you get a ValueError: Unknown run spec name error, you should set up your PYTHONPATH as explained above, and then try again. Debugging with models The above run entry uses the openai/gpt2 model, which is a lightweight model that is reasonably fast, even when using only CPU inference without a GPU. However, you might want to avoid waiting for model inference when implementing a scenario in order to speed up your iteration times. To do so, you can use the simple/model1 , which simply echoes the last word in the prompt. Example helm-run command: helm-run --run-entries custom:model=simple/model1 --suite custom --max-eval-instances 5 Note: Both the custom Scenario subclass and the custom run spec function will be added to custom Python modules that have to be importable by Python. The easiest way to do this is to place your custom Python modules under the current working directory and then run export PYTHONPATH=\".:$PYTHONPATH\" in your shell. Refer to the Importing Custom Modules documentation for other ways to do this. Contributing your scenario We welcome scenario contributions to HELM if they fit the following criteria: It is commonly-used or notable benchmark (e.g. it has a published paper). It uses publicly available datasets. It fills a gap in coverage by HELM's existing scenarios. If your scenario fits this criteria, you should move the files to the conventional HELM locations, and open a pull request. Your *_scenario.py file should be placed in src/helm/benchmark/scenarios/ and your *_run_specs.py file should be placed in src/helm/benchmark/scenarios/ . More documentation on the contributor workflow will be added later.","title":"Adding New Scenarios"},{"location":"adding_new_scenarios/#adding-new-scenarios","text":"HELM comes with more than a hundred built-in scenarios. However, you may want to run HELM on a scenario that is not built into HELM yet, or you may want to run HELM on scenarios that use your private datasets. Because HELM is a modular framework with a plug-in architecture, you can run evaluations with your custom scenarios on HELM without needing to modify HELM code. There are two steps to adding a custom scenario: adding the custom Scenario subclass, and adding a custom run spec function. The easiest way to implement the custom Scenario subclass and the custom run spec function would be to copy from an appropriate example and then make the appropriate modifications. Determine the task of your scenario, then find the corresponding example Scenario subclass and run spec function from the list below from the simple_scenarios.py and simple_run_specs.py files: Multiple-choice question answering : SimpleMCQAScenario and get_simple_mcqa_run_spec() Short-answer question answering : SimpleShortAnswerQAScenario and get_simple_short_answer_qa_run_spec() Open-ended question answering : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Summarization : This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations. Multi-class classification : SimpleClassificationScenario and get_simple_classification_run_spec() Sentiment analysis : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Toxicity detection : This a sub-type of the Classification task. Set input_noun , output_noun and instructions appropriately. Multi-label classification : This is currently unsupported by HELM. Named entity recognition : This is currently unsupported by HELM. If your task is not listed, you may still implement your task using custom adapters and metrics, but there is limited official support for doing so.","title":"Adding New Scenarios"},{"location":"adding_new_scenarios/#custom-scenario-subclass","text":"For this tutorial, we will create a MyScenario class in the the my_scenario module. Make a file called ./my_scenario.py under the my_scenario directory. Create a new class called MyScenario . Find the appropriate example scenario and copy its implementation into MyScenario , making sure to also copy all the required imports. Now we will create a test for the scenario to make sure that it is working correctly. Create a file called ./my_scenario_test.py under the my_scenario directory. Create a test_my_scenario() function in this file. Find the appropriate example scenario test from test_simple_scenarios.py and copy its implementation into test_my_scenario() . You can now run python3 -m pytest test_my_scenario.py to test the example scenario. The test should pass. If you get a ModuleNotFound error, you should set up your PYTHONPATH as explained above, and then try again. Now, modify MyScenario to include the actual logic to load the instances from your dataset. Modify the test accordingly. Use the test to ensure that your implementation is working.","title":"Custom Scenario subclass"},{"location":"adding_new_scenarios/#downloading-data-to-local-disk","text":"Frequently, your Scenario will want to download and cache data onto the local disk, rather than downloading it from the internet every time. The output_path argument passed into the get_instances() method will contain a file path to a scenario-specific download folder that you should download these files to. The folder will be under the scenarios subdirectory under the benchmark_output/ folder (or the path specified by the --output-path flag for helm-run ). You can use the ensure_directory_exists() and ensure_file_downloaded() helper functions to download files, which has the advantage of skipping the download if the file already exists. You can also use set unpack=True in ensure_file_downloaded() to automatically unpack most archive files (e.g. .tar.gz and .zip files). For examples, refer to: gsm_scenario.py - download a JSONL files mmlu_scenario.py - download CSV files narrativeqa_scenario.py - download a zip file containing CSV files","title":"Downloading data to local disk"},{"location":"adding_new_scenarios/#working-with-hugging-face-datasets","text":"Another frequent use case is downloading data from Hugging Face datasets. You can use load_dataset() to do so. It is recommended that you set the cache_dir parameter to a subdirectory within output_path . This ensures hermeticity by ensuring that the data is downloaded into the scenario-specific download folder. For an example, refer to: math_scenario.py legalbench_scenario.py","title":"Working with Hugging Face datasets"},{"location":"adding_new_scenarios/#custom-run-spec-function","text":"A run spec function is the entry point to the scenario. A run spec function produces a RunSpec (a configuration for an evaluation run). helm-run will run the run spec function to get the RunSpec , and then it will run the evaluation defined by that RunSpec . HELM will search for modules with names matching these patterns for run spec functions: helm.benchmark.run_specs.*_run_specs helm_*_run_specs (i.e. a root module) For this tutorial, we will create a get_my_run_spec() function in the helm_my_run_specs module. Under the src/helm/benchmark/scenarios/ directory, create a file called helm_my_run_specs.py . Then, create a get_my_run_spec() function in this file and find the appropriate example run spec function from simple_run_specs.py to copy its implementation into get_my_run_spec() . Change the file accordingly to the needs of your scenario. Now run: helm-run --run-entries custom:model=openai/gpt2 --suite custom --max-eval-instances 5 If you get a ValueError: Unknown run spec name error, you should set up your PYTHONPATH as explained above, and then try again.","title":"Custom run spec function"},{"location":"adding_new_scenarios/#debugging-with-models","text":"The above run entry uses the openai/gpt2 model, which is a lightweight model that is reasonably fast, even when using only CPU inference without a GPU. However, you might want to avoid waiting for model inference when implementing a scenario in order to speed up your iteration times. To do so, you can use the simple/model1 , which simply echoes the last word in the prompt. Example helm-run command: helm-run --run-entries custom:model=simple/model1 --suite custom --max-eval-instances 5 Note: Both the custom Scenario subclass and the custom run spec function will be added to custom Python modules that have to be importable by Python. The easiest way to do this is to place your custom Python modules under the current working directory and then run export PYTHONPATH=\".:$PYTHONPATH\" in your shell. Refer to the Importing Custom Modules documentation for other ways to do this.","title":"Debugging with models"},{"location":"adding_new_scenarios/#contributing-your-scenario","text":"We welcome scenario contributions to HELM if they fit the following criteria: It is commonly-used or notable benchmark (e.g. it has a published paper). It uses publicly available datasets. It fills a gap in coverage by HELM's existing scenarios. If your scenario fits this criteria, you should move the files to the conventional HELM locations, and open a pull request. Your *_scenario.py file should be placed in src/helm/benchmark/scenarios/ and your *_run_specs.py file should be placed in src/helm/benchmark/scenarios/ . More documentation on the contributor workflow will be added later.","title":"Contributing your scenario"},{"location":"adding_new_tokenizers/","text":"Adding New Tokenizers HELM comes with many built-in tokenizers, but in some cases, you may need to add your own custom tokenizer for your custom model. Creating a tokenizer configuration file Create a file called tokenizer_configs.yaml in your local configuration folder (e.g. ./prod_env/tokenizer_configs.yaml ). This file should contain a YAML-formatted TokenizerConfigs object. For an example of this format, refer to the built-in tokenizer_configs.yaml in the GitHub repository, or follow the example below for your preferred model platform. After adding a tokenizer configuration, you can then use the tokenizer in your custom model deployments by setting the specifying the tokenizer name in the tokenizer field of the model deployment. Hugging Face tokenizers To add a Hugging Face tokenizer, follow the format below, setting name to Hugging Face hub model ID. tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" args: pretrained_model_name_or_path: bigscience/bloom end_of_text_token: \"<s>\" prefix_token: \"</s>\" Note that pretrained_model_name_or_path can also be set to a path to load a Hugging Face tokenizer from local disk. If pretrained_model_name_or_path (or args ) is omitted, the model will be loaded from Hugging Face Hub using name as the model ID by default. For example: tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" end_of_text_token: \"<s>\" prefix_token: \"</s>\" To find the values for end_of_text_token and prefix_token , you can run the following Python code snippet below (replacing bigscience/bloom with the Hugging Face Hub model ID). If any special token is unknown, it should be set to the empty string \"\" . from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\") print(f'end_of_text_token: \"{tokenizer.eos_token}\"\\nprefix_token: \"{tokenizer.bos_token}\"') HELM does not auto-infer special token information because some tokenizers on Hugging Face Model Hub may have incorrect or missing special token values. Therefore, you must manually set these values and verify that they are correct.","title":"Adding New Tokenizers"},{"location":"adding_new_tokenizers/#adding-new-tokenizers","text":"HELM comes with many built-in tokenizers, but in some cases, you may need to add your own custom tokenizer for your custom model.","title":"Adding New Tokenizers"},{"location":"adding_new_tokenizers/#creating-a-tokenizer-configuration-file","text":"Create a file called tokenizer_configs.yaml in your local configuration folder (e.g. ./prod_env/tokenizer_configs.yaml ). This file should contain a YAML-formatted TokenizerConfigs object. For an example of this format, refer to the built-in tokenizer_configs.yaml in the GitHub repository, or follow the example below for your preferred model platform. After adding a tokenizer configuration, you can then use the tokenizer in your custom model deployments by setting the specifying the tokenizer name in the tokenizer field of the model deployment.","title":"Creating a tokenizer configuration file"},{"location":"adding_new_tokenizers/#hugging-face-tokenizers","text":"To add a Hugging Face tokenizer, follow the format below, setting name to Hugging Face hub model ID. tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" args: pretrained_model_name_or_path: bigscience/bloom end_of_text_token: \"<s>\" prefix_token: \"</s>\" Note that pretrained_model_name_or_path can also be set to a path to load a Hugging Face tokenizer from local disk. If pretrained_model_name_or_path (or args ) is omitted, the model will be loaded from Hugging Face Hub using name as the model ID by default. For example: tokenizer_configs: - name: bigscience/bloom tokenizer_spec: class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\" end_of_text_token: \"<s>\" prefix_token: \"</s>\" To find the values for end_of_text_token and prefix_token , you can run the following Python code snippet below (replacing bigscience/bloom with the Hugging Face Hub model ID). If any special token is unknown, it should be set to the empty string \"\" . from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\") print(f'end_of_text_token: \"{tokenizer.eos_token}\"\\nprefix_token: \"{tokenizer.bos_token}\"') HELM does not auto-infer special token information because some tokenizers on Hugging Face Model Hub may have incorrect or missing special token values. Therefore, you must manually set these values and verify that they are correct.","title":"Hugging Face tokenizers"},{"location":"architecture_assessment/","text":"HELM Framework Architectural Assessment for Legal-10 Integration Executive Summary The HELM framework provides a clean extension architecture that allows Legal-10 to integrate completely without touching core framework code. The system uses: Inheritance-based extension (scenarios, metrics, adapters) Decorator-based registration (run specs via @run_spec_function ) Factory pattern instantiation (scenarios, metrics via class names) YAML-based configuration (models, deployments) 1. Constitutional Boundaries - DO NOT TOUCH These files form the backbone of HELM's execution engine and must NOT be modified when integrating Legal-10. Core Framework Files Category Files Purpose Why Not to Touch Base Classes scenario.py adapter.py metric.py run_spec.py Define contracts all extensions must follow Frozen dataclasses and abstract methods form the interface contract Execution Engine runner.py executor.py scenario_state.py request_state.py Orchestrate benchmark execution pipeline Changing flow logic breaks all existing benchmarks Registration config_registry.py model_metadata_registry.py model_deployment_registry.py Discover and register components Central registry used by entire framework Factory/Discovery adapter_factory.py run_spec_factory.py object_spec.py Instantiate components from specs Routing logic for all adapter/scenario creation Specific Files (Absolute Paths) DO NOT MODIFY: /src/helm/benchmark/scenarios/scenario.py - Base Scenario class /src/helm/benchmark/run_spec.py - RunSpec dataclass and decorator /src/helm/benchmark/adaptation/adapter_spec.py - AdapterSpec and constants /src/helm/benchmark/adaptation/adapters/adapter.py - Base Adapter class /src/helm/benchmark/metrics/metric.py - Base Metric class /src/helm/benchmark/runner.py - Execution orchestrator /src/helm/benchmark/executor.py - Request executor /src/helm/benchmark/model_metadata_registry.py - Model registry /src/helm/benchmark/model_deployment_registry.py - Deployment registry /src/helm/benchmark/config_registry.py - Configuration loader Key Principle: Never modify frozen dataclasses, abstract methods, or execution flow logic. 2. Safe Extension Points - Legal-10 Implementation These are the designed interfaces for adding Legal-10 content. What to CREATE (not modify) src/helm/benchmark/ \u251c\u2500\u2500 scenarios/ \u2502 \u251c\u2500\u2500 legal_10_clerc_scenario.py \u2190 S3, S4: Known/Unknown Authority \u2502 \u251c\u2500\u2500 legal_10_keycite_scenario.py \u2190 S5: Validate Authority \u2502 \u251c\u2500\u2500 legal_10_cuad_scenario.py \u2190 S6: Fact Extraction \u2502 \u251c\u2500\u2500 legal_10_lexam_scenario.py \u2190 S8: Synthesize Results \u2502 \u251c\u2500\u2500 legal_10_citation_scenario.py \u2190 S9: Citation Integrity \u2502 \u251c\u2500\u2500 legal_10_shield_scenario.py \u2190 S10: Copyright Compliance \u2502 \u2514\u2500\u2500 legal_10_agentbench_scenario.py \u2190 S1, S2: Research Planning/Stopping \u2502 \u251c\u2500\u2500 run_specs/ \u2502 \u2514\u2500\u2500 legal_10_run_specs.py \u2190 All 10 @run_spec_function defs \u2502 \u2514\u2500\u2500 metrics/ \u2514\u2500\u2500 legal_10_metrics.py \u2190 Custom metrics if needed docs/ \u251c\u2500\u2500 legal_10_overview.md \u2190 Benchmark introduction \u251c\u2500\u2500 legal_10_scenarios.md \u2190 Scenario details \u2514\u2500\u2500 legal_10_setup.md \u2190 Setup guide A. Adding Custom Scenarios Pattern: Create Scenario subclass inheriting from Scenario ABC Location: Create new files in /src/helm/benchmark/scenarios/ Template: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT ) from helm.common.general import ensure_directory_exists class Legal10CLERCScenario(Scenario): \"\"\" CLERC: Citation Resolution benchmark for Legal-10 Tests known authority retrieval (S3) \"\"\" name = \"legal_10_clerc\" description = \"CLERC: Known Authority Retrieval\" tags = [\"legal\", \"retrieval\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Load CLERC dataset # Create Instance objects with Input and References instances = [] # Example instance creation instance = Instance( input=Input(text=\"Find 42 U.S.C. \u00a7 1983\"), references=[ Reference(Output(text=\"correct_citation\"), tags=[CORRECT_TAG]) ], split=TEST_SPLIT, id=\"clerc_001\" ) instances.append(instance) return instances Extension Points: - Subclass Scenario - Implement get_instances(output_path) method - Set name , description , tags class variables - Optional: Override get_metadata() for custom display B. Adding Custom Run Specs Pattern: Create function decorated with @run_spec_function(name) Location: Create new file /src/helm/benchmark/run_specs/legal_10_run_specs.py Template: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) @run_spec_function(\"legal_10_clerc\") def get_legal_10_clerc_spec() -> RunSpec: \"\"\"S3: Known Authority - CLERC dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario\", args={} ) adapter_spec = get_generation_adapter_spec( instructions=\"Retrieve the legal authority for the given citation.\", input_noun=\"Citation\", output_noun=\"Authority\", max_tokens=512 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_clerc\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_rag\"] ) @run_spec_function(\"legal_10_casehold\") def get_legal_10_casehold_spec() -> RunSpec: \"\"\"S7: Distinguish Cases - CaseHOLD dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario\", args={} ) adapter_spec = get_multiple_choice_adapter_spec( method=\"ADAPT_MULTIPLE_CHOICE_JOINT\", instructions=\"Which holding is most relevant?\", input_noun=\"Passage\", output_noun=\"Answer\", max_train_instances=2 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_casehold\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_closed_book\"] ) Discovery Mechanism: - Decorator @run_spec_function(\"name\") automatically registers the function - Dynamic discovery via discover_run_spec_functions() scans all run_specs/ modules - Invoked via CLI: helm-run --run-specs legal_10_clerc C. Adding Custom Metrics (Optional) Pattern: Subclass Metric and implement evaluate() method Location: Create /src/helm/benchmark/metrics/legal_10_metrics.py Template: from typing import List from helm.benchmark.metrics.metric import ( Metric, MetricSpec, MetricResult, PerInstanceStats, Stat ) from helm.benchmark.metrics.metric_name import MetricName from helm.benchmark.adaptation.scenario_state import ScenarioState from helm.benchmark.metrics.metric_service import MetricService class CitationValidationMetric(Metric): \"\"\"Custom metric for citation integrity (S9)\"\"\" def evaluate( self, scenario_state: ScenarioState, metric_service: MetricService, eval_cache_path: str, parallelism: int ) -> MetricResult: # Custom evaluation logic stats = [] per_instance_stats = [] for request_state in scenario_state.request_states: # Validate citation exists and is correctly formatted citation_valid = self._validate_citation(request_state.result.completions[0].text) per_instance_stats.append(PerInstanceStats( instance_id=request_state.instance.id, trial_index=0, stats=[Stat(MetricName(\"citation_valid\")).add(1 if citation_valid else 0)] )) # Aggregate stats total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1) stats.append(Stat(MetricName(\"citation_accuracy\")).add(total_valid / len(per_instance_stats))) return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats) def _validate_citation(self, text: str) -> bool: # Implementation for citation validation return True # Placeholder # Helper function for run specs def get_legal_10_citation_metric_specs() -> List[MetricSpec]: return [ MetricSpec( class_name=\"helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric\", args={} ) ] When to Create Custom Metrics: - Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice - For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping) D. Adapters (Usually NOT Needed) Note: HELM provides standard adapters that handle most use cases: - GenerationAdapter - For open-ended generation (most Legal-10 tasks) - MultipleChoiceJointAdapter - For multiple choice (S7: CaseHOLD) - ChatAdapter - For chat-based models Only create custom adapters if you need specialized prompt formatting beyond what adapter_spec.instructions provides. 3. Data Flow Architecture Complete flow from input to output (unchanged by Legal-10): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Input: RunSpec (name, scenario, adapter, metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SCENARIO CREATION \u2502 \u2502 Runner.run_one() \u2192 create_scenario(scenario_spec) \u2502 \u2502 \u2192 Calls Scenario.get_instances(output_path) \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA PREPROCESSING (Optional) \u2502 \u2502 DataPreprocessor.preprocess(instances, parallelism) \u2502 \u2502 Applies data augmentations if specified \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 ADAPTATION: Instance \u2192 Request \u2502 \u2502 AdapterFactory.get_adapter(adapter_spec) \u2502 \u2502 \u2192 Adapter.adapt(instances, parallelism) \u2502 \u2502 Converts each Instance to RequestState(s) \u2502 \u2502 Returns: ScenarioState (list of RequestState) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 EXECUTION: Request \u2192 Result \u2502 \u2502 Executor.execute(scenario_state) \u2502 \u2502 - Parallel execution (parallelism parameter) \u2502 \u2502 - Makes API/local calls via client \u2502 \u2502 - Populates RequestState.result \u2502 \u2502 Returns: ScenarioState (with results filled in) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS EVALUATION \u2502 \u2502 For each MetricSpec: \u2502 \u2502 metric = create_metric(metric_spec) \u2502 \u2502 metric.evaluate(scenario_state, metric_service, ...) \u2502 \u2502 Returns: MetricResult \u2502 \u2502 - aggregated_stats: List[Stat] (global metrics) \u2502 \u2502 - per_instance_stats: List[PerInstanceStats] (per item) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Output Files (written to benchmark_output/runs/): \u2502 \u2502 - run_spec.json (RunSpec as JSON) \u2502 \u2502 - scenario.json (Scenario instances) \u2502 \u2502 - scenario_state.json (RequestState list) \u2502 \u2502 - stats.json (aggregated metrics) \u2502 \u2502 - per_instance_stats.json (per-item metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key Data Structures: 1. Instance: Single input data point with references 2. RequestState: Instance + Request + Result + metadata 3. ScenarioState: Collection of RequestStates (immutable) 4. Stat: Single metric value with MetricName context 5. PerInstanceStats: Stats grouped by (instance_id, trial_index) 4. Configuration & Discovery Systems A. Run Spec Discovery Location: All files in /src/helm/benchmark/run_specs/ Mechanism: Decorator @run_spec_function(\"name\") registers automatically Discovery: discover_run_spec_functions() uses pkgutil.iter_modules() to scan package Registry: _REGISTERED_RUN_SPEC_FUNCTIONS dict Access: get_run_spec_function(name) looks up by name For Legal-10: Create legal_10_run_specs.py in run_specs/ directory B. Model Metadata Discovery Built-in: /src/helm/config/model_metadata.yaml Local Override: $LOCAL_PATH/model_metadata.yaml Format: YAML with models list of ModelMetadata objects Registration: register_model_metadata(metadata) adds to ALL_MODELS_METADATA Access: get_model_metadata(name) retrieves by name C. Model Deployment Discovery Built-in: /src/helm/config/model_deployments.yaml Local Override: $LOCAL_PATH/model_deployments.yaml Format: YAML with model_deployments list + client_spec class_name Registration: register_model_deployment(deployment) adds to registry Access: get_model_deployment(name) retrieves by name D. Scenario Discovery NO central registry - scenarios found via ScenarioSpec.class_name Pattern: class_name: \"helm.benchmark.scenarios.module.ClassName\" Loading: create_scenario(scenario_spec) \u2192 create_object() \u2192 import + instantiate 5. Legal-10 Specific Implementation Roadmap Legal-10 Skills Mapping Skill Name Dataset Modality Status S1 Research Planning LegalAgentBench AG To Implement S2 Strategic Stopping LegalAgentBench AG To Implement S3 Known Authority CLERC RAG To Implement S4 Unknown Authority CLERC (semantic) RAG To Implement S5 Validate Authority KeyCite-CLERC RAG To Implement S6 Fact Extraction CUAD RAG To Implement S7 Distinguish Cases CaseHOLD CB \u2705 Exists S8 Synthesize Results LEXam CB To Implement S9 Citation Integrity Dahl 10-types CB To Implement S10 Copyright Compliance SHIELD CB To Implement Implementation Checklist Phase 1: Setup - [ ] Create /src/helm/benchmark/scenarios/legal_10/ subdirectory (optional) - [ ] Create /src/helm/benchmark/run_specs/legal_10_run_specs.py - [ ] Create /docs/legal_10_overview.md - [ ] Update /mkdocs.yml navigation Phase 2: Closed-Book Scenarios (S7-S10) - [ ] S7: Verify existing casehold_scenario.py works - [ ] S8: Create legal_10_lexam_scenario.py - [ ] S9: Create legal_10_citation_scenario.py - [ ] S10: Create legal_10_shield_scenario.py Phase 3: RAG Scenarios (S3-S6) - [ ] S3, S4: Create legal_10_clerc_scenario.py - [ ] S5: Create legal_10_keycite_scenario.py - [ ] S6: Create legal_10_cuad_scenario.py Phase 4: Agentic Scenarios (S1-S2) - [ ] S1, S2: Create legal_10_agentbench_scenario.py - [ ] Consider custom metrics for research planning quality Phase 5: Integration - [ ] Create all @run_spec_function entries in legal_10_run_specs.py - [ ] Add Legal-10 group to /src/helm/benchmark/static/schema_legal.yaml - [ ] Create run entries in /src/helm/benchmark/presentation/run_entries_legal.conf - [ ] Write unit tests for each scenario Phase 6: Documentation - [ ] Complete /docs/legal_10_setup.md - [ ] Complete /docs/legal_10_scenarios.md - [ ] Update skill pages with implementation details 6. DO NOT TOUCH vs SAFE TO EXTEND Summary \u274c ABSOLUTELY DO NOT MODIFY Frozen dataclasses: RunSpec , AdapterSpec , Instance , Reference , Output , Input , RequestState , ScenarioState Abstract base classes: Scenario.get_instances() , Adapter.adapt() , Metric.evaluate() Core execution: Runner.run_one() , Executor.execute() Registries and discovery: @run_spec_function decorator, discover_run_spec_functions() , config registry Factory patterns: AdapterFactory.get_adapter() , create_object() , create_scenario() Core routing: Model metadata/deployment registries \u2705 SAFE TO EXTEND (Create new, don't modify existing) Scenarios: Create new *_scenario.py files, implement Scenario subclass Run Specs: Create new *_run_specs.py files, use @run_spec_function() decorator Metrics: Create new *_metrics.py files, subclass Metric , provide MetricSpec helper Configuration: Add local YAML files, don't modify built-in config YAML Documentation: Add new .md files, update navigation in mkdocs.yml 7. Testing Your Integration Verify Run Spec Discovery # List all available run specs (should include legal_10_*) helm-run --help # Describe a specific Legal-10 run spec helm-run --run-specs legal_10_clerc --describe Run a Single Scenario # Run Legal-10 CaseHOLD scenario helm-run \\ --run-specs legal_10_casehold \\ --suite legal_10 \\ --max-eval-instances 10 \\ --output-path benchmark_output/test_run Run All Legal-10 Scenarios # Run complete Legal-10 suite helm-run \\ --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \\ --suite legal_10 \\ --models-to-run anthropic/claude-3-sonnet-20240229 \\ --output-path benchmark_output/legal_10_full 8. Integration Best Practices Naming Conventions Component Convention Example Scenario file legal_10_<dataset>_scenario.py legal_10_clerc_scenario.py Scenario class Legal10<Name>Scenario Legal10CLERCScenario Run spec function get_legal_10_<name>_spec() get_legal_10_clerc_spec() Run spec name legal_10_<name> \"legal_10_clerc\" Groups [\"legal_10\", \"legal_10_<modality>\"] [\"legal_10\", \"legal_10_rag\"] Tags Lowercase, hyphenated [\"legal\", \"retrieval\"] Import Patterns Standard imports for scenarios: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists Standard imports for run specs: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) Error Handling Scenarios should raise ValueError for invalid configuration Use hlog() from helm.common.hierarchical_logger for logging Let exceptions propagate - HELM's runner handles them gracefully Performance Use ensure_directory_exists() before file operations Cache downloaded datasets in output_path Use parallelism parameter for concurrent processing Avoid loading entire datasets into memory if possible Conclusion HELM's architecture provides clear separation between: - Framework code (do not touch) - Extension points (safe to add Legal-10 content) By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are: Modular: Each skill is an independent scenario Discoverable: Run specs auto-register via decorator Maintainable: Clear separation from core framework Upgradeable: HELM updates won't break Legal-10 code The key is to extend, never modify - inherit from base classes, use decorators, and create new files rather than changing existing ones.","title":"HELM Framework Architectural Assessment for Legal-10 Integration"},{"location":"architecture_assessment/#helm-framework-architectural-assessment-for-legal-10-integration","text":"","title":"HELM Framework Architectural Assessment for Legal-10 Integration"},{"location":"architecture_assessment/#executive-summary","text":"The HELM framework provides a clean extension architecture that allows Legal-10 to integrate completely without touching core framework code. The system uses: Inheritance-based extension (scenarios, metrics, adapters) Decorator-based registration (run specs via @run_spec_function ) Factory pattern instantiation (scenarios, metrics via class names) YAML-based configuration (models, deployments)","title":"Executive Summary"},{"location":"architecture_assessment/#1-constitutional-boundaries-do-not-touch","text":"These files form the backbone of HELM's execution engine and must NOT be modified when integrating Legal-10.","title":"1. Constitutional Boundaries - DO NOT TOUCH"},{"location":"architecture_assessment/#core-framework-files","text":"Category Files Purpose Why Not to Touch Base Classes scenario.py adapter.py metric.py run_spec.py Define contracts all extensions must follow Frozen dataclasses and abstract methods form the interface contract Execution Engine runner.py executor.py scenario_state.py request_state.py Orchestrate benchmark execution pipeline Changing flow logic breaks all existing benchmarks Registration config_registry.py model_metadata_registry.py model_deployment_registry.py Discover and register components Central registry used by entire framework Factory/Discovery adapter_factory.py run_spec_factory.py object_spec.py Instantiate components from specs Routing logic for all adapter/scenario creation","title":"Core Framework Files"},{"location":"architecture_assessment/#specific-files-absolute-paths","text":"DO NOT MODIFY: /src/helm/benchmark/scenarios/scenario.py - Base Scenario class /src/helm/benchmark/run_spec.py - RunSpec dataclass and decorator /src/helm/benchmark/adaptation/adapter_spec.py - AdapterSpec and constants /src/helm/benchmark/adaptation/adapters/adapter.py - Base Adapter class /src/helm/benchmark/metrics/metric.py - Base Metric class /src/helm/benchmark/runner.py - Execution orchestrator /src/helm/benchmark/executor.py - Request executor /src/helm/benchmark/model_metadata_registry.py - Model registry /src/helm/benchmark/model_deployment_registry.py - Deployment registry /src/helm/benchmark/config_registry.py - Configuration loader Key Principle: Never modify frozen dataclasses, abstract methods, or execution flow logic.","title":"Specific Files (Absolute Paths)"},{"location":"architecture_assessment/#2-safe-extension-points-legal-10-implementation","text":"These are the designed interfaces for adding Legal-10 content.","title":"2. Safe Extension Points - Legal-10 Implementation"},{"location":"architecture_assessment/#what-to-create-not-modify","text":"src/helm/benchmark/ \u251c\u2500\u2500 scenarios/ \u2502 \u251c\u2500\u2500 legal_10_clerc_scenario.py \u2190 S3, S4: Known/Unknown Authority \u2502 \u251c\u2500\u2500 legal_10_keycite_scenario.py \u2190 S5: Validate Authority \u2502 \u251c\u2500\u2500 legal_10_cuad_scenario.py \u2190 S6: Fact Extraction \u2502 \u251c\u2500\u2500 legal_10_lexam_scenario.py \u2190 S8: Synthesize Results \u2502 \u251c\u2500\u2500 legal_10_citation_scenario.py \u2190 S9: Citation Integrity \u2502 \u251c\u2500\u2500 legal_10_shield_scenario.py \u2190 S10: Copyright Compliance \u2502 \u2514\u2500\u2500 legal_10_agentbench_scenario.py \u2190 S1, S2: Research Planning/Stopping \u2502 \u251c\u2500\u2500 run_specs/ \u2502 \u2514\u2500\u2500 legal_10_run_specs.py \u2190 All 10 @run_spec_function defs \u2502 \u2514\u2500\u2500 metrics/ \u2514\u2500\u2500 legal_10_metrics.py \u2190 Custom metrics if needed docs/ \u251c\u2500\u2500 legal_10_overview.md \u2190 Benchmark introduction \u251c\u2500\u2500 legal_10_scenarios.md \u2190 Scenario details \u2514\u2500\u2500 legal_10_setup.md \u2190 Setup guide","title":"What to CREATE (not modify)"},{"location":"architecture_assessment/#a-adding-custom-scenarios","text":"Pattern: Create Scenario subclass inheriting from Scenario ABC Location: Create new files in /src/helm/benchmark/scenarios/ Template: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT ) from helm.common.general import ensure_directory_exists class Legal10CLERCScenario(Scenario): \"\"\" CLERC: Citation Resolution benchmark for Legal-10 Tests known authority retrieval (S3) \"\"\" name = \"legal_10_clerc\" description = \"CLERC: Known Authority Retrieval\" tags = [\"legal\", \"retrieval\", \"legal_10\"] def get_instances(self, output_path: str) -> List[Instance]: # Load CLERC dataset # Create Instance objects with Input and References instances = [] # Example instance creation instance = Instance( input=Input(text=\"Find 42 U.S.C. \u00a7 1983\"), references=[ Reference(Output(text=\"correct_citation\"), tags=[CORRECT_TAG]) ], split=TEST_SPLIT, id=\"clerc_001\" ) instances.append(instance) return instances Extension Points: - Subclass Scenario - Implement get_instances(output_path) method - Set name , description , tags class variables - Optional: Override get_metadata() for custom display","title":"A. Adding Custom Scenarios"},{"location":"architecture_assessment/#b-adding-custom-run-specs","text":"Pattern: Create function decorated with @run_spec_function(name) Location: Create new file /src/helm/benchmark/run_specs/legal_10_run_specs.py Template: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs ) @run_spec_function(\"legal_10_clerc\") def get_legal_10_clerc_spec() -> RunSpec: \"\"\"S3: Known Authority - CLERC dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario\", args={} ) adapter_spec = get_generation_adapter_spec( instructions=\"Retrieve the legal authority for the given citation.\", input_noun=\"Citation\", output_noun=\"Authority\", max_tokens=512 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_clerc\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_rag\"] ) @run_spec_function(\"legal_10_casehold\") def get_legal_10_casehold_spec() -> RunSpec: \"\"\"S7: Distinguish Cases - CaseHOLD dataset\"\"\" scenario_spec = ScenarioSpec( class_name=\"helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario\", args={} ) adapter_spec = get_multiple_choice_adapter_spec( method=\"ADAPT_MULTIPLE_CHOICE_JOINT\", instructions=\"Which holding is most relevant?\", input_noun=\"Passage\", output_noun=\"Answer\", max_train_instances=2 ) metric_specs = get_exact_match_metric_specs() return RunSpec( name=\"legal_10_casehold\", scenario_spec=scenario_spec, adapter_spec=adapter_spec, metric_specs=metric_specs, groups=[\"legal_10\", \"legal_10_closed_book\"] ) Discovery Mechanism: - Decorator @run_spec_function(\"name\") automatically registers the function - Dynamic discovery via discover_run_spec_functions() scans all run_specs/ modules - Invoked via CLI: helm-run --run-specs legal_10_clerc","title":"B. Adding Custom Run Specs"},{"location":"architecture_assessment/#c-adding-custom-metrics-optional","text":"Pattern: Subclass Metric and implement evaluate() method Location: Create /src/helm/benchmark/metrics/legal_10_metrics.py Template: from typing import List from helm.benchmark.metrics.metric import ( Metric, MetricSpec, MetricResult, PerInstanceStats, Stat ) from helm.benchmark.metrics.metric_name import MetricName from helm.benchmark.adaptation.scenario_state import ScenarioState from helm.benchmark.metrics.metric_service import MetricService class CitationValidationMetric(Metric): \"\"\"Custom metric for citation integrity (S9)\"\"\" def evaluate( self, scenario_state: ScenarioState, metric_service: MetricService, eval_cache_path: str, parallelism: int ) -> MetricResult: # Custom evaluation logic stats = [] per_instance_stats = [] for request_state in scenario_state.request_states: # Validate citation exists and is correctly formatted citation_valid = self._validate_citation(request_state.result.completions[0].text) per_instance_stats.append(PerInstanceStats( instance_id=request_state.instance.id, trial_index=0, stats=[Stat(MetricName(\"citation_valid\")).add(1 if citation_valid else 0)] )) # Aggregate stats total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1) stats.append(Stat(MetricName(\"citation_accuracy\")).add(total_valid / len(per_instance_stats))) return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats) def _validate_citation(self, text: str) -> bool: # Implementation for citation validation return True # Placeholder # Helper function for run specs def get_legal_10_citation_metric_specs() -> List[MetricSpec]: return [ MetricSpec( class_name=\"helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric\", args={} ) ] When to Create Custom Metrics: - Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice - For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping)","title":"C. Adding Custom Metrics (Optional)"},{"location":"architecture_assessment/#d-adapters-usually-not-needed","text":"Note: HELM provides standard adapters that handle most use cases: - GenerationAdapter - For open-ended generation (most Legal-10 tasks) - MultipleChoiceJointAdapter - For multiple choice (S7: CaseHOLD) - ChatAdapter - For chat-based models Only create custom adapters if you need specialized prompt formatting beyond what adapter_spec.instructions provides.","title":"D. Adapters (Usually NOT Needed)"},{"location":"architecture_assessment/#3-data-flow-architecture","text":"Complete flow from input to output (unchanged by Legal-10): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Input: RunSpec (name, scenario, adapter, metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 SCENARIO CREATION \u2502 \u2502 Runner.run_one() \u2192 create_scenario(scenario_spec) \u2502 \u2502 \u2192 Calls Scenario.get_instances(output_path) \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 DATA PREPROCESSING (Optional) \u2502 \u2502 DataPreprocessor.preprocess(instances, parallelism) \u2502 \u2502 Applies data augmentations if specified \u2502 \u2502 Returns: List[Instance] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 ADAPTATION: Instance \u2192 Request \u2502 \u2502 AdapterFactory.get_adapter(adapter_spec) \u2502 \u2502 \u2192 Adapter.adapt(instances, parallelism) \u2502 \u2502 Converts each Instance to RequestState(s) \u2502 \u2502 Returns: ScenarioState (list of RequestState) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 EXECUTION: Request \u2192 Result \u2502 \u2502 Executor.execute(scenario_state) \u2502 \u2502 - Parallel execution (parallelism parameter) \u2502 \u2502 - Makes API/local calls via client \u2502 \u2502 - Populates RequestState.result \u2502 \u2502 Returns: ScenarioState (with results filled in) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 METRICS EVALUATION \u2502 \u2502 For each MetricSpec: \u2502 \u2502 metric = create_metric(metric_spec) \u2502 \u2502 metric.evaluate(scenario_state, metric_service, ...) \u2502 \u2502 Returns: MetricResult \u2502 \u2502 - aggregated_stats: List[Stat] (global metrics) \u2502 \u2502 - per_instance_stats: List[PerInstanceStats] (per item) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Output Files (written to benchmark_output/runs/): \u2502 \u2502 - run_spec.json (RunSpec as JSON) \u2502 \u2502 - scenario.json (Scenario instances) \u2502 \u2502 - scenario_state.json (RequestState list) \u2502 \u2502 - stats.json (aggregated metrics) \u2502 \u2502 - per_instance_stats.json (per-item metrics) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Key Data Structures: 1. Instance: Single input data point with references 2. RequestState: Instance + Request + Result + metadata 3. ScenarioState: Collection of RequestStates (immutable) 4. Stat: Single metric value with MetricName context 5. PerInstanceStats: Stats grouped by (instance_id, trial_index)","title":"3. Data Flow Architecture"},{"location":"architecture_assessment/#4-configuration-discovery-systems","text":"","title":"4. Configuration &amp; Discovery Systems"},{"location":"architecture_assessment/#a-run-spec-discovery","text":"Location: All files in /src/helm/benchmark/run_specs/ Mechanism: Decorator @run_spec_function(\"name\") registers automatically Discovery: discover_run_spec_functions() uses pkgutil.iter_modules() to scan package Registry: _REGISTERED_RUN_SPEC_FUNCTIONS dict Access: get_run_spec_function(name) looks up by name For Legal-10: Create legal_10_run_specs.py in run_specs/ directory","title":"A. Run Spec Discovery"},{"location":"architecture_assessment/#b-model-metadata-discovery","text":"Built-in: /src/helm/config/model_metadata.yaml Local Override: $LOCAL_PATH/model_metadata.yaml Format: YAML with models list of ModelMetadata objects Registration: register_model_metadata(metadata) adds to ALL_MODELS_METADATA Access: get_model_metadata(name) retrieves by name","title":"B. Model Metadata Discovery"},{"location":"architecture_assessment/#c-model-deployment-discovery","text":"Built-in: /src/helm/config/model_deployments.yaml Local Override: $LOCAL_PATH/model_deployments.yaml Format: YAML with model_deployments list + client_spec class_name Registration: register_model_deployment(deployment) adds to registry Access: get_model_deployment(name) retrieves by name","title":"C. Model Deployment Discovery"},{"location":"architecture_assessment/#d-scenario-discovery","text":"NO central registry - scenarios found via ScenarioSpec.class_name Pattern: class_name: \"helm.benchmark.scenarios.module.ClassName\" Loading: create_scenario(scenario_spec) \u2192 create_object() \u2192 import + instantiate","title":"D. Scenario Discovery"},{"location":"architecture_assessment/#5-legal-10-specific-implementation-roadmap","text":"","title":"5. Legal-10 Specific Implementation Roadmap"},{"location":"architecture_assessment/#legal-10-skills-mapping","text":"Skill Name Dataset Modality Status S1 Research Planning LegalAgentBench AG To Implement S2 Strategic Stopping LegalAgentBench AG To Implement S3 Known Authority CLERC RAG To Implement S4 Unknown Authority CLERC (semantic) RAG To Implement S5 Validate Authority KeyCite-CLERC RAG To Implement S6 Fact Extraction CUAD RAG To Implement S7 Distinguish Cases CaseHOLD CB \u2705 Exists S8 Synthesize Results LEXam CB To Implement S9 Citation Integrity Dahl 10-types CB To Implement S10 Copyright Compliance SHIELD CB To Implement","title":"Legal-10 Skills Mapping"},{"location":"architecture_assessment/#implementation-checklist","text":"Phase 1: Setup - [ ] Create /src/helm/benchmark/scenarios/legal_10/ subdirectory (optional) - [ ] Create /src/helm/benchmark/run_specs/legal_10_run_specs.py - [ ] Create /docs/legal_10_overview.md - [ ] Update /mkdocs.yml navigation Phase 2: Closed-Book Scenarios (S7-S10) - [ ] S7: Verify existing casehold_scenario.py works - [ ] S8: Create legal_10_lexam_scenario.py - [ ] S9: Create legal_10_citation_scenario.py - [ ] S10: Create legal_10_shield_scenario.py Phase 3: RAG Scenarios (S3-S6) - [ ] S3, S4: Create legal_10_clerc_scenario.py - [ ] S5: Create legal_10_keycite_scenario.py - [ ] S6: Create legal_10_cuad_scenario.py Phase 4: Agentic Scenarios (S1-S2) - [ ] S1, S2: Create legal_10_agentbench_scenario.py - [ ] Consider custom metrics for research planning quality Phase 5: Integration - [ ] Create all @run_spec_function entries in legal_10_run_specs.py - [ ] Add Legal-10 group to /src/helm/benchmark/static/schema_legal.yaml - [ ] Create run entries in /src/helm/benchmark/presentation/run_entries_legal.conf - [ ] Write unit tests for each scenario Phase 6: Documentation - [ ] Complete /docs/legal_10_setup.md - [ ] Complete /docs/legal_10_scenarios.md - [ ] Update skill pages with implementation details","title":"Implementation Checklist"},{"location":"architecture_assessment/#6-do-not-touch-vs-safe-to-extend-summary","text":"","title":"6. DO NOT TOUCH vs SAFE TO EXTEND Summary"},{"location":"architecture_assessment/#absolutely-do-not-modify","text":"Frozen dataclasses: RunSpec , AdapterSpec , Instance , Reference , Output , Input , RequestState , ScenarioState Abstract base classes: Scenario.get_instances() , Adapter.adapt() , Metric.evaluate() Core execution: Runner.run_one() , Executor.execute() Registries and discovery: @run_spec_function decorator, discover_run_spec_functions() , config registry Factory patterns: AdapterFactory.get_adapter() , create_object() , create_scenario() Core routing: Model metadata/deployment registries","title":"\u274c ABSOLUTELY DO NOT MODIFY"},{"location":"architecture_assessment/#safe-to-extend-create-new-dont-modify-existing","text":"Scenarios: Create new *_scenario.py files, implement Scenario subclass Run Specs: Create new *_run_specs.py files, use @run_spec_function() decorator Metrics: Create new *_metrics.py files, subclass Metric , provide MetricSpec helper Configuration: Add local YAML files, don't modify built-in config YAML Documentation: Add new .md files, update navigation in mkdocs.yml","title":"\u2705 SAFE TO EXTEND (Create new, don't modify existing)"},{"location":"architecture_assessment/#7-testing-your-integration","text":"","title":"7. Testing Your Integration"},{"location":"architecture_assessment/#verify-run-spec-discovery","text":"# List all available run specs (should include legal_10_*) helm-run --help # Describe a specific Legal-10 run spec helm-run --run-specs legal_10_clerc --describe","title":"Verify Run Spec Discovery"},{"location":"architecture_assessment/#run-a-single-scenario","text":"# Run Legal-10 CaseHOLD scenario helm-run \\ --run-specs legal_10_casehold \\ --suite legal_10 \\ --max-eval-instances 10 \\ --output-path benchmark_output/test_run","title":"Run a Single Scenario"},{"location":"architecture_assessment/#run-all-legal-10-scenarios","text":"# Run complete Legal-10 suite helm-run \\ --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \\ --suite legal_10 \\ --models-to-run anthropic/claude-3-sonnet-20240229 \\ --output-path benchmark_output/legal_10_full","title":"Run All Legal-10 Scenarios"},{"location":"architecture_assessment/#8-integration-best-practices","text":"","title":"8. Integration Best Practices"},{"location":"architecture_assessment/#naming-conventions","text":"Component Convention Example Scenario file legal_10_<dataset>_scenario.py legal_10_clerc_scenario.py Scenario class Legal10<Name>Scenario Legal10CLERCScenario Run spec function get_legal_10_<name>_spec() get_legal_10_clerc_spec() Run spec name legal_10_<name> \"legal_10_clerc\" Groups [\"legal_10\", \"legal_10_<modality>\"] [\"legal_10\", \"legal_10_rag\"] Tags Lowercase, hyphenated [\"legal\", \"retrieval\"]","title":"Naming Conventions"},{"location":"architecture_assessment/#import-patterns","text":"Standard imports for scenarios: from typing import List from helm.benchmark.scenarios.scenario import ( Scenario, Instance, Input, Output, Reference, TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG ) from helm.common.general import ensure_directory_exists Standard imports for run specs: from helm.benchmark.run_spec import RunSpec, run_spec_function from helm.benchmark.scenarios.scenario import ScenarioSpec from helm.benchmark.adaptation.common_adapter_specs import ( get_generation_adapter_spec, get_multiple_choice_adapter_spec ) from helm.benchmark.metrics.common_metric_specs import ( get_exact_match_metric_specs, get_f1_metric_specs )","title":"Import Patterns"},{"location":"architecture_assessment/#error-handling","text":"Scenarios should raise ValueError for invalid configuration Use hlog() from helm.common.hierarchical_logger for logging Let exceptions propagate - HELM's runner handles them gracefully","title":"Error Handling"},{"location":"architecture_assessment/#performance","text":"Use ensure_directory_exists() before file operations Cache downloaded datasets in output_path Use parallelism parameter for concurrent processing Avoid loading entire datasets into memory if possible","title":"Performance"},{"location":"architecture_assessment/#conclusion","text":"HELM's architecture provides clear separation between: - Framework code (do not touch) - Extension points (safe to add Legal-10 content) By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are: Modular: Each skill is an independent scenario Discoverable: Run specs auto-register via decorator Maintainable: Clear separation from core framework Upgradeable: HELM updates won't break Legal-10 code The key is to extend, never modify - inherit from base classes, use decorators, and create new files rather than changing existing ones.","title":"Conclusion"},{"location":"benchmark/","text":"Advanced Benchmarking Guide Running Restricted Benchmarks Some of the benchmarks (NewsQA) depend on data that's not public: all such data will be stored in the restricted directory. You need to make sure that directory exists. Dry Runs The helm-run provides several flags that can be used to test that the configuration and scenario are working correctly without actually sending requests to the model # Just load the config file helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --skip-instances # Create the instances and the requests, but don't send requests to the model helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --dry-run Estimating Token Usage To estimate token usage without making any requests, append the --dry-run option: helm-run -r <RunSpec to estimate token usage> --suite $SUITE --max-eval-instances <Number of eval instances> --dry-run and check the output in benchmark_output/runs/$SUITE . sum indicates the estimated total number of tokens used for the specific RunSpec . For the OpenAI models, we use a GPT-2 Tokenizer to estimate the token usage. The tokenizer will be downloaded and cached when running a dry run. Perspective API We use Google's Perspective API to calculate the toxicity of completions. To send requests to PerspectiveAPI, we need to generate an API key from GCP. Follow the Get Started guide to request the service and the Enable the API guide to generate the API key. Once you have a valid API key, add an entry to credentials.conf : perspectiveApiKey: <Generated API key> By default, Perspective API allows only 1 query per second. Fill out this form to increase the request quota.","title":"Advanced Benchmarking Guide"},{"location":"benchmark/#advanced-benchmarking-guide","text":"","title":"Advanced Benchmarking Guide"},{"location":"benchmark/#running-restricted-benchmarks","text":"Some of the benchmarks (NewsQA) depend on data that's not public: all such data will be stored in the restricted directory. You need to make sure that directory exists.","title":"Running Restricted Benchmarks"},{"location":"benchmark/#dry-runs","text":"The helm-run provides several flags that can be used to test that the configuration and scenario are working correctly without actually sending requests to the model # Just load the config file helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --skip-instances # Create the instances and the requests, but don't send requests to the model helm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --dry-run","title":"Dry Runs"},{"location":"benchmark/#estimating-token-usage","text":"To estimate token usage without making any requests, append the --dry-run option: helm-run -r <RunSpec to estimate token usage> --suite $SUITE --max-eval-instances <Number of eval instances> --dry-run and check the output in benchmark_output/runs/$SUITE . sum indicates the estimated total number of tokens used for the specific RunSpec . For the OpenAI models, we use a GPT-2 Tokenizer to estimate the token usage. The tokenizer will be downloaded and cached when running a dry run.","title":"Estimating Token Usage"},{"location":"benchmark/#perspective-api","text":"We use Google's Perspective API to calculate the toxicity of completions. To send requests to PerspectiveAPI, we need to generate an API key from GCP. Follow the Get Started guide to request the service and the Enable the API guide to generate the API key. Once you have a valid API key, add an entry to credentials.conf : perspectiveApiKey: <Generated API key> By default, Perspective API allows only 1 query per second. Fill out this form to increase the request quota.","title":"Perspective API"},{"location":"code/","text":"Code Structure Warning \u2014 The document is stale and was last modified more than ten months ago. The information below may be outdated and incorrect. Please proceed with caution! Birds-Eye View Here's a birds-eye view of how the benchmarking process interacts with the main classes (see benchmark ): A Scenario (given by a ScenarioSpec ) specifies a task and a data distribution. It specifies a set of Instance s, where each Instance has an input (e.g., question) and a set of Reference outputs (e.g., multiple choice answers). A DataPreprocessor takes in a Scenario and produces a list of Instance s. Each Instance is given a unique ID. The set of Instance s is augmented according to DataAugmenterSpec . An Adapter (given by an AdaptationSpec ) takes a list of Instance s and adapts it to a set of Request s to the API (e.g., the model, temperature, number of in-context training examples). Formally, the output is a ScenarioState containing a set of RequestState s, where each RequestState consists of a Request and any metadata used to track the role of this Request (e.g., the relevant Instance and Reference ). An Executor (given by an ExecutionSpec ) executes each Request in the RequestState to produce a RequestResult for each one; everything is encapsulated in a ScenarioState . A Metric (given by a MetricSpec ) takes a ScenarioState containing RequestResults s and produces a set of Stat s (e.g., accuracy, accuracy@5, toxicity, bias, etc.). A Runner is the top-level controller that runs the above steps and is driven by a set of RunSpec s. There are three types of classes: Specifications (e.g., AdapterSpec , ExecutionSpec , RunSpec ): specified manually by the user. Note that Scenario and Metric are subclassed, so they are constructed by ObjectSpec , which specifies the subclass name and a free-form dictionary of arguments. States (e.g., Instance , ScenarioState , Request , RequestResult ): these are automatically generated and can be serialized. Controllers (e.g., Scenario , Adapter , Executor , Metric , Runner ): these have the bulk of the code and should not be serialized. Adding new scenarios In order to implement new scenarios: Create a new Python file in the scenarios folder. Within the scenario file, create a Scenario class, e.g. YourScenario . YourScenario should implement get_instances , a method that downloads the dataset files if they don't already exist and returns a list of Instance s. Each Instance must have a list of (potentially one) Reference answers: a correct answer may be indicated with a CORRECT_TAG in a Reference instance's tags argument. In addition, you must specify the split of the Instance as one of TRAIN_SPLIT , VALID_SPLIT , or TEST_SPLIT constants as in scenario.py . For Scenario s with datasets that cannot be publicly shared, place a copy of the dataset at path restricted/<Name of the Scenario> and read from that path. See NewsQAScenario and ICEScenario for some examples. Note that you need not enumerate every possible correct answer (nor must there even necessarily be a correct answer). Make sure to document your scenario well with a clear docstring. In addition, specify its name , description , and tags . Identify the appropriate metric for your task in one of the *_metrics.py files. If the metric you'd like to use does not exist, follow the directions in Adding new metrics . Many will be in basic_metrics.py . Define a function in run_specs.py annotated with run_spec_function to: Construct a ScenarioSpec for your scenario using a class name corresponding to the Python path of the class (e.g. helm.benchmark.scenarios.your_scenario.YourScenario ) and any arguments which must be passed as a dictionary of args . Construct an AdapterSpec for your scenario specifying the type of language model generation which must be performed for the task. Construct one or more MetricSpec objects for your task, specifying the classname with the Python path of the object, with the same arguments as the ScenarioSpec constructor. Construct and return RunSpec object, with a name corresponding to the scenario name and any patterns to match in curly braces, a scenario_spec , an adapter_spec , metric_specs , and groups . Attempt to run your task with venv/bin/helm-run -r yourscenarioname:arg=value where yourscenarioname matches the name specified in YourScenario Update src/helm/benchmark/static/contamination.yaml with models that were trained on your scenario (i.e. contaminated). Add a schema to src/helm/benchmark/static/schema.yaml and add the scenario to subgroups as needed. Adding new metrics To add a new metric, first determine if your metric is generic and likely to be widely used, or specific to your task. For generic metrics: Add a method to basic_metrics.py which takes two arguments: the gold answer and the model's pred iction. Add your method to the metric_fn_mapping lookup. For task specific metrics: Create a new yourtask_metrics.py file for class YourTaskMetric which inherits from Metric in metric.py . Define methods __init__ and evaluate_generation returning a list of Stat objects. Your metric is responsible for producing Stat objects: Each Stat should correspond to a distinct aggregate measurement over the generated examples. Some may have one metric (e.g. accuracy), while others may quantify multiple aspects (e.g. multiple distance metrics). For each value generated for a Stat , add it to yourstat using yourstat.add(value) . Usually, there will only be one value for each Stat , but multiple can be used, e.g. to show variance. Data augmentations To apply data augmentation, create a DataAugmenterSpec with a list of PerturbationSpec s and pass it into RunSpec . The following is an example: data_augmenter_spec = DataAugmenterSpec( perturbation_specs=[ PerturbationSpec( class_name=\"helm.benchmark.augmentations.perturbation.ExtraSpacePerturbation\", args={\"num_spaces\": 5}, ) ], should_perturb_references=False, should_augment_train_instances=False, should_include_original_train=False, should_augment_eval_instances=True, should_include_original_eval=True, ) run_spec = RunSpec( ... data_augmenter_spec=data_augmenter_spec ) In the example above, the DataPreprocessor will augment the set of evaluation instances by perturbing the original set of instances with the ExtraSpacePerturbation , where spaces in the text are replaced with num_spaces number of spaces. We currently only support applying a single perturbation to an instance instead of chaining multiple perturbations and applying it onto a single instance. Adding a new perturbation To add a new perturbation to the framework, create a new file at src/helm/benchmark/augmentations with the name <Name of perturbation>_perturbation.py e.g., typo_perturbation.py . Inside the file, create a new class (name it <Name of the perturbation>Perturbation e.g., TypoPerturbation ) that extends the abstract class Perturbation and implement the perturb method which takes in text and outputs the perturbed text. Add a test for the new perturbation in test_perturbation.py . Supporting new Hugging Face tokenizers Give the tokenizer a name. Use the same name that's used in Hugging Face (e.g., \"EleutherAI/gpt-j-6B\"). In HuggingFaceTokenizers , we load and cache tokenizers in memory. Add logic to handle the tokenizer in the load_tokenizer method. Add a test in test_huggingface_tokenizer.py to make sure we can load the tokenizer from Hugging Face. Add a new class <Name of tokenizer>WindowService in file <Name of tokenizer>_window_service.py . Follow what we did for GPTJWindowService . Import the new WindowService and map the model(s) to it in WindowServiceFactory . HEIM (text-to-image evaluation) The overall code structure is the same as HELM's. When adding new scenarios and metrics for image generation, place the Python files under the image_generation package (e.g., src/helm/benchmark/scenarios/image_generation ).","title":"Code Structure"},{"location":"code/#code-structure","text":"Warning \u2014 The document is stale and was last modified more than ten months ago. The information below may be outdated and incorrect. Please proceed with caution!","title":"Code Structure"},{"location":"code/#birds-eye-view","text":"Here's a birds-eye view of how the benchmarking process interacts with the main classes (see benchmark ): A Scenario (given by a ScenarioSpec ) specifies a task and a data distribution. It specifies a set of Instance s, where each Instance has an input (e.g., question) and a set of Reference outputs (e.g., multiple choice answers). A DataPreprocessor takes in a Scenario and produces a list of Instance s. Each Instance is given a unique ID. The set of Instance s is augmented according to DataAugmenterSpec . An Adapter (given by an AdaptationSpec ) takes a list of Instance s and adapts it to a set of Request s to the API (e.g., the model, temperature, number of in-context training examples). Formally, the output is a ScenarioState containing a set of RequestState s, where each RequestState consists of a Request and any metadata used to track the role of this Request (e.g., the relevant Instance and Reference ). An Executor (given by an ExecutionSpec ) executes each Request in the RequestState to produce a RequestResult for each one; everything is encapsulated in a ScenarioState . A Metric (given by a MetricSpec ) takes a ScenarioState containing RequestResults s and produces a set of Stat s (e.g., accuracy, accuracy@5, toxicity, bias, etc.). A Runner is the top-level controller that runs the above steps and is driven by a set of RunSpec s. There are three types of classes: Specifications (e.g., AdapterSpec , ExecutionSpec , RunSpec ): specified manually by the user. Note that Scenario and Metric are subclassed, so they are constructed by ObjectSpec , which specifies the subclass name and a free-form dictionary of arguments. States (e.g., Instance , ScenarioState , Request , RequestResult ): these are automatically generated and can be serialized. Controllers (e.g., Scenario , Adapter , Executor , Metric , Runner ): these have the bulk of the code and should not be serialized.","title":"Birds-Eye View"},{"location":"code/#adding-new-scenarios","text":"In order to implement new scenarios: Create a new Python file in the scenarios folder. Within the scenario file, create a Scenario class, e.g. YourScenario . YourScenario should implement get_instances , a method that downloads the dataset files if they don't already exist and returns a list of Instance s. Each Instance must have a list of (potentially one) Reference answers: a correct answer may be indicated with a CORRECT_TAG in a Reference instance's tags argument. In addition, you must specify the split of the Instance as one of TRAIN_SPLIT , VALID_SPLIT , or TEST_SPLIT constants as in scenario.py . For Scenario s with datasets that cannot be publicly shared, place a copy of the dataset at path restricted/<Name of the Scenario> and read from that path. See NewsQAScenario and ICEScenario for some examples. Note that you need not enumerate every possible correct answer (nor must there even necessarily be a correct answer). Make sure to document your scenario well with a clear docstring. In addition, specify its name , description , and tags . Identify the appropriate metric for your task in one of the *_metrics.py files. If the metric you'd like to use does not exist, follow the directions in Adding new metrics . Many will be in basic_metrics.py . Define a function in run_specs.py annotated with run_spec_function to: Construct a ScenarioSpec for your scenario using a class name corresponding to the Python path of the class (e.g. helm.benchmark.scenarios.your_scenario.YourScenario ) and any arguments which must be passed as a dictionary of args . Construct an AdapterSpec for your scenario specifying the type of language model generation which must be performed for the task. Construct one or more MetricSpec objects for your task, specifying the classname with the Python path of the object, with the same arguments as the ScenarioSpec constructor. Construct and return RunSpec object, with a name corresponding to the scenario name and any patterns to match in curly braces, a scenario_spec , an adapter_spec , metric_specs , and groups . Attempt to run your task with venv/bin/helm-run -r yourscenarioname:arg=value where yourscenarioname matches the name specified in YourScenario Update src/helm/benchmark/static/contamination.yaml with models that were trained on your scenario (i.e. contaminated). Add a schema to src/helm/benchmark/static/schema.yaml and add the scenario to subgroups as needed.","title":"Adding new scenarios"},{"location":"code/#adding-new-metrics","text":"To add a new metric, first determine if your metric is generic and likely to be widely used, or specific to your task. For generic metrics: Add a method to basic_metrics.py which takes two arguments: the gold answer and the model's pred iction. Add your method to the metric_fn_mapping lookup. For task specific metrics: Create a new yourtask_metrics.py file for class YourTaskMetric which inherits from Metric in metric.py . Define methods __init__ and evaluate_generation returning a list of Stat objects. Your metric is responsible for producing Stat objects: Each Stat should correspond to a distinct aggregate measurement over the generated examples. Some may have one metric (e.g. accuracy), while others may quantify multiple aspects (e.g. multiple distance metrics). For each value generated for a Stat , add it to yourstat using yourstat.add(value) . Usually, there will only be one value for each Stat , but multiple can be used, e.g. to show variance.","title":"Adding new metrics"},{"location":"code/#data-augmentations","text":"To apply data augmentation, create a DataAugmenterSpec with a list of PerturbationSpec s and pass it into RunSpec . The following is an example: data_augmenter_spec = DataAugmenterSpec( perturbation_specs=[ PerturbationSpec( class_name=\"helm.benchmark.augmentations.perturbation.ExtraSpacePerturbation\", args={\"num_spaces\": 5}, ) ], should_perturb_references=False, should_augment_train_instances=False, should_include_original_train=False, should_augment_eval_instances=True, should_include_original_eval=True, ) run_spec = RunSpec( ... data_augmenter_spec=data_augmenter_spec ) In the example above, the DataPreprocessor will augment the set of evaluation instances by perturbing the original set of instances with the ExtraSpacePerturbation , where spaces in the text are replaced with num_spaces number of spaces. We currently only support applying a single perturbation to an instance instead of chaining multiple perturbations and applying it onto a single instance.","title":"Data augmentations"},{"location":"code/#adding-a-new-perturbation","text":"To add a new perturbation to the framework, create a new file at src/helm/benchmark/augmentations with the name <Name of perturbation>_perturbation.py e.g., typo_perturbation.py . Inside the file, create a new class (name it <Name of the perturbation>Perturbation e.g., TypoPerturbation ) that extends the abstract class Perturbation and implement the perturb method which takes in text and outputs the perturbed text. Add a test for the new perturbation in test_perturbation.py .","title":"Adding a new perturbation"},{"location":"code/#supporting-new-hugging-face-tokenizers","text":"Give the tokenizer a name. Use the same name that's used in Hugging Face (e.g., \"EleutherAI/gpt-j-6B\"). In HuggingFaceTokenizers , we load and cache tokenizers in memory. Add logic to handle the tokenizer in the load_tokenizer method. Add a test in test_huggingface_tokenizer.py to make sure we can load the tokenizer from Hugging Face. Add a new class <Name of tokenizer>WindowService in file <Name of tokenizer>_window_service.py . Follow what we did for GPTJWindowService . Import the new WindowService and map the model(s) to it in WindowServiceFactory .","title":"Supporting new Hugging Face tokenizers"},{"location":"code/#heim-text-to-image-evaluation","text":"The overall code structure is the same as HELM's. When adding new scenarios and metrics for image generation, place the Python files under the image_generation package (e.g., src/helm/benchmark/scenarios/image_generation ).","title":"HEIM (text-to-image evaluation)"},{"location":"credentials/","text":"Credentials Credentials file You should create a credentials.conf file in your local configuration folder, which is ./prod_env/ by default, unless you have overridden it using the --local-path flag to helm-run . This file should be in HOCON format. Example: platformOneApiKey: sk-abcdefgh platformTneApiKey: sk-ijklmnop Here are the keys that must be set for to access these platforms: AI21: ai21ApiKey Aleph Alpha: AlephAlphaApiKey Anthropic: anthropicApiKey Cohere: cohereApiKey Google: googleProjectId , googleLocation , also see Additional Setup below GooseAI: gooseApiKey Hugging Face Hub: None, but see Additional Setup below Mistral AI: mistralaiApiKey OpenAI: openaiApiKey , openApiOrgId Perspective: perspectiveApiKey Writer: writerApiKey Additional setup Google You will need to install the Google Cloud CLI . Then, as the user that will be running helm-run , run: gcloud auth application-default login gcloud auth application-default set-quota-project 123456789012 Replace 123456789012 with your actual numeric project ID. Hugging Face Hub If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 2) through Hugging Face, you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run: huggingface-cli login Refer to Hugging Face's documentation for more information.","title":"Credentials"},{"location":"credentials/#credentials","text":"","title":"Credentials"},{"location":"credentials/#credentials-file","text":"You should create a credentials.conf file in your local configuration folder, which is ./prod_env/ by default, unless you have overridden it using the --local-path flag to helm-run . This file should be in HOCON format. Example: platformOneApiKey: sk-abcdefgh platformTneApiKey: sk-ijklmnop Here are the keys that must be set for to access these platforms: AI21: ai21ApiKey Aleph Alpha: AlephAlphaApiKey Anthropic: anthropicApiKey Cohere: cohereApiKey Google: googleProjectId , googleLocation , also see Additional Setup below GooseAI: gooseApiKey Hugging Face Hub: None, but see Additional Setup below Mistral AI: mistralaiApiKey OpenAI: openaiApiKey , openApiOrgId Perspective: perspectiveApiKey Writer: writerApiKey","title":"Credentials file"},{"location":"credentials/#additional-setup","text":"","title":"Additional setup"},{"location":"credentials/#google","text":"You will need to install the Google Cloud CLI . Then, as the user that will be running helm-run , run: gcloud auth application-default login gcloud auth application-default set-quota-project 123456789012 Replace 123456789012 with your actual numeric project ID.","title":"Google"},{"location":"credentials/#hugging-face-hub","text":"If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 2) through Hugging Face, you need to be authenticated to Hugging Face through the CLI. As the user that will be running helm-run , run: huggingface-cli login Refer to Hugging Face's documentation for more information.","title":"Hugging Face Hub"},{"location":"developer_adding_new_models/","text":"Adding New Clients Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! Overview of the process To add a new model you need to define 3 objects: * a ModelMetadata objects that defines properties of your model (name, metadata, capabilities, ...). * one or several ModelDeployment which defines how to query a model (mainly by providing a Client , a WindowService and a Tokenizer ). You can define several deployments for a single model ( local/your-model , huggingface/your-model , together/your-model , ...). * a TokenizerConfig which defines how to build the Tokenizer (mainly by providing a TokenizerSpec ). In some cases you might have to define additionally: * a Client if your query method differ from any clients we have implemented. This will be then referenced in the ModelDeployment . We recommend checking HTTPModelClient and HuggingFaceClient which can be used in a lot of cases. If you identify the need for a new client, a good starting to point is to have a look at SimpleClient . * a WindowService . First have a look at DefaultWindowService to check if this is not enough for your use case. If you need you own truncate_from_right function, then you might need to create your own WindowService . In that case, a good starting point is to have a look at YaLMWindowService . Where to create the objects There are two cases: private models that should only be accessible to you and models not yet supported by HELM but that would benefit everyone if added. In the first case, you should create the files model_deployments.yaml , model_metadata.yaml and tokenizer_configs.yaml in prod_env/ (A folder that you should create at the root of the repo if not already done). HELM will automatically registed any model defined in these files without any change in the code while ignoring them on Github which can be convenient for you. Then you can simply duplicate the corresponding files from src/helm/config , delete the models and add yours. Follow the next section for an example. In the second case, if you want to add a model to HELM, you can directly do it in src/helm/config . You can then open a Pull Request on Github to share the model. When you do, make sure to: * Include any link justifying the metadata used in ModelMetadata such as the release data, number of parameters, capabilities and so on (you should not infer anything). * Check that you are respecting the format used in those files ( ModelMetadata should be named as <CREATOR-ORGANIZATION>/<MODEL-NAME> and the ModelDeployment should be named as <HOST-ORGANIZATION>/<MODEL-NAME> , for example ModelMetadata : openai/gpt2 and ModelDeployment : huggingface/gpt2 ). Add the appropriate comments and so on. * Run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=<YOUR-DEPLOYMENT>\" --suite v1 --max-eval-instances 10 and make sure that everything works. Include the logs from the terminal in your PR. * Not create unnecessary objects ( Client TokenizerCOnfig , WindowService ) and if you have to create one of these objects, document in your PR why you had to. Make them general enough so that they could be re-used by other models (especially the Client ). Example In src/helm/config/model_metadata.yaml : # [...] models: - name: simple/model1 [...] # NEW MODEL STARTS HERE - name: simple/tutorial display_name: Tutorial Model description: This is a simple model used in the tutorial. creator_organization_name: Helm access: open release_date: 2023-01-01 tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG] [...] In src/helm/config/model_deployments.yaml : # [...] model_deployments: - name: simple/model1 [...] - name: simple/tutorial model_name: simple/tutorial tokenizer_name: simple/model1 max_sequence_length: 2048 client_spec: class_name: \"helm.clients.simple_client.SimpleClient\" args: {} window_service_spec: class_name: \"helm.benchmark.window_services.openai_window_service.OpenAIWindowService\" args: {} [...] We won't be adding any TokenizerConfig here as we are reusing simple/model1 . This shows a good practice when adding a new model, always check if the correct tokenizer does not already exists. You should now be able to run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=simple/tutorial\" --suite v1 --max-eval-instances 10 without any error.","title":"Adding New Clients"},{"location":"developer_adding_new_models/#adding-new-clients","text":"Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution!","title":"Adding New Clients"},{"location":"developer_adding_new_models/#overview-of-the-process","text":"To add a new model you need to define 3 objects: * a ModelMetadata objects that defines properties of your model (name, metadata, capabilities, ...). * one or several ModelDeployment which defines how to query a model (mainly by providing a Client , a WindowService and a Tokenizer ). You can define several deployments for a single model ( local/your-model , huggingface/your-model , together/your-model , ...). * a TokenizerConfig which defines how to build the Tokenizer (mainly by providing a TokenizerSpec ). In some cases you might have to define additionally: * a Client if your query method differ from any clients we have implemented. This will be then referenced in the ModelDeployment . We recommend checking HTTPModelClient and HuggingFaceClient which can be used in a lot of cases. If you identify the need for a new client, a good starting to point is to have a look at SimpleClient . * a WindowService . First have a look at DefaultWindowService to check if this is not enough for your use case. If you need you own truncate_from_right function, then you might need to create your own WindowService . In that case, a good starting point is to have a look at YaLMWindowService .","title":"Overview of the process"},{"location":"developer_adding_new_models/#where-to-create-the-objects","text":"There are two cases: private models that should only be accessible to you and models not yet supported by HELM but that would benefit everyone if added. In the first case, you should create the files model_deployments.yaml , model_metadata.yaml and tokenizer_configs.yaml in prod_env/ (A folder that you should create at the root of the repo if not already done). HELM will automatically registed any model defined in these files without any change in the code while ignoring them on Github which can be convenient for you. Then you can simply duplicate the corresponding files from src/helm/config , delete the models and add yours. Follow the next section for an example. In the second case, if you want to add a model to HELM, you can directly do it in src/helm/config . You can then open a Pull Request on Github to share the model. When you do, make sure to: * Include any link justifying the metadata used in ModelMetadata such as the release data, number of parameters, capabilities and so on (you should not infer anything). * Check that you are respecting the format used in those files ( ModelMetadata should be named as <CREATOR-ORGANIZATION>/<MODEL-NAME> and the ModelDeployment should be named as <HOST-ORGANIZATION>/<MODEL-NAME> , for example ModelMetadata : openai/gpt2 and ModelDeployment : huggingface/gpt2 ). Add the appropriate comments and so on. * Run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=<YOUR-DEPLOYMENT>\" --suite v1 --max-eval-instances 10 and make sure that everything works. Include the logs from the terminal in your PR. * Not create unnecessary objects ( Client TokenizerCOnfig , WindowService ) and if you have to create one of these objects, document in your PR why you had to. Make them general enough so that they could be re-used by other models (especially the Client ).","title":"Where to create the objects"},{"location":"developer_adding_new_models/#example","text":"In src/helm/config/model_metadata.yaml : # [...] models: - name: simple/model1 [...] # NEW MODEL STARTS HERE - name: simple/tutorial display_name: Tutorial Model description: This is a simple model used in the tutorial. creator_organization_name: Helm access: open release_date: 2023-01-01 tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG] [...] In src/helm/config/model_deployments.yaml : # [...] model_deployments: - name: simple/model1 [...] - name: simple/tutorial model_name: simple/tutorial tokenizer_name: simple/model1 max_sequence_length: 2048 client_spec: class_name: \"helm.clients.simple_client.SimpleClient\" args: {} window_service_spec: class_name: \"helm.benchmark.window_services.openai_window_service.OpenAIWindowService\" args: {} [...] We won't be adding any TokenizerConfig here as we are reusing simple/model1 . This shows a good practice when adding a new model, always check if the correct tokenizer does not already exists. You should now be able to run helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=simple/tutorial\" --suite v1 --max-eval-instances 10 without any error.","title":"Example"},{"location":"developer_setup/","text":"Developer Setup Check your system Python version Check your system verison of Python by running: python --version If your version of Python is older than 3.10, you must use either Conda or pyenv to install a version of Python >=3.10 when setting up your virtual environment. Set up the Python virtual environment First, create a Python virtual environment with Python version >= 3.10 and activate it. Using Virtualenv ( requires system Python version >=3.10): # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3 venv # Activate the virtual environment. # Run this every time you open your shell. source venv/bin/activate Using Conda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. # Run this every time you open your shell. conda activate crfm-helm Using pyenv and pyenv-virtualenv : # Create a virtual environment. # Only run this the first time. pyenv virtualenv 3.10 crfm-helm # Activate the virtual environment. # Run this every time you open your shell. pyenv activate crfm-helm Install Python dependencies To install any dependencies: pip install --force-reinstall -e .[dev] Run Python tests Currently, running all the unit tests takes about 10 minutes. To run all unit tests: python -m pytest Append -vv to output the full diff and results: python -m pytest -vv When modifying the Python code, you usually want to only run certain relevant tests. To run a specific test file, specify the file path as follows: python -m pytest path/to/test_file.py -vv Run linter and type-checker You should always ensure that your code is linted and type-checked before creating a pull request. This is typically enforced by our git pre-commit hooks. Install the pre-commit hooks by running: pre-commit install This will automatically run the linter and type-checker whenever you run git push to push a branch. To skip running the linter and type checker when pushing a branch, use the --no-verify flag with git push . To run the linter and type-checker manually: ./pre-commit.sh Alternatively, you can run only the linter or only the type checker separately: # Linters black src scripts flake8 src scripts # Type checker mypy src scripts Executing helm commands with local modifications The recommended way to execute helm-run , helm-summarize , helm-server , etc, with your local version of the repository is to do an editable install, using the following steps: Activate your virtual environment. Change directory to the repository root (contains pyproject.toml). Make sure you don't have an existing helm installation for that environment with pip uninstall crfm-helm Run pip install -e . Now calling helm-run while the environment is activated will read from your local source. Without installing If you have a compelling reason not to do an editable install, you can execute commands by: Change directory to src Execute the module you want with a command like: python -m helm.benchmark.run Checking in code The HELM repository does not allow direct modifications of the main branch. Instead, developers create a Pull Request which must then be approved by a different person before merging into main. Here is an example workflow: git checkout main to start from the main branch. git pull origin main to get up to date. Make whatever changes you'll like to group into a single review. Run tests. Make a new branch with git checkout -b <your-handle>/<change-identifier . For example, yifanmai/fix-optional-suggestions . If you did NOT install the precommit, run the linter and type checker with ./pre-commit.sh git commit -a to commit all you changes. If you want to ignore precommit warnings, you can add --no-verify . git push origin <your-handle>/<change-identifier> to upload to github. Loading any HELM github page should now prompt you about creating a new pull request. If not, you can also find your branch on the branches page to create one. Update the title and description as necessary, then create the pull request. Once the reviewer is satisfied, they can approve and either of you can then Squash and Merge the branch into main.","title":"Developer Setup"},{"location":"developer_setup/#developer-setup","text":"","title":"Developer Setup"},{"location":"developer_setup/#check-your-system-python-version","text":"Check your system verison of Python by running: python --version If your version of Python is older than 3.10, you must use either Conda or pyenv to install a version of Python >=3.10 when setting up your virtual environment.","title":"Check your system Python version"},{"location":"developer_setup/#set-up-the-python-virtual-environment","text":"First, create a Python virtual environment with Python version >= 3.10 and activate it. Using Virtualenv ( requires system Python version >=3.10): # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3 venv # Activate the virtual environment. # Run this every time you open your shell. source venv/bin/activate Using Conda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. # Run this every time you open your shell. conda activate crfm-helm Using pyenv and pyenv-virtualenv : # Create a virtual environment. # Only run this the first time. pyenv virtualenv 3.10 crfm-helm # Activate the virtual environment. # Run this every time you open your shell. pyenv activate crfm-helm","title":"Set up the Python virtual environment"},{"location":"developer_setup/#install-python-dependencies","text":"To install any dependencies: pip install --force-reinstall -e .[dev]","title":"Install Python dependencies"},{"location":"developer_setup/#run-python-tests","text":"Currently, running all the unit tests takes about 10 minutes. To run all unit tests: python -m pytest Append -vv to output the full diff and results: python -m pytest -vv When modifying the Python code, you usually want to only run certain relevant tests. To run a specific test file, specify the file path as follows: python -m pytest path/to/test_file.py -vv","title":"Run Python tests"},{"location":"developer_setup/#run-linter-and-type-checker","text":"You should always ensure that your code is linted and type-checked before creating a pull request. This is typically enforced by our git pre-commit hooks. Install the pre-commit hooks by running: pre-commit install This will automatically run the linter and type-checker whenever you run git push to push a branch. To skip running the linter and type checker when pushing a branch, use the --no-verify flag with git push . To run the linter and type-checker manually: ./pre-commit.sh Alternatively, you can run only the linter or only the type checker separately: # Linters black src scripts flake8 src scripts # Type checker mypy src scripts","title":"Run linter and type-checker"},{"location":"developer_setup/#executing-helm-commands-with-local-modifications","text":"The recommended way to execute helm-run , helm-summarize , helm-server , etc, with your local version of the repository is to do an editable install, using the following steps: Activate your virtual environment. Change directory to the repository root (contains pyproject.toml). Make sure you don't have an existing helm installation for that environment with pip uninstall crfm-helm Run pip install -e . Now calling helm-run while the environment is activated will read from your local source.","title":"Executing helm commands with local modifications"},{"location":"developer_setup/#without-installing","text":"If you have a compelling reason not to do an editable install, you can execute commands by: Change directory to src Execute the module you want with a command like: python -m helm.benchmark.run","title":"Without installing"},{"location":"developer_setup/#checking-in-code","text":"The HELM repository does not allow direct modifications of the main branch. Instead, developers create a Pull Request which must then be approved by a different person before merging into main. Here is an example workflow: git checkout main to start from the main branch. git pull origin main to get up to date. Make whatever changes you'll like to group into a single review. Run tests. Make a new branch with git checkout -b <your-handle>/<change-identifier . For example, yifanmai/fix-optional-suggestions . If you did NOT install the precommit, run the linter and type checker with ./pre-commit.sh git commit -a to commit all you changes. If you want to ignore precommit warnings, you can add --no-verify . git push origin <your-handle>/<change-identifier> to upload to github. Loading any HELM github page should now prompt you about creating a new pull request. If not, you can also find your branch on the branches page to create one. Update the title and description as necessary, then create the pull request. Once the reviewer is satisfied, they can approve and either of you can then Squash and Merge the branch into main.","title":"Checking in code"},{"location":"editing_documentation/","text":"Editing Documentation The documentation that you are reading now is an invaluable resource for newcomers and experienced users alike. Contributions to the documentation are very welcome. We currently use the MkDocs as our static site generator and ReadTheDocs as our web host. To edit the documentation, first clone the repository locally, then install HELM from the repository by following the Developer Setup instructions. After that, install the MkDocs dependencies by running the following from the root of the repository: pip install -r docs/requirements.txt You should now be able to run MkDocs from the root of the repository: mkdocs serve Then navigate to http://localhost:8000/ to view your locally-built documentation. The source Markdown files for the documentation are stored in the docs/ folder. By default, MkDocs watches the source directories for changes and automatically re-renders the web pages when it detects changes. If you are creating a new page, you should add your page to the nav section in mkdocs.yml . This will add your page to the table of contents in the side menu. We make heavy use of plugins and macros for auto-generating documentation from code and docstrings. For more information, please refer to the documentation for these plugins e.g. mkdocs-macros , mkdocstrings and mkdocstrings-python .","title":"Editing Documentation"},{"location":"editing_documentation/#editing-documentation","text":"The documentation that you are reading now is an invaluable resource for newcomers and experienced users alike. Contributions to the documentation are very welcome. We currently use the MkDocs as our static site generator and ReadTheDocs as our web host. To edit the documentation, first clone the repository locally, then install HELM from the repository by following the Developer Setup instructions. After that, install the MkDocs dependencies by running the following from the root of the repository: pip install -r docs/requirements.txt You should now be able to run MkDocs from the root of the repository: mkdocs serve Then navigate to http://localhost:8000/ to view your locally-built documentation. The source Markdown files for the documentation are stored in the docs/ folder. By default, MkDocs watches the source directories for changes and automatically re-renders the web pages when it detects changes. If you are creating a new page, you should add your page to the nav section in mkdocs.yml . This will add your page to the table of contents in the side menu. We make heavy use of plugins and macros for auto-generating documentation from code and docstrings. For more information, please refer to the documentation for these plugins e.g. mkdocs-macros , mkdocstrings and mkdocstrings-python .","title":"Editing Documentation"},{"location":"extension_multilingual/","text":"Multilingual","title":"Multilingual"},{"location":"extension_multilingual/#multilingual","text":"","title":"Multilingual"},{"location":"huggingface_models/","text":"Hugging Face Model Hub Integration HELM can be used to evaluate AutoModelForCausalLM models (e.g. BioMedLM ) on Hugging Face Model Hub or local disk. Note that only AutoModelForCausalLM models are supported; other classes such as AutoModelForSeq2SeqLM may be supported in the future. Using model_deployments.yaml You can add Hugging Face models using the method discussed in Adding New Models . This can be used for both models on Hugging Face Hub and local disk. Please refer to that page for instructions for how to do so. Using command-line flags In some cases, you can use command-line flags with helm-run to evaluating Hugging Face models. This provides a more convenient way to use Hugging Face models that does not require configuration files. To use AutoModelForCausalLM models from Hugging Face Model Hub, add the Hugging Face model IDs to the --enable-huggingface-models flags to helm-run . This will make the corresponding Hugging Face models available to use in your run spec descriptions. In the run spec description, use the Hugging Face model ID as the model name. To use a revision of a model other than the default main revision, append a @ followed by the revision name to the model ID passed to the --enable-huggingface-models flag. Current restrictions with command-line flags: Models without a namespace are not supported (e.g. bert-base-uncased ). The model must have model_max_length set in the tokenizer configuration. Example model on Hugging Face Hub: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM \\ --enable-huggingface-models stanford-crfm/BioMedLM \\ --suite v1 \\ --max-eval-instances 10 # Run boolq on stanford-crfm/BioMedLM at revision main helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM@main \\ --enable-huggingface-models stanford-crfm/BioMedLM@main \\ --suite v1 \\ --max-eval-instances 10 Example model on local disk: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=your-org/your-model \\ --enable-local-huggingface-models path/to/your-org/your-model \\ --suite v1 \\ --max-eval-instances 10","title":"Hugging Face Model Hub Integration"},{"location":"huggingface_models/#hugging-face-model-hub-integration","text":"HELM can be used to evaluate AutoModelForCausalLM models (e.g. BioMedLM ) on Hugging Face Model Hub or local disk. Note that only AutoModelForCausalLM models are supported; other classes such as AutoModelForSeq2SeqLM may be supported in the future.","title":"Hugging Face Model Hub Integration"},{"location":"huggingface_models/#using-model_deploymentsyaml","text":"You can add Hugging Face models using the method discussed in Adding New Models . This can be used for both models on Hugging Face Hub and local disk. Please refer to that page for instructions for how to do so.","title":"Using model_deployments.yaml"},{"location":"huggingface_models/#using-command-line-flags","text":"In some cases, you can use command-line flags with helm-run to evaluating Hugging Face models. This provides a more convenient way to use Hugging Face models that does not require configuration files. To use AutoModelForCausalLM models from Hugging Face Model Hub, add the Hugging Face model IDs to the --enable-huggingface-models flags to helm-run . This will make the corresponding Hugging Face models available to use in your run spec descriptions. In the run spec description, use the Hugging Face model ID as the model name. To use a revision of a model other than the default main revision, append a @ followed by the revision name to the model ID passed to the --enable-huggingface-models flag. Current restrictions with command-line flags: Models without a namespace are not supported (e.g. bert-base-uncased ). The model must have model_max_length set in the tokenizer configuration. Example model on Hugging Face Hub: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM \\ --enable-huggingface-models stanford-crfm/BioMedLM \\ --suite v1 \\ --max-eval-instances 10 # Run boolq on stanford-crfm/BioMedLM at revision main helm-run \\ --run-entries boolq:model=stanford-crfm/BioMedLM@main \\ --enable-huggingface-models stanford-crfm/BioMedLM@main \\ --suite v1 \\ --max-eval-instances 10 Example model on local disk: # Run boolq on stanford-crfm/BioMedLM at the default main revision helm-run \\ --run-entries boolq:model=your-org/your-model \\ --enable-local-huggingface-models path/to/your-org/your-model \\ --suite v1 \\ --max-eval-instances 10","title":"Using command-line flags"},{"location":"importing_custom_modules/","text":"Importing Custom Modules HELM is a modular framework with a plug-in architecture. You can write your own implementation for a client, tokenizer, scenario or metric and use them in HELM with HELM installed as a library, without needing to modify HELM itself. The main way for you to use your code in HELM is to write a custom Python class that is a subclass of Client , Tokenizer , Scenario or Metric in a Python module. You can then specify a ClientSpec , TokenizerSpec , ScenarioSpec or MetricSpec (which are all classes of ObjectSpec ) where the class_name is the name of your custom Python class. However, HELM will only be able to use custom classes that can be imported by Python. Depending on your setup, you may need to do additional steps. Add the current working directory to PYTHONPATH If the custom classes live in a Python module under the current working directory, you should modify PYTHONPATH to make that Python module importable. This is required because Python does not add the current working directory to the Python module search path running when using command line comments / Python entry points such as helm-run . See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . To make your file importable by Python, you have to add . to your PYTHONPATH so that Python will search in your current working directory for your custom Python modules. In Bash, you can do this by running export PYTHONPATH=\".:$PYTHONPATH\" before running helm-run , or by prefixing helm-run with PYTHONPATH=\".:$PYTHONPATH . Put your custom classes in a Python package If your custom classes are located in a Python package, you can simply install your package (optionally in editable mode) and it will automatically be importable by Python. Be sure to install your Python package in the same Python environment as HELM. Write a Python wrapper script If you are using a Python wrapper script that calls helm.benchmark.run.run_benchmark() instead of using helm-run , Python will automatically add the directory containing that script to the Python module search path. If your custom classes live in a Python module under that directory, they will automatically be importable by Python. See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . Suppose you added a script called run_helm.py that calls helm.benchmark.run.run_benchmark() directly. When run using python run_helm.py , HELM will be able to import your modules without any additional changes.","title":"Importing Custom Modules"},{"location":"importing_custom_modules/#importing-custom-modules","text":"HELM is a modular framework with a plug-in architecture. You can write your own implementation for a client, tokenizer, scenario or metric and use them in HELM with HELM installed as a library, without needing to modify HELM itself. The main way for you to use your code in HELM is to write a custom Python class that is a subclass of Client , Tokenizer , Scenario or Metric in a Python module. You can then specify a ClientSpec , TokenizerSpec , ScenarioSpec or MetricSpec (which are all classes of ObjectSpec ) where the class_name is the name of your custom Python class. However, HELM will only be able to use custom classes that can be imported by Python. Depending on your setup, you may need to do additional steps.","title":"Importing Custom Modules"},{"location":"importing_custom_modules/#add-the-current-working-directory-to-pythonpath","text":"If the custom classes live in a Python module under the current working directory, you should modify PYTHONPATH to make that Python module importable. This is required because Python does not add the current working directory to the Python module search path running when using command line comments / Python entry points such as helm-run . See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . To make your file importable by Python, you have to add . to your PYTHONPATH so that Python will search in your current working directory for your custom Python modules. In Bash, you can do this by running export PYTHONPATH=\".:$PYTHONPATH\" before running helm-run , or by prefixing helm-run with PYTHONPATH=\".:$PYTHONPATH .","title":"Add the current working directory to PYTHONPATH"},{"location":"importing_custom_modules/#put-your-custom-classes-in-a-python-package","text":"If your custom classes are located in a Python package, you can simply install your package (optionally in editable mode) and it will automatically be importable by Python. Be sure to install your Python package in the same Python environment as HELM.","title":"Put your custom classes in a Python package"},{"location":"importing_custom_modules/#write-a-python-wrapper-script","text":"If you are using a Python wrapper script that calls helm.benchmark.run.run_benchmark() instead of using helm-run , Python will automatically add the directory containing that script to the Python module search path. If your custom classes live in a Python module under that directory, they will automatically be importable by Python. See Python's documentation for more details. For example, suppose you implemented a custom Client subclass named MyClient in the my_client.py file under your current working directory, and you have a ClientSpec specifying the class_name as my_client.MyClient . Suppose you added a script called run_helm.py that calls helm.benchmark.run.run_benchmark() directly. When run using python run_helm.py , HELM will be able to import your modules without any additional changes.","title":"Write a Python wrapper script"},{"location":"installation/","text":"Installation Create a virtual environment It is recommended to install HELM into a virtual environment with Python version >=3.10 to avoid dependency conflicts. HELM requires Python >=3.10. To create, a Python virtual environment and activate it, follow the instructions below. Using Virtualenv : # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3.10 helm-venv # Activate the virtual environment. source helm-venv/bin/activate Using Anaconda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. conda activate crfm-helm Install HELM Within this virtual environment, run: pip install crfm-helm","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#create-a-virtual-environment","text":"It is recommended to install HELM into a virtual environment with Python version >=3.10 to avoid dependency conflicts. HELM requires Python >=3.10. To create, a Python virtual environment and activate it, follow the instructions below. Using Virtualenv : # Create a virtual environment. # Only run this the first time. python3 -m pip install virtualenv python3 -m virtualenv -p python3.10 helm-venv # Activate the virtual environment. source helm-venv/bin/activate Using Anaconda : # Create a virtual environment. # Only run this the first time. conda create -n crfm-helm python=3.10 pip # Activate the virtual environment. conda activate crfm-helm","title":"Create a virtual environment"},{"location":"installation/#install-helm","text":"Within this virtual environment, run: pip install crfm-helm","title":"Install HELM"},{"location":"metrics/","text":"Metrics ::: helm.benchmark.metrics options: filters: [\"^(?!test_).+ metrics$\", \"Metric$\", \"^evaluate \"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Metrics"},{"location":"metrics/#metrics","text":"::: helm.benchmark.metrics options: filters: [\"^(?!test_).+ metrics$\", \"Metric$\", \"^evaluate \"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Metrics"},{"location":"mission-statement/","text":"","title":"Mission statement"},{"location":"models/","text":"Models Text Models {% for tag in [\"TEXT_MODEL_TAG\", \"CODE_MODEL_TAG\"] %} {% for organization, models in models_by_organization_with_tag(tag).items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} {% endfor %} Vision-Language Models {% for organization, models in models_by_organization_with_tag(\"VISION_LANGUAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} Text-to-image Models {% for organization, models in models_by_organization_with_tag(\"TEXT_TO_IMAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %} Audio-Language Models {% for organization, models in models_by_organization_with_tag(\"AUDIO_LANGUAGE_MODEL_TAG\").items() %} {{ organization }} {% for model in models %} {{ model.display_name }} \u2014 {{ model.name }} {{ model.description }} {% endfor %} {% endfor %}","title":"Models"},{"location":"models/#models","text":"","title":"Models"},{"location":"models/#text-models","text":"{% for tag in [\"TEXT_MODEL_TAG\", \"CODE_MODEL_TAG\"] %} {% for organization, models in models_by_organization_with_tag(tag).items() %}","title":"Text Models"},{"location":"models/#organization","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname","text":"{{ model.description }} {% endfor %} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#vision-language-models","text":"{% for organization, models in models_by_organization_with_tag(\"VISION_LANGUAGE_MODEL_TAG\").items() %}","title":"Vision-Language Models"},{"location":"models/#organization_1","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_1","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#text-to-image-models","text":"{% for organization, models in models_by_organization_with_tag(\"TEXT_TO_IMAGE_MODEL_TAG\").items() %}","title":"Text-to-image Models"},{"location":"models/#organization_2","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_2","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"models/#audio-language-models","text":"{% for organization, models in models_by_organization_with_tag(\"AUDIO_LANGUAGE_MODEL_TAG\").items() %}","title":"Audio-Language Models"},{"location":"models/#organization_3","text":"{% for model in models %}","title":"{{ organization }}"},{"location":"models/#modeldisplay_name-modelname_3","text":"{{ model.description }} {% endfor %} {% endfor %}","title":"{{ model.display_name }} &mdash; {{ model.name }}"},{"location":"notes/","text":"Legal-10 Development Notes Dataset Availability Status \u2713 Available on HuggingFace CUAD - theatticusproject/cuad-qa (S6: Fact Extraction) CLERC - jhu-clsp/CLERC (S3, S4, S5: Citation/Authority tasks) LEXam - LEXam-Benchmark/LEXam (S8: Synthesize Results) FairLex - coastalcph/fairlex (Extension: Multilingual) CaseHOLD - casehold/casehold (S7: Distinguish Cases) - Already in HELM \u26a0 Available on GitHub Dahl - reglab/legal_hallucinations (S9: Citation Integrity) LegalAgentBench - CSHaitao/LegalAgentBench (S1, S2: Agentic) - Chinese-focused \u274c Not Publicly Available SHIELD - Dataset not public due to copyright concerns (S10: Copyright Compliance) Paper: arXiv:2406.12975 Existing HELM Implementations Already Implemented CaseHOLD - src/helm/benchmark/scenarios/casehold_scenario.py ~53k questions, 5-choice MC Run spec: enterprise_run_specs.py::get_casehold_spec() LegalBench - src/helm/benchmark/scenarios/legalbench_scenario.py ~100 instances per subset (5 subsets) Run spec: lite_run_specs.py::get_legalbench_spec(subset) To Implement (8 datasets) CLERC (S3, S4, S5) CUAD (S6) LEXam (S8) LegalAgentBench (S1, S2) KeyCite (S5) - unclear if separate or part of CLERC SHIELD (S10) Dahl (S9) FairLex (Extension) Implementation Patterns (from CaseHOLD & LegalBench) Dataset Loading Use HuggingFace load_dataset() with cache_dir parameter Cache to output_path/data/ Support train/test splits Instance Structure Instance( input=Input(text=\"...\"), references=[Reference(Output(text=\"...\"), tags=[\"correct\" or []])], split=\"train\" or \"test\", id=f\"id{unique_id}\" ) Run Spec Pattern Few-shot: 2-5 examples Metrics: get_exact_match_metric_specs() Adapt methods: ADAPT_MULTIPLE_CHOICE_JOINT or ADAPT_GENERATION Computational Estimates Per-Dataset (based on CaseHOLD/LegalBench) CaseHOLD : ~1200-1850 tokens/instance, 2 few-shot examples LegalBench : ~650-1250 tokens/instance, 5 few-shot examples Estimated runtime : 30-60 min for 1 model across all 10 skills (~1000-1500 test instances) Branding Changes Completed [x] README.md updated with Legal-10 mission statement [x] CITATION.bib updated (author: Jon Chung) [ ] pyproject.toml (package name, description, author) [ ] LICENSE (copyright holder) [ ] Frontend branding Next Steps Decide on test set sampling strategy (all datasets vary in size) Create legal10_run_specs.py for all 10 skills Implement missing scenario files Handle SHIELD alternative (S10) Handle LegalAgentBench language issue (S1, S2)","title":"Legal-10 Development Notes"},{"location":"notes/#legal-10-development-notes","text":"","title":"Legal-10 Development Notes"},{"location":"notes/#dataset-availability-status","text":"","title":"Dataset Availability Status"},{"location":"notes/#available-on-huggingface","text":"CUAD - theatticusproject/cuad-qa (S6: Fact Extraction) CLERC - jhu-clsp/CLERC (S3, S4, S5: Citation/Authority tasks) LEXam - LEXam-Benchmark/LEXam (S8: Synthesize Results) FairLex - coastalcph/fairlex (Extension: Multilingual) CaseHOLD - casehold/casehold (S7: Distinguish Cases) - Already in HELM","title":"\u2713 Available on HuggingFace"},{"location":"notes/#available-on-github","text":"Dahl - reglab/legal_hallucinations (S9: Citation Integrity) LegalAgentBench - CSHaitao/LegalAgentBench (S1, S2: Agentic) - Chinese-focused","title":"\u26a0 Available on GitHub"},{"location":"notes/#not-publicly-available","text":"SHIELD - Dataset not public due to copyright concerns (S10: Copyright Compliance) Paper: arXiv:2406.12975","title":"\u274c Not Publicly Available"},{"location":"notes/#existing-helm-implementations","text":"","title":"Existing HELM Implementations"},{"location":"notes/#already-implemented","text":"CaseHOLD - src/helm/benchmark/scenarios/casehold_scenario.py ~53k questions, 5-choice MC Run spec: enterprise_run_specs.py::get_casehold_spec() LegalBench - src/helm/benchmark/scenarios/legalbench_scenario.py ~100 instances per subset (5 subsets) Run spec: lite_run_specs.py::get_legalbench_spec(subset)","title":"Already Implemented"},{"location":"notes/#to-implement-8-datasets","text":"CLERC (S3, S4, S5) CUAD (S6) LEXam (S8) LegalAgentBench (S1, S2) KeyCite (S5) - unclear if separate or part of CLERC SHIELD (S10) Dahl (S9) FairLex (Extension)","title":"To Implement (8 datasets)"},{"location":"notes/#implementation-patterns-from-casehold-legalbench","text":"","title":"Implementation Patterns (from CaseHOLD &amp; LegalBench)"},{"location":"notes/#dataset-loading","text":"Use HuggingFace load_dataset() with cache_dir parameter Cache to output_path/data/ Support train/test splits","title":"Dataset Loading"},{"location":"notes/#instance-structure","text":"Instance( input=Input(text=\"...\"), references=[Reference(Output(text=\"...\"), tags=[\"correct\" or []])], split=\"train\" or \"test\", id=f\"id{unique_id}\" )","title":"Instance Structure"},{"location":"notes/#run-spec-pattern","text":"Few-shot: 2-5 examples Metrics: get_exact_match_metric_specs() Adapt methods: ADAPT_MULTIPLE_CHOICE_JOINT or ADAPT_GENERATION","title":"Run Spec Pattern"},{"location":"notes/#computational-estimates","text":"","title":"Computational Estimates"},{"location":"notes/#per-dataset-based-on-caseholdlegalbench","text":"CaseHOLD : ~1200-1850 tokens/instance, 2 few-shot examples LegalBench : ~650-1250 tokens/instance, 5 few-shot examples Estimated runtime : 30-60 min for 1 model across all 10 skills (~1000-1500 test instances)","title":"Per-Dataset (based on CaseHOLD/LegalBench)"},{"location":"notes/#branding-changes-completed","text":"[x] README.md updated with Legal-10 mission statement [x] CITATION.bib updated (author: Jon Chung) [ ] pyproject.toml (package name, description, author) [ ] LICENSE (copyright holder) [ ] Frontend branding","title":"Branding Changes Completed"},{"location":"notes/#next-steps","text":"Decide on test set sampling strategy (all datasets vary in size) Create legal10_run_specs.py for all 10 skills Implement missing scenario files Handle SHIELD alternative (S10) Handle LegalAgentBench language issue (S1, S2)","title":"Next Steps"},{"location":"perturbations/","text":"Perturbations ::: helm.benchmark.augmentations options: filters: [\"^(?!test_).+_perturbation$\", \".+Perturbation$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Perturbations"},{"location":"perturbations/#perturbations","text":"::: helm.benchmark.augmentations options: filters: [\"^(?!test_).+_perturbation$\", \".+Perturbation$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Perturbations"},{"location":"proxy_server/","text":"Proxy Server Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! We provide a single unified entry point into accessing large language models (e.g., GPT-3, Jurassic). This provides both a web interface and a REST API. Using (for most people) To use the web interface, go to https://crfm-models.stanford.edu. To use the REST API, see demo.py . Deploying locally Create prod_env/credentials.conf to contain the API keys for any language models you have access to. { openaiApiKey: \"...\", ai21ApiKey: \"...\" } To start a local server (go to http://localhost:1959 to try it out): venv/bin/crfm-proxy-server When starting the server for the first time, the server will create an admin account with the API key: root . If you're deploying the server to production, make sure to rotate the API key of the default admin account. For macOS developers Bypass the added security that restricts multithreading by running: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES venv/bin/crfm-proxy-server","title":"Proxy Server"},{"location":"proxy_server/#proxy-server","text":"Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution! We provide a single unified entry point into accessing large language models (e.g., GPT-3, Jurassic). This provides both a web interface and a REST API.","title":"Proxy Server"},{"location":"proxy_server/#using-for-most-people","text":"To use the web interface, go to https://crfm-models.stanford.edu. To use the REST API, see demo.py .","title":"Using (for most people)"},{"location":"proxy_server/#deploying-locally","text":"Create prod_env/credentials.conf to contain the API keys for any language models you have access to. { openaiApiKey: \"...\", ai21ApiKey: \"...\" } To start a local server (go to http://localhost:1959 to try it out): venv/bin/crfm-proxy-server When starting the server for the first time, the server will create an admin account with the API key: root . If you're deploying the server to production, make sure to rotate the API key of the default admin account.","title":"Deploying locally"},{"location":"proxy_server/#for-macos-developers","text":"Bypass the added security that restricts multithreading by running: OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES venv/bin/crfm-proxy-server","title":"For macOS developers"},{"location":"quick_start/","text":"Quick Start","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"","title":"Quick Start"},{"location":"run_entries/","text":"Run Entries Using run entries Run entries are the main way of specifying to helm-run which evaluation runs to execute. For instance, in order to evaluate GPT-2 on MedQA, we would pass the following run entry to helm-run : med_qa:model=openai/gpt2 There are two ways of passing the run entry to helm-run . We can use the --run-entries flag. For example: helm-run --run-entries med_qa:model=openai/gpt2 --suite my-suite --max-eval-instances 10 Alternatively, we can put the run entry into a run_entries.conf file, and the pass that file to helm-run using the --conf-file flag. The run_entries.conf file is a run entry configuration file that conforms to the format documented here . For example: helm-run --conf-file run_entries.conf --suite my-suite --max-eval-instances 10 Constructing run entires Specifying the run spec function name The first part of the run entry before the : is the run spec function name. For example, in the run entry med_qa:model=openai/gpt2 , the run spec function name is med_qa . A catalog of all run spec function names will be added to the documentation in the future. For now, the best way to find the run spec function name is to look through functions decorated with the @run_spec_function() in the Python modules helm.benchmark.run_specs.*_run_specs . The run spec function name is the decorator's parameter e.g. @run_spec_function(\"med_qa\") indicates a run spec function name of med_qa . Note: the run spec function name is frequently the same as the scenario name by convention, but this is not always the case. For instance, the openbookqa scenario has a run spec function that is named commonsense . Run entry arguments The second part of the run entry after the : is a mapping of argument names to argument values. The string has the format arg_name_1=arg_value_1,arg_name_2=arg_value_2 i.e. the name and value of each argument is joined by = , and the argument name-value pairs are joined by , . All argument values must be non-empty strings. The run entry arguments are used for two different things: run spec function arguments, and run expanders. For instance, in the example run entry mmlu:subject=anatomy,model=openai/gpt2 , a run spec function argument is specified by subject=anatomy , and a run expander is specified by model=openai/gpt2 . As in the above example, you can mix run expanders and run spec function arguments in a single run entry. If there is a name conflict between a run expander name and a run spec function argument name, the run expander has precedence. Run spec function arguments Some run spec functions take in arguments. For instance, the MMLU run spec function get_mmlu_spec() takes in a subject argument. MMLU is a question answering scenario that covers multiple academic subjects. The subject argument specifies that the question set corresponding to that academic subject should be used for that evaluation run. For instance, to evaluate MMLU with the anatomy subject on GPT-2, the run entry should be: mmlu:subject=anatomy,model=openai/gpt2 A catalog of all run spec functions' parameters will be added to the documentation in the future. For now, the best way to find the run spec function parameters would be to inspect the function definition in the Python modules helm.benchmark.run_specs.*_run_specs for the run spec function in question. Run expanders Run expanders are functions that modify how evaluation runs work. Concretely, a run expander operates on a configuration of an evaluation run (a RunSpec ) and produces zero, one or multiple evaluation runs configurations with modified configurations ( RunSpecs ). Run expanders are an advanced topic. For most use cases, the only run expander that you will need to use is the model run expander. The model=openai/gpt2 argument pair in the run entry indicates that the evaluation run should use the openai/gpt2 model. More explanation may be added to the documentation in the future. Run entry naming The first part of the run entry name is usually be the name of the scenario by convention, but this may not always be the case. For instance, the run entry commonsense:dataset=openbookqa,model=openai/gpt2 uses the openbookqa scenario. The first part of the run entry name is usually be the name of the run spec function name by convention, but this may not always be the case. For instance, the run entry disinformation:type=wedging,model=openai/gpt2 results in the RunSpec name disinfo:type=wedging,model=openai_gpt2 . Run entries and RunSpec s You may have noticed that some run entries can produce multiple evaluation runs. Concretely, single run entry can produce multiple RunSpec s, and each RunSpec specifies a single evaluation run. This is because run expanders are functions that take in a RunSpec and can produce multiple RunSpec . As explained previously, the model run expander is an example of this. The model run expander The model run expander is the most commonly used run expander. As discussed earlier, it can be used to set the model for each run entry. The model run expander also supports wildcard values . For instance, the med_qa:model=text run entry will run the med_qa scenario on every text model that helm-run can find in its configuration files. The wildcard is intended to be used in conjuction with the --models-to-run , which controls which models will actually be evaluated. For example, helm-run --run-entries med_qa:model=text --models-to-run openai/gpt2 openai/gpt-3.5-turbo-613 will run med_qa on only openai/gpt2 and openai/gpt-3.5-turbo-613 . Wildcard values for the model run expander are common used in run entries configuration files which will are discussed here .","title":"Run Entries"},{"location":"run_entries/#run-entries","text":"","title":"Run Entries"},{"location":"run_entries/#using-run-entries","text":"Run entries are the main way of specifying to helm-run which evaluation runs to execute. For instance, in order to evaluate GPT-2 on MedQA, we would pass the following run entry to helm-run : med_qa:model=openai/gpt2 There are two ways of passing the run entry to helm-run . We can use the --run-entries flag. For example: helm-run --run-entries med_qa:model=openai/gpt2 --suite my-suite --max-eval-instances 10 Alternatively, we can put the run entry into a run_entries.conf file, and the pass that file to helm-run using the --conf-file flag. The run_entries.conf file is a run entry configuration file that conforms to the format documented here . For example: helm-run --conf-file run_entries.conf --suite my-suite --max-eval-instances 10","title":"Using run entries"},{"location":"run_entries/#constructing-run-entires","text":"","title":"Constructing run entires"},{"location":"run_entries/#specifying-the-run-spec-function-name","text":"The first part of the run entry before the : is the run spec function name. For example, in the run entry med_qa:model=openai/gpt2 , the run spec function name is med_qa . A catalog of all run spec function names will be added to the documentation in the future. For now, the best way to find the run spec function name is to look through functions decorated with the @run_spec_function() in the Python modules helm.benchmark.run_specs.*_run_specs . The run spec function name is the decorator's parameter e.g. @run_spec_function(\"med_qa\") indicates a run spec function name of med_qa . Note: the run spec function name is frequently the same as the scenario name by convention, but this is not always the case. For instance, the openbookqa scenario has a run spec function that is named commonsense .","title":"Specifying the run spec function name"},{"location":"run_entries/#run-entry-arguments","text":"The second part of the run entry after the : is a mapping of argument names to argument values. The string has the format arg_name_1=arg_value_1,arg_name_2=arg_value_2 i.e. the name and value of each argument is joined by = , and the argument name-value pairs are joined by , . All argument values must be non-empty strings. The run entry arguments are used for two different things: run spec function arguments, and run expanders. For instance, in the example run entry mmlu:subject=anatomy,model=openai/gpt2 , a run spec function argument is specified by subject=anatomy , and a run expander is specified by model=openai/gpt2 . As in the above example, you can mix run expanders and run spec function arguments in a single run entry. If there is a name conflict between a run expander name and a run spec function argument name, the run expander has precedence.","title":"Run entry arguments"},{"location":"run_entries/#run-spec-function-arguments","text":"Some run spec functions take in arguments. For instance, the MMLU run spec function get_mmlu_spec() takes in a subject argument. MMLU is a question answering scenario that covers multiple academic subjects. The subject argument specifies that the question set corresponding to that academic subject should be used for that evaluation run. For instance, to evaluate MMLU with the anatomy subject on GPT-2, the run entry should be: mmlu:subject=anatomy,model=openai/gpt2 A catalog of all run spec functions' parameters will be added to the documentation in the future. For now, the best way to find the run spec function parameters would be to inspect the function definition in the Python modules helm.benchmark.run_specs.*_run_specs for the run spec function in question.","title":"Run spec function arguments"},{"location":"run_entries/#run-expanders","text":"Run expanders are functions that modify how evaluation runs work. Concretely, a run expander operates on a configuration of an evaluation run (a RunSpec ) and produces zero, one or multiple evaluation runs configurations with modified configurations ( RunSpecs ). Run expanders are an advanced topic. For most use cases, the only run expander that you will need to use is the model run expander. The model=openai/gpt2 argument pair in the run entry indicates that the evaluation run should use the openai/gpt2 model. More explanation may be added to the documentation in the future.","title":"Run expanders"},{"location":"run_entries/#run-entry-naming","text":"The first part of the run entry name is usually be the name of the scenario by convention, but this may not always be the case. For instance, the run entry commonsense:dataset=openbookqa,model=openai/gpt2 uses the openbookqa scenario. The first part of the run entry name is usually be the name of the run spec function name by convention, but this may not always be the case. For instance, the run entry disinformation:type=wedging,model=openai/gpt2 results in the RunSpec name disinfo:type=wedging,model=openai_gpt2 .","title":"Run entry naming"},{"location":"run_entries/#run-entries-and-runspecs","text":"You may have noticed that some run entries can produce multiple evaluation runs. Concretely, single run entry can produce multiple RunSpec s, and each RunSpec specifies a single evaluation run. This is because run expanders are functions that take in a RunSpec and can produce multiple RunSpec . As explained previously, the model run expander is an example of this.","title":"Run entries and RunSpecs"},{"location":"run_entries/#the-model-run-expander","text":"The model run expander is the most commonly used run expander. As discussed earlier, it can be used to set the model for each run entry. The model run expander also supports wildcard values . For instance, the med_qa:model=text run entry will run the med_qa scenario on every text model that helm-run can find in its configuration files. The wildcard is intended to be used in conjuction with the --models-to-run , which controls which models will actually be evaluated. For example, helm-run --run-entries med_qa:model=text --models-to-run openai/gpt2 openai/gpt-3.5-turbo-613 will run med_qa on only openai/gpt2 and openai/gpt-3.5-turbo-613 . Wildcard values for the model run expander are common used in run entries configuration files which will are discussed here .","title":"The model run expander"},{"location":"run_entries_configuration_files/","text":"Run Entries Configuration Files In the tutorial, we have been using --run-entries to specify run entries for helm-run . However, we can also put the run entries into a run entries configuration file , and then pass the file to helm-run using the --conf-file flag. This has a number of advantages: This prevents the command line invocation of helm-run from getting too long when a large number of run entries are run. The run entries configuration file can be shared with other users and commited to Git. For example, instead of running: helm-run --run-specs mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite tutorial --max-eval-instances 10 You can instead create a tutorial_run_entries.conf file in your current working directory: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 Model Run Expander Wildcards It is very common to use run entries configuration file with a model run expander wildcards e.g. model=text . For instance, entries: [ {description: \"mmlu:subject=anatomy,model=text\", priority: 1}, {description: \"mmlu:subject=philosophy,model=text\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 --models-to-run openai/gpt2 This has exactly the same behavior has the previous example. For more information on model run expander wildcards, refer to the run entry format documentation. Priorities You can use the --priority flag in conjunction with --conf-file . This filters out run entries with a higher priority value than the specified --priority value. For instance, with this run entries configuration file: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 2}, ] If run with --priority 1 , only the first run entry will be run, and the second will be filtered out. If run with --priority 2 , both run entries will be run.","title":"Run Entries Configuration Files"},{"location":"run_entries_configuration_files/#run-entries-configuration-files","text":"In the tutorial, we have been using --run-entries to specify run entries for helm-run . However, we can also put the run entries into a run entries configuration file , and then pass the file to helm-run using the --conf-file flag. This has a number of advantages: This prevents the command line invocation of helm-run from getting too long when a large number of run entries are run. The run entries configuration file can be shared with other users and commited to Git. For example, instead of running: helm-run --run-specs mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite tutorial --max-eval-instances 10 You can instead create a tutorial_run_entries.conf file in your current working directory: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10","title":"Run Entries Configuration Files"},{"location":"run_entries_configuration_files/#model-run-expander-wildcards","text":"It is very common to use run entries configuration file with a model run expander wildcards e.g. model=text . For instance, entries: [ {description: \"mmlu:subject=anatomy,model=text\", priority: 1}, {description: \"mmlu:subject=philosophy,model=text\", priority: 1}, ] You would then use this file with helm-run with this command: helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 --models-to-run openai/gpt2 This has exactly the same behavior has the previous example. For more information on model run expander wildcards, refer to the run entry format documentation.","title":"Model Run Expander Wildcards"},{"location":"run_entries_configuration_files/#priorities","text":"You can use the --priority flag in conjunction with --conf-file . This filters out run entries with a higher priority value than the specified --priority value. For instance, with this run entries configuration file: entries: [ {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1}, {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 2}, ] If run with --priority 1 , only the first run entry will be run, and the second will be filtered out. If run with --priority 2 , both run entries will be run.","title":"Priorities"},{"location":"scenarios/","text":"Scenarios ::: helm.benchmark.scenarios options: filters: [\"^(?!test_).+_scenario$\", \"Scenario$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Scenarios"},{"location":"scenarios/#scenarios","text":"::: helm.benchmark.scenarios options: filters: [\"^(?!test_).+_scenario$\", \"Scenario$\"] show_submodules: true show_root_heading: false show_root_toc_entry: false members_order: alphabetical","title":"Scenarios"},{"location":"schemas/","text":"div.doc-function { display: none; } Schemas ::: helm.benchmark.scenarios.scenario.Scenario ::: helm.benchmark.adaptation.scenario_state.ScenarioState ::: helm.benchmark.adaptation.request_state.RequestState ::: helm.benchmark.scenarios.scenario.Instance ::: helm.benchmark.scenarios.scenario.Reference ::: helm.benchmark.augmentations.perturbation_description.PerturbationDescription ::: helm.common.request.Request ::: helm.common.request.RequestResult ::: helm.benchmark.metrics.metric.PerInstanceStats ::: helm.benchmark.metrics.statistic.Stat","title":"Schemas"},{"location":"schemas/#schemas","text":"::: helm.benchmark.scenarios.scenario.Scenario ::: helm.benchmark.adaptation.scenario_state.ScenarioState ::: helm.benchmark.adaptation.request_state.RequestState ::: helm.benchmark.scenarios.scenario.Instance ::: helm.benchmark.scenarios.scenario.Reference ::: helm.benchmark.augmentations.perturbation_description.PerturbationDescription ::: helm.common.request.Request ::: helm.common.request.RequestResult ::: helm.benchmark.metrics.metric.PerInstanceStats ::: helm.benchmark.metrics.statistic.Stat","title":"Schemas"},{"location":"skill_10_copyright_compliance/","text":"Skill 10: Copyright Compliance","title":"Skill 10: Copyright Compliance"},{"location":"skill_10_copyright_compliance/#skill-10-copyright-compliance","text":"","title":"Skill 10: Copyright Compliance"},{"location":"skill_1_research_planning/","text":"Skill 1: Research Planning","title":"Skill 1: Research Planning"},{"location":"skill_1_research_planning/#skill-1-research-planning","text":"","title":"Skill 1: Research Planning"},{"location":"skill_2_strategic_stopping/","text":"Skill 2: Strategic Stopping","title":"Skill 2: Strategic Stopping"},{"location":"skill_2_strategic_stopping/#skill-2-strategic-stopping","text":"","title":"Skill 2: Strategic Stopping"},{"location":"skill_3_known_authority/","text":"Skill 3: Known Authority","title":"Skill 3: Known Authority"},{"location":"skill_3_known_authority/#skill-3-known-authority","text":"","title":"Skill 3: Known Authority"},{"location":"skill_4_unknown_authority/","text":"Skill 4: Unknown Authority","title":"Skill 4: Unknown Authority"},{"location":"skill_4_unknown_authority/#skill-4-unknown-authority","text":"","title":"Skill 4: Unknown Authority"},{"location":"skill_5_validate_authority/","text":"Skill 5: Validate Authority","title":"Skill 5: Validate Authority"},{"location":"skill_5_validate_authority/#skill-5-validate-authority","text":"","title":"Skill 5: Validate Authority"},{"location":"skill_6_fact_extraction/","text":"Skill 6: Fact Extraction","title":"Skill 6: Fact Extraction"},{"location":"skill_6_fact_extraction/#skill-6-fact-extraction","text":"","title":"Skill 6: Fact Extraction"},{"location":"skill_7_distinguish_cases/","text":"Skill 7: Distinguish Cases","title":"Skill 7: Distinguish Cases"},{"location":"skill_7_distinguish_cases/#skill-7-distinguish-cases","text":"","title":"Skill 7: Distinguish Cases"},{"location":"skill_8_synthesize_results/","text":"Skill 8: Synthesize Results","title":"Skill 8: Synthesize Results"},{"location":"skill_8_synthesize_results/#skill-8-synthesize-results","text":"","title":"Skill 8: Synthesize Results"},{"location":"skill_9_citation_integrity/","text":"Skill 9: Citation Integrity","title":"Skill 9: Citation Integrity"},{"location":"skill_9_citation_integrity/#skill-9-citation-integrity","text":"","title":"Skill 9: Citation Integrity"},{"location":"tutorial/","text":"Tutorial This tutorial will explain how to use the HELM command line tools to run benchmarks, aggregate statistics, and visualize results. We will run two runs using the mmlu scenario on the openai/gpt2 model. The mmlu scenario implements the Massive Multitask Language (MMLU) benchmark from this paper , and consists of a Question Answering (QA) task using a dataset with questions from 57 subjects such as elementary mathematics, US history, computer science, law, and more. Note that GPT-2 performs poorly on MMLU, so this is just a proof of concept. We will run two runs: the first using questions about anatomy, and the second using questions about philosophy. Using helm-run helm-run is a command line tool for running benchmarks. To run this benchmark using the HELM command-line tools, we need to specify run entries that describes the desired runs. For this example, the run entries are mmlu:subject=anatomy,model=openai/gpt2 (for anatomy) and mmlu:subject=philosophy,model=openai/gpt2 (for philosophy). We will now use helm-run to execute the runs. Run this command: helm-run --run-entries mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite my-suite --max-eval-instances 10 The meaning of the arguments are as follows: --run-entries specifies the run entries from the desired runs. --suite specifies a subdirectory under the output directory in which all the output will be placed. --max-eval-instances limits evaluation to only N instances (i.e. items) from the benchmark, using a randomly shuffled order of instances. helm-run creates an environment directory environment and an output directory by default. The environment directory is prod_env/ by default and can be set using --local-path . Credentials for making API calls should be added to a credentials.conf file in this directory. The output directory is benchmark_output/ by default and can be set using --output-path . After running this command, navigate to the benchmark_output/runs/my-suite/ directory. This should contain a two sub-directories named mmlu:subject=anatomy,model=openai_gpt2 and mmlu:subject=philosophy,model=openai_gpt2 . Note that the names of these sub-directories is based on the run entries we used earlier, but with / replaced with _ . Each output sub-directory will contain several JSON files that were generated during the corresponding run: run_spec.json contains the RunSpec , which specifies the scenario, adapter and metrics for the run. scenario.json contains a serialized Scenario , which contains the scenario for the run and specifies the instances (i.e. inputs) used. scenario_state.json contains a serialized ScenarioState , which contains every request to and response from the model. per_instance_stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics for each instance (i.e. input). stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics, aggregated across all instances (i.e. inputs). Using helm-summarize The helm-summarize reads the output files of helm-run and computes aggregate statistics across runs. Run the following: helm-summarize --suite my-suite This reads the pre-existing files in benchmark_output/runs/my-suite/ that were written by helm-run previously, and writes the following new files back to benchmark_output/runs/my-suite/ : summary.json contains a serialized ExecutiveSummary with a date and suite name. run_specs.json contains the run entries for all the runs. runs.json contains serialized list of Run , which contains the run path, run spec and adapter spec and statistics for each run. groups.json contains a serialized list of Table , each containing information about groups in a group category. groups_metadata.json contains a list of all the groups along with a human-readable description and a taxonomy. Additionally, for each group and group-relavent metric, it will output a pair of files: benchmark_output/runs/my-suite/groups/latex/<group_name>_<metric_name>.tex and benchmark_output/runs/my-suite/groups/json/<group_name>_<metric_name>.json . These files contain the statistics for that metric from each run within the group. Using helm-server Finally, the helm-server command launches a web server to visualize the output files of helm-run and helm-benchmark . Run: helm-server --suite my-suite Open a browser and go to http://localhost:8000/ to view the visualization. You should see a similar view as live website for the paper , but for the data from your benchmark runs. The website has the following sections accessible from the top menu bar: Leaderboards contains the leaderboards with aggregate metrics. Models contains a list of models and their descriptions Scenarios contains a list of scenarios and their descriptions. Predictions contains a searchable list of runs.","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"This tutorial will explain how to use the HELM command line tools to run benchmarks, aggregate statistics, and visualize results. We will run two runs using the mmlu scenario on the openai/gpt2 model. The mmlu scenario implements the Massive Multitask Language (MMLU) benchmark from this paper , and consists of a Question Answering (QA) task using a dataset with questions from 57 subjects such as elementary mathematics, US history, computer science, law, and more. Note that GPT-2 performs poorly on MMLU, so this is just a proof of concept. We will run two runs: the first using questions about anatomy, and the second using questions about philosophy.","title":"Tutorial"},{"location":"tutorial/#using-helm-run","text":"helm-run is a command line tool for running benchmarks. To run this benchmark using the HELM command-line tools, we need to specify run entries that describes the desired runs. For this example, the run entries are mmlu:subject=anatomy,model=openai/gpt2 (for anatomy) and mmlu:subject=philosophy,model=openai/gpt2 (for philosophy). We will now use helm-run to execute the runs. Run this command: helm-run --run-entries mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite my-suite --max-eval-instances 10 The meaning of the arguments are as follows: --run-entries specifies the run entries from the desired runs. --suite specifies a subdirectory under the output directory in which all the output will be placed. --max-eval-instances limits evaluation to only N instances (i.e. items) from the benchmark, using a randomly shuffled order of instances. helm-run creates an environment directory environment and an output directory by default. The environment directory is prod_env/ by default and can be set using --local-path . Credentials for making API calls should be added to a credentials.conf file in this directory. The output directory is benchmark_output/ by default and can be set using --output-path . After running this command, navigate to the benchmark_output/runs/my-suite/ directory. This should contain a two sub-directories named mmlu:subject=anatomy,model=openai_gpt2 and mmlu:subject=philosophy,model=openai_gpt2 . Note that the names of these sub-directories is based on the run entries we used earlier, but with / replaced with _ . Each output sub-directory will contain several JSON files that were generated during the corresponding run: run_spec.json contains the RunSpec , which specifies the scenario, adapter and metrics for the run. scenario.json contains a serialized Scenario , which contains the scenario for the run and specifies the instances (i.e. inputs) used. scenario_state.json contains a serialized ScenarioState , which contains every request to and response from the model. per_instance_stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics for each instance (i.e. input). stats.json contains a serialized list of PerInstanceStats , which contains the statistics produced for the metrics, aggregated across all instances (i.e. inputs).","title":"Using helm-run"},{"location":"tutorial/#using-helm-summarize","text":"The helm-summarize reads the output files of helm-run and computes aggregate statistics across runs. Run the following: helm-summarize --suite my-suite This reads the pre-existing files in benchmark_output/runs/my-suite/ that were written by helm-run previously, and writes the following new files back to benchmark_output/runs/my-suite/ : summary.json contains a serialized ExecutiveSummary with a date and suite name. run_specs.json contains the run entries for all the runs. runs.json contains serialized list of Run , which contains the run path, run spec and adapter spec and statistics for each run. groups.json contains a serialized list of Table , each containing information about groups in a group category. groups_metadata.json contains a list of all the groups along with a human-readable description and a taxonomy. Additionally, for each group and group-relavent metric, it will output a pair of files: benchmark_output/runs/my-suite/groups/latex/<group_name>_<metric_name>.tex and benchmark_output/runs/my-suite/groups/json/<group_name>_<metric_name>.json . These files contain the statistics for that metric from each run within the group.","title":"Using helm-summarize"},{"location":"tutorial/#using-helm-server","text":"Finally, the helm-server command launches a web server to visualize the output files of helm-run and helm-benchmark . Run: helm-server --suite my-suite Open a browser and go to http://localhost:8000/ to view the visualization. You should see a similar view as live website for the paper , but for the data from your benchmark runs. The website has the following sections accessible from the top menu bar: Leaderboards contains the leaderboards with aggregate metrics. Models contains a list of models and their descriptions Scenarios contains a list of scenarios and their descriptions. Predictions contains a searchable list of runs.","title":"Using helm-server"}]}