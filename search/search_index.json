{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Legal-10","text":""},{"location":"#legal-10-benchmark","title":"Legal-10 Benchmark","text":"<p>Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG).</p> <p>We open-source everything: datasets, harness, scoring, and operational tooling, while ensuring we protect test We will release benchmark. We support evaluation of both open-weight and closed API models via a reproducible run-bundle protocol.</p>"},{"location":"#key-principle","title":"Key Principle","text":"<p>Grounded in Professional Standards. </p> <ul> <li>MacCrate Report (ABA, 1992) \u2014 foundational lawyering skills</li> <li>AALL Principles &amp; Standards (2013) \u2014 legal research competency standards</li> <li>Shultz &amp; Zedeck (2011) \u2014 empirically-derived lawyer effectiveness factors</li> </ul> <p>Every skill maps to reputable frameworks the legal profession has validated over 30+ years. These standards are used to translate human-lawyer competencies into 10 minimal viable tests across distinct cognitive circuits.</p> <p>Testing conducted using three modalities of Agentic Workflow (AG), RAG, or Closed Book (CB). Each skill is tested in the modality that most closely matches how human lawyers perform that skill in computer-mediated work.</p>"},{"location":"#the-10-skills","title":"The 10 Skills","text":"Skill Name Modality Dataset S1 Research Planning AG L10 AG S2 Strategic Stopping AG L10 AG S3 Known Authority RAG CLERC S4 Unknown Authority RAG CLERC S5 Validate Authority RAG CLERC S6 Fact Extraction RAG CUAD S7 Distinguish Cases CB CaseHOLD S8 Synthesize Results CB LEXam S9 Citation Integrity CB S10 Copyright Compliance CB SHIELD"},{"location":"#extensions","title":"Extensions","text":"# Name Modality Dataset 1 Multilingual Reasoning CB FairLex"},{"location":"#transparent-community-driven","title":"Transparent, Community-Driven","text":"<ul> <li>open-source harness, datasets, assessment criteria</li> <li>Auditable run bundles, SHA-256 hashes, signed manifests</li> <li>Maintain transparent coverage disclosure, governance</li> <li>Public append-only log \u2014 all submissions preserved</li> </ul>"},{"location":"#objective-informative","title":"Objective, Informative","text":"<ul> <li>ensure understanding of quality implications of products being used by industry end users.</li> <li>ensure evaluations are controlled so reports are objective and purposeful</li> <li>Built using Stanford CFRM HELM (Holistic Evaluation of Language Models) Architecture</li> </ul>"},{"location":"#scoreboard-policy","title":"Scoreboard Policy","text":"<ul> <li>No one games system. Make it transparently clear. </li> <li>No selective omission or quiet withdrawal \u2192 if 10, then run all 10. </li> <li>Skill-level reporting \u2192 system prevents unsatisfactory areas or skills from being hidden </li> <li>system prevents failures from being hidden </li> </ul>"},{"location":"adding_new_models/","title":"Adding New Models","text":"<p>HELM comes with more than a hundred built-in models. If you want to run a HELM evaluation on a model that is not built-in, you can configure HELM to add your own model. This also allows you to evaluate private models that are not publicly accessible, such as a model checkpoint on local disk, or a model server on a private network</p> <p>HELM comes with many built-in <code>Client</code> classes (i.e. model API clients) and <code>Tokenizer</code> clients. If there is already an existing <code>Client</code> and <code>Tokenizer</code> class for your use case, you can simply add it to your local configuration. You would only need to implement a new class if you are adding a model with a API format or inference platform that is currently not supported by HELM.</p> <p>If you wish to evaluate a model not covered by an existing <code>Client</code> and <code>Tokenizer</code>, you can implement your own <code>Client</code> and <code>Tokenizer</code> subclasses. Instructions for adding custom <code>Client</code> and <code>Tokenizer</code> subclasses will be added to the documentation in the future.</p>"},{"location":"adding_new_models/#adding-a-model-locally","title":"Adding a Model Locally","text":""},{"location":"adding_new_models/#model-metadata","title":"Model Metadata","text":"<p>Create a local model metadata configuration file if it does not already exist. The file should be a <code>prod_env/model_metadata.yaml</code> by default, or at <code>$LOCAL_PATH/model_metadata.yaml</code> if <code>--local-path</code> is set where <code>$LOCAL_FOLDER</code> is the value of the flag.</p> <p>This file should contain a YAML-formatted <code>ModelMetadataList</code> object. For an example of this format, refer to <code>model_metadata.yaml</code> in the GitHub repository, or follow the example below:</p> <pre><code>models:\n  - name: eleutherai/pythia-70m\n    display_name: Pythia (70M)\n    description: Pythia (70M parameters). The Pythia project combines interpretability analysis and scaling laws to understand how knowledge develops and evolves during training in autoregressive transformers.\n    creator_organization_name: EleutherAI\n    access: open\n    num_parameters: 95600000\n    release_date: 2023-02-13\n    tags: [TEXT_MODEL_TAG, PARTIAL_FUNCTIONALITY_TEXT_MODEL_TAG]\n</code></pre>"},{"location":"adding_new_models/#model-deployment","title":"Model Deployment","text":"<p>A model deployment defines the actual implementation of the model. The model deployment configuration tells HELM how to generate outputs from the model model by running local inference or or sending requests to an API. Every model should have at least one model deployment. However, since there are sometimes multiple implementations or inference platform providers for the same model, a model can have more than one model deployment. For instance, the model <code>google/gemma-2-9b-it</code> has the model deployments <code>together/gemma-2-9b-it</code> (remote inference using Together AI's API) and <code>google/gemma-2-9b-it</code> (local inference with Hugging Face).</p> <p>Create a local model deployments configuration file if it does not already exist. The file should be a <code>prod_env/model_metadata.yaml</code> by default, or at <code>$LOCAL_PATH/model_metadata.yaml</code> if <code>--local-path</code> is set where <code>$LOCAL_FOLDER</code> is the value of the flag.</p> <p>This file should contain a YAML-formatted <code>ModelDeployments</code> object. For an example of this format, refer to <code>model_deployments.yaml</code> in the GitHub repository, or follow an example below for your preferred model platform.</p> <p>Note that the model deployment name will frequently differ from the model name. The model deployment name should be <code>$HOST_ORGANIZATON/$MODEL_NAME</code>, while the model name should be <code>$CREATOR_ORGANIZATON/$MODEL_NAME</code>.</p>"},{"location":"adding_new_models/#hugging-face","title":"Hugging Face","text":"<p>Example:</p> <pre><code>model_deployments:\n  - name: huggingface/pythia-70m\n    model_name: eleutherai/pythia-70m\n    tokenizer_name: EleutherAI/gpt-neox-20b\n    max_sequence_length: 2048\n    client_spec:\n      class_name: \"helm.clients.huggingface_client.HuggingFaceClient\"\n      args:\n        pretrained_model_name_or_path: EleutherAI/pythia-70m\n</code></pre> <p>Note: If <code>pretrained_model_name_or_path</code> is omitted, the model will be loaded from Hugging Face Hub using <code>model_name</code> (not <code>name</code>) by default.</p> <p>Examples of common arguments within <code>args</code>:</p> <ul> <li>Loading from local disk: <code>pretrained_model_name_or_path: /path/to/my/model</code></li> <li>Revision: <code>revision: my_revision</code></li> <li>Quantization: <code>load_in_8bit: true</code></li> <li>Model precision: <code>torch_dtype: torch.float16</code></li> <li>Model device: <code>device: cpu</code> or <code>device: cuda:0</code></li> <li>Allow running remote code: <code>trust_remote_code: true</code></li> <li>Multi-GPU: <code>device_map: auto</code></li> </ul> <p>Notes:</p> <ul> <li>This uses local inference with Hugging Face. It will attempt to use GPU inference if available, and use CPU inference otherwise.</li> <li>Multi-GPU inference can be enabled by setting <code>device_map: auto</code> in the <code>args</code>.</li> <li>GPU models loaded by <code>helm-run</code> will remain loaded on the GPU for the lifespan of <code>helm-run</code>.</li> <li>If evaluating multiple models, it is prudent to evaluate each model with a separate <code>helm-run</code> invocation.</li> <li>If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 3), you need to be authenticated to Hugging Face through the CLI. As the user that will be running <code>helm-run</code>, run <code>huggingface-cli login</code> in your shell. Refer to Hugging Face's documentation for more information.</li> </ul>"},{"location":"adding_new_models/#vllm","title":"vLLM","text":"<pre><code>model_deployments:\n  - name: vllm/pythia-70m\n    model_name: eleutherai/pythia-70m\n    tokenizer_name: EleutherAI/gpt-neox-20b\n    max_sequence_length: 2048\n    client_spec:\n      class_name: \"helm.clients.vllm_client.VLLMClient\"\n      args:\n        base_url:  http://mymodelserver:8000/v1/\n</code></pre> <p>For non-chat models, set <code>class_name</code> in <code>client_spec</code> to <code>helm.clients.vllm_client.VLLMClient</code>. For chat models, set <code>class_name</code> in <code>client_spec</code> to <code>helm.clients.vllm_client.VLLMChatClient</code>.</p> <p>Set <code>base_url</code> to the URL of your inference server. On your inference server, run vLLM's OpenAI compatible server with:</p> <pre><code>python -m vllm.entrypoints.openai.api_server --model EleutherAI/pythia-70m\n</code></pre>"},{"location":"adding_new_models/#together-ai","title":"Together AI","text":"<pre><code>model_deployments:\n  - name: together/gemma-2-9b-it\n    model_name: google/gemma-2-9b-it\n    tokenizer_name: google/gemma-2-9b\n    max_sequence_length: 8191\n    client_spec:\n      class_name: \"helm.clients.together_client.TogetherClient\"\n      args:\n        together_model: google/gemma-2-9b-it\n</code></pre> <p>Notes:</p> <ul> <li>You will need to add Together AI credentials to your credentials file e.g. add <code>togetherApiKey: your-api-key</code> to <code>./prod_env/credentials.conf</code>.</li> <li>If <code>together_model</code> is omitted, the Together model with <code>model_name</code> (not <code>name</code>) will be used by default.</li> <li>This above model may not be currently available on Together AI. Consult Together AI's Inference Models documentation for a list of currently available models and corresponding model strings.</li> </ul>"},{"location":"adding_new_models/#testing-new-models","title":"Testing New Models","text":"<p>After you've added your model, you can run your model with <code>helm-run</code> using a run entry such as <code>mmlu:subject=anatomy,model=your-org/your-model</code>. It is also recommended to use the <code>--disable-cache</code> flag so that in the event that you made a mistake, the incorrect requests are not written to the request cache. Example:</p> <pre><code>helm-run --run-entry mmlu:subject=anatomy,model=your-org/your-model --suite my-suite --max-eval-instances 10 --disable-cache\n\nhelm-summarize --suite my-suite\n\nhelm-server\n</code></pre>"},{"location":"adding_new_models/#adding-new-models-to-helm","title":"Adding New Models to HELM","text":"<p>If your model is publicly accessible, you may want to add it to the HELM itself so that all HELM users may use the model. This should only be done only if the model may be easily accessible by other users.</p> <p>To do so, simply add your new model metadata and model deployments to the respective configuration files in the HELM repository at <code>src/helm/config/</code>, rather than the local config files, and then open a pull request on GitHub. If you already added your model to your local configuration files at <code>prod_env/</code>, you should move those changes to the corresponding configuration files in <code>src/helm/config/</code> - do not add the model to both <code>src/helm/config/</code> and <code>prod_env/</code> simulatenously.</p> <p>Test the changes using the same procedure above, and then open a pull request on HELM GitHub repository.</p>"},{"location":"adding_new_scenarios/","title":"Adding New Scenarios","text":"<p>HELM comes with more than a hundred built-in scenarios. However, you may want to run HELM on a scenario that is not built into HELM yet, or you may want to run HELM on scenarios that use your private datasets. Because HELM is a modular framework with a plug-in architecture, you can run evaluations with your custom scenarios on HELM without needing to modify HELM code.</p> <p>There are two steps to adding a custom scenario: adding the custom <code>Scenario</code> subclass, and adding a custom run spec function.</p> <p>The easiest way to implement the custom <code>Scenario</code> subclass and the custom run spec function would be to copy from an appropriate example and then make the appropriate modifications. Determine the task of your scenario, then find the corresponding example <code>Scenario</code> subclass and run spec function from the list below from the <code>simple_scenarios.py</code> and <code>simple_run_specs.py</code> files:</p> <ul> <li>Multiple-choice question answering: <code>SimpleMCQAScenario</code> and <code>get_simple_mcqa_run_spec()</code></li> <li>Short-answer question answering: <code>SimpleShortAnswerQAScenario</code> and <code>get_simple_short_answer_qa_run_spec()</code></li> <li>Open-ended question answering: This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations.</li> <li>Summarization: This is similar to short-answer question answering, but overlap-based automated metrics may be unsuitable for long generations.</li> <li>Multi-class classification: <code>SimpleClassificationScenario</code> and <code>get_simple_classification_run_spec()</code></li> <li>Sentiment analysis: This a sub-type of the Classification task. Set <code>input_noun</code>, <code>output_noun</code> and <code>instructions</code> appropriately.</li> <li>Toxicity detection: This a sub-type of the Classification task. Set <code>input_noun</code>, <code>output_noun</code> and <code>instructions</code> appropriately.</li> <li>Multi-label classification: This is currently unsupported by HELM.</li> <li>Named entity recognition: This is currently unsupported by HELM.</li> </ul> <p>If your task is not listed, you may still implement your task using custom adapters and metrics, but there is limited official support for doing so.</p>"},{"location":"adding_new_scenarios/#custom-scenario-subclass","title":"Custom <code>Scenario</code> subclass","text":"<p>For this tutorial, we will create a <code>MyScenario</code> class in the the <code>my_scenario</code> module. Make a file called <code>./my_scenario.py</code> under the my_scenario directory. Create a new class called <code>MyScenario</code>. Find the appropriate example scenario and copy its implementation into <code>MyScenario</code>, making sure to also copy all the required imports.</p> <p>Now we will create a test for the scenario to make sure that it is working correctly. Create a file called <code>./my_scenario_test.py</code> under the my_scenario directory. Create a <code>test_my_scenario()</code> function in this file. Find the appropriate example scenario test from <code>test_simple_scenarios.py</code> and copy its implementation into <code>test_my_scenario()</code>.</p> <p>You can now run <code>python3 -m pytest test_my_scenario.py</code> to test the example scenario. The test should pass. If you get a <code>ModuleNotFound</code> error, you should set up your <code>PYTHONPATH</code> as explained above, and then try again.</p> <p>Now, modify <code>MyScenario</code> to include the actual logic to load the instances from your dataset. Modify the test accordingly. Use the test to ensure that your implementation is working.</p>"},{"location":"adding_new_scenarios/#downloading-data-to-local-disk","title":"Downloading data to local disk","text":"<p>Frequently, your <code>Scenario</code> will want to download and cache data onto the local disk, rather than downloading it from the internet every time. The <code>output_path</code> argument passed into the <code>get_instances()</code> method will contain a file path to a scenario-specific download folder that you should download these files to. The folder will be under the <code>scenarios</code> subdirectory under the <code>benchmark_output/</code> folder (or the path specified by the <code>--output-path</code> flag for <code>helm-run</code>). You can use the <code>ensure_directory_exists()</code> and <code>ensure_file_downloaded()</code> helper functions to download files, which has the advantage of skipping the download if the file already exists. You can also use set <code>unpack=True</code> in <code>ensure_file_downloaded()</code> to automatically unpack most archive files (e.g. <code>.tar.gz</code> and <code>.zip</code> files).</p> <p>For examples, refer to:</p> <ul> <li><code>gsm_scenario.py</code> - download a JSONL files</li> <li><code>mmlu_scenario.py</code> - download CSV files</li> <li><code>narrativeqa_scenario.py</code> - download a zip file containing CSV files</li> </ul>"},{"location":"adding_new_scenarios/#working-with-hugging-face-datasets","title":"Working with Hugging Face datasets","text":"<p>Another frequent use case is downloading data from Hugging Face datasets. You can use <code>load_dataset()</code> to do so. It is recommended that you set the <code>cache_dir</code> parameter to a subdirectory within <code>output_path</code>. This ensures hermeticity by ensuring that the data is downloaded into the scenario-specific download folder.</p> <p>For an example, refer to:</p> <ul> <li><code>math_scenario.py</code></li> <li><code>legalbench_scenario.py</code></li> </ul>"},{"location":"adding_new_scenarios/#custom-run-spec-function","title":"Custom run spec function","text":"<p>A run spec function is the entry point to the scenario. A run spec function produces a <code>RunSpec</code> (a configuration for an evaluation run). <code>helm-run</code> will run the run spec function to get the <code>RunSpec</code>, and then it will run the evaluation defined by that <code>RunSpec</code>.</p> <p>HELM will search for modules with names matching these patterns for run spec functions:</p> <ul> <li><code>helm.benchmark.run_specs.*_run_specs</code></li> <li><code>helm_*_run_specs</code> (i.e. a root module)</li> </ul> <p>For this tutorial, we will create a <code>get_my_run_spec()</code> function in the <code>helm_my_run_specs</code> module. Under the <code>src/helm/benchmark/scenarios/</code> directory, create a file called <code>helm_my_run_specs.py</code>. Then, create a <code>get_my_run_spec()</code> function in this file and find the appropriate example run spec function from <code>simple_run_specs.py</code> to copy its implementation into <code>get_my_run_spec()</code>. Change the file accordingly to the needs of your scenario.</p> <p>Now run:</p> <pre><code>helm-run --run-entries custom:model=openai/gpt2 --suite custom --max-eval-instances 5\n</code></pre> <p>If you get a <code>ValueError: Unknown run spec name</code> error, you should set up your <code>PYTHONPATH</code> as explained above, and then try again.</p>"},{"location":"adding_new_scenarios/#debugging-with-models","title":"Debugging with models","text":"<p>The above run entry uses the <code>openai/gpt2</code> model, which is a lightweight model that is reasonably fast, even when using only CPU inference without a GPU.</p> <p>However, you might want to avoid waiting for model inference when implementing a scenario in order to speed up your iteration times. To do so, you can use the <code>simple/model1</code>, which simply echoes the last word in the prompt. Example <code>helm-run</code> command:</p> <pre><code>helm-run --run-entries custom:model=simple/model1 --suite custom --max-eval-instances 5\n</code></pre> <p>Note: Both the custom <code>Scenario</code> subclass and the custom run spec function will be added to custom Python modules that have to be importable by Python. The easiest way to do this is to place your custom Python modules under the current working directory and then run <code>export PYTHONPATH=\".:$PYTHONPATH\"</code> in your shell. Refer to the Importing Custom Modules documentation for other ways to do this.</p>"},{"location":"adding_new_scenarios/#contributing-your-scenario","title":"Contributing your scenario","text":"<p>We welcome scenario contributions to HELM if they fit the following criteria:</p> <ul> <li>It is commonly-used or notable benchmark (e.g. it has a published paper).</li> <li>It uses publicly available datasets.</li> <li>It fills a gap in coverage by HELM's existing scenarios.</li> </ul> <p>If your scenario fits this criteria, you should move the files to the conventional HELM locations, and open a pull request. Your <code>*_scenario.py</code> file should be placed in <code>src/helm/benchmark/scenarios/</code> and  your <code>*_run_specs.py</code> file should be placed in <code>src/helm/benchmark/scenarios/</code>. More documentation on the contributor workflow will be added later.</p>"},{"location":"adding_new_tokenizers/","title":"Adding New Tokenizers","text":"<p>HELM comes with many built-in tokenizers, but in some cases, you may need to add your own custom tokenizer for your custom model.</p>"},{"location":"adding_new_tokenizers/#creating-a-tokenizer-configuration-file","title":"Creating a tokenizer configuration file","text":"<p>Create a file called <code>tokenizer_configs.yaml</code> in your local configuration folder (e.g. <code>./prod_env/tokenizer_configs.yaml</code>).</p> <p>This file should contain a YAML-formatted <code>TokenizerConfigs</code> object. For an example of this format, refer to the built-in <code>tokenizer_configs.yaml</code> in the GitHub repository, or follow the example below for your preferred model platform.</p> <p>After adding a tokenizer configuration, you can then use the tokenizer in your custom model deployments by setting the specifying the tokenizer name in the <code>tokenizer</code> field of the model deployment.</p>"},{"location":"adding_new_tokenizers/#hugging-face-tokenizers","title":"Hugging Face tokenizers","text":"<p>To add a Hugging Face tokenizer, follow the format below, setting <code>name</code> to Hugging Face hub model ID.</p> <pre><code>tokenizer_configs:\n  - name: bigscience/bloom\n    tokenizer_spec:\n      class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\"\n      args:\n        pretrained_model_name_or_path: bigscience/bloom\n    end_of_text_token: \"&lt;s&gt;\"\n    prefix_token: \"&lt;/s&gt;\"\n</code></pre> <p>Note that <code>pretrained_model_name_or_path</code> can also be set to a path to load a Hugging Face tokenizer from local disk.</p> <p>If <code>pretrained_model_name_or_path</code> (or <code>args</code>) is omitted, the model will be loaded from Hugging Face Hub using <code>name</code> as the model ID by default. For example:</p> <pre><code>tokenizer_configs:\n  - name: bigscience/bloom\n    tokenizer_spec:\n      class_name: \"helm.tokenizers.huggingface_tokenizer.HuggingFaceTokenizer\"\n    end_of_text_token: \"&lt;s&gt;\"\n    prefix_token: \"&lt;/s&gt;\"\n</code></pre> <p>To find the values for <code>end_of_text_token</code> and <code>prefix_token</code>, you can run the following Python code snippet below (replacing <code>bigscience/bloom</code> with the Hugging Face Hub model ID). If any special token is unknown, it should be set to the empty string <code>\"\"</code>.</p> <pre><code>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\nprint(f'end_of_text_token: \"{tokenizer.eos_token}\"\\nprefix_token: \"{tokenizer.bos_token}\"')\n</code></pre> <p>HELM does not auto-infer special token information because some tokenizers on Hugging Face Model Hub may have incorrect or missing special token values. Therefore, you must manually set these values and verify that they are correct.</p>"},{"location":"agentic_research/","title":"Legal-10 Agentic Benchmark Research (Internal)","text":"<p>This page is for internal use only. The detailed research notes have been removed from the public documentation build.</p> <p>For the full internal draft, refer to <code>internal/agentic_research_internal.md</code> or contact the maintainers for access.</p>"},{"location":"benchmark/","title":"Advanced Benchmarking Guide","text":""},{"location":"benchmark/#running-restricted-benchmarks","title":"Running Restricted Benchmarks","text":"<p>Some of the benchmarks (NewsQA) depend on data that's not public: all such data will be stored in the <code>restricted</code> directory.  You need to make sure that directory exists.</p>"},{"location":"benchmark/#dry-runs","title":"Dry Runs","text":"<p>The <code>helm-run</code> provides several flags that can be used to test that the configuration and scenario are working correctly without actually sending requests to the model</p> <pre><code># Just load the config file\nhelm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10 --suite v1 --skip-instances\n\n# Create the instances and the requests, but don't send requests to the model\nhelm-run --conf src/helm/benchmark/presentation/run_entries_small.conf --max-eval-instances 10  --suite v1 --dry-run\n</code></pre>"},{"location":"benchmark/#estimating-token-usage","title":"Estimating Token Usage","text":"<p>To estimate token usage without making any requests, append the <code>--dry-run</code> option:</p> <pre><code>helm-run -r &lt;RunSpec to estimate token usage&gt; --suite $SUITE --max-eval-instances &lt;Number of eval instances&gt; --dry-run\n</code></pre> <p>and check the output in <code>benchmark_output/runs/$SUITE</code>.</p> <p><code>sum</code> indicates the estimated total number of tokens used for the specific <code>RunSpec</code>.</p> <p>For the OpenAI models, we use a GPT-2 Tokenizer to estimate the token usage. The tokenizer will be downloaded and cached when running a dry run.</p>"},{"location":"benchmark/#perspective-api","title":"Perspective API","text":"<p>We use Google's Perspective API to calculate the toxicity of completions. To send requests to PerspectiveAPI, we need to generate an API key from GCP. Follow the Get Started guide to request the service and the Enable the API guide to generate the API key. Once you have a valid API key, add an entry to <code>credentials.conf</code>:</p> <pre><code>perspectiveApiKey: &lt;Generated API key&gt;\n</code></pre> <p>By default, Perspective API allows only 1 query per second. Fill out this form to increase the request quota.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Legal-10 is organized as 10 skills, grouped into three modalities:</p> <ul> <li>Closed-Book (CB)</li> <li>Retrieval-Augmented Generation (RAG)</li> <li>Agentic (AG)</li> </ul> <p>For now, these pages describe the skills and the evaluation intent; the benchmark runs are executed through HELM so every run produces auditable artifacts under <code>benchmark_output/</code>.</p> <p>Extensions (Future): additional skills and multilingual modules will live under the Benchmarks section once they are stable.</p>"},{"location":"code/","title":"Code Structure","text":"<p>Warning \u2014 The document is stale and was last modified more than ten months ago. The information below may be outdated and incorrect. Please proceed with caution!</p>"},{"location":"code/#birds-eye-view","title":"Birds-Eye View","text":"<p>Here's a birds-eye view of how the benchmarking process interacts with the main classes (see <code>benchmark</code>):</p> <ul> <li> <p>A <code>Scenario</code> (given by a <code>ScenarioSpec</code>) specifies a task and a data   distribution.  It specifies a set of <code>Instance</code>s, where each <code>Instance</code> has   an input (e.g., question) and a set of <code>Reference</code> outputs (e.g., multiple   choice answers).</p> </li> <li> <p>A <code>DataPreprocessor</code> takes in a <code>Scenario</code> and produces a list of <code>Instance</code>s.   Each <code>Instance</code> is given a unique ID. The set of <code>Instance</code>s is augmented   according to <code>DataAugmenterSpec</code>.</p> </li> <li> <p>An <code>Adapter</code> (given by an <code>AdaptationSpec</code>) takes a list of <code>Instance</code>s and   adapts it to a set of <code>Request</code>s to the API (e.g., the model, temperature,   number of in-context training examples).  Formally, the output   is a <code>ScenarioState</code> containing a set of <code>RequestState</code>s, where each   <code>RequestState</code> consists of a <code>Request</code> and any metadata used to track the   role of this <code>Request</code> (e.g., the relevant <code>Instance</code> and <code>Reference</code>).</p> </li> <li> <p>An <code>Executor</code> (given by an <code>ExecutionSpec</code>) executes each <code>Request</code> in the   <code>RequestState</code> to produce a <code>RequestResult</code> for each one; everything is   encapsulated in a <code>ScenarioState</code>.</p> </li> <li> <p>A <code>Metric</code> (given by a <code>MetricSpec</code>) takes a <code>ScenarioState</code> containing   <code>RequestResults</code>s and produces a set of <code>Stat</code>s (e.g., accuracy, accuracy@5,   toxicity, bias, etc.).</p> </li> <li> <p>A <code>Runner</code> is the top-level controller that runs the above steps and is   driven by a set of <code>RunSpec</code>s.</p> </li> </ul> <p>There are three types of classes:</p> <ul> <li>Specifications (e.g., <code>AdapterSpec</code>, <code>ExecutionSpec</code>, <code>RunSpec</code>):   specified manually by the user.  Note that <code>Scenario</code> and <code>Metric</code> are   subclassed, so they are constructed by <code>ObjectSpec</code>, which specifies the   subclass name and a free-form dictionary of arguments.</li> <li>States (e.g., <code>Instance</code>, <code>ScenarioState</code>, <code>Request</code>, <code>RequestResult</code>): these   are automatically generated and can be serialized.</li> <li>Controllers (e.g., <code>Scenario</code>, <code>Adapter</code>, <code>Executor</code>, <code>Metric</code>, <code>Runner</code>):   these have the bulk of the code and should not be serialized.</li> </ul>"},{"location":"code/#adding-new-scenarios","title":"Adding new scenarios","text":"<p>In order to implement new scenarios:</p> <ol> <li>Create a new Python file in the <code>scenarios</code> folder.</li> <li>Within the scenario file, create a <code>Scenario</code> class, e.g. <code>YourScenario</code>.</li> <li><code>YourScenario</code> should implement <code>get_instances</code>, a method that downloads the     dataset files if they don't already exist and returns a list of <code>Instance</code>s.     Each <code>Instance</code> must have a list of (potentially one)    <code>Reference</code> answers: a correct answer may be indicated with a <code>CORRECT_TAG</code> in     a <code>Reference</code> instance's <code>tags</code> argument. In addition, you     must specify the <code>split</code> of the <code>Instance</code> as one of <code>TRAIN_SPLIT</code>,    <code>VALID_SPLIT</code>, or <code>TEST_SPLIT</code> constants as in <code>scenario.py</code>.</li> <li>For <code>Scenario</code>s with datasets that cannot be publicly shared, place a copy of the       dataset at path <code>restricted/&lt;Name of the Scenario&gt;</code> and read from that path.       See <code>NewsQAScenario</code> and <code>ICEScenario</code> for some examples.</li> <li>Note that you need not enumerate every possible correct answer (nor must    there even necessarily be a correct answer). </li> <li>Make sure to document your scenario well with a clear docstring. </li> <li>In addition, specify its <code>name</code>, <code>description</code>, and <code>tags</code>.</li> <li>Identify the appropriate metric for your task in one of the <code>*_metrics.py</code> files.    If the metric you'd like to use does not exist, follow the directions in Adding new metrics.    Many will be in <code>basic_metrics.py</code>.</li> <li>Define a function in <code>run_specs.py</code> annotated with <code>run_spec_function</code> to:</li> <li>Construct a <code>ScenarioSpec</code>     for your scenario using a class name corresponding to the Python path of     the class (e.g. <code>helm.benchmark.scenarios.your_scenario.YourScenario</code>) and any     arguments which must be passed as a dictionary of <code>args</code>.</li> <li>Construct an <code>AdapterSpec</code> for your    scenario specifying the type of language model generation which must be     performed for the task.</li> <li>Construct one or more <code>MetricSpec</code>    objects for your task, specifying the classname with the Python path of    the object, with the same arguments as the <code>ScenarioSpec</code> constructor.</li> <li>Construct and return <code>RunSpec</code> object, with a     <code>name</code> corresponding to the scenario name and any patterns to match in     curly braces, a <code>scenario_spec</code>, an <code>adapter_spec</code>, <code>metric_specs</code>,     and <code>groups</code>. </li> <li>Attempt to run your task with    <code>venv/bin/helm-run -r yourscenarioname:arg=value</code> where     <code>yourscenarioname</code> matches the <code>name</code> specified in YourScenario</li> <li>Update <code>src/helm/benchmark/static/contamination.yaml</code> with models that were trained on your scenario (i.e. contaminated).</li> <li>Add a schema to <code>src/helm/benchmark/static/schema.yaml</code> and add the scenario to <code>subgroups</code> as needed.</li> </ol>"},{"location":"code/#adding-new-metrics","title":"Adding new metrics","text":"<p>To add a new metric, first determine if your metric is generic and likely to be widely used, or specific to your task.</p> <ul> <li>For generic metrics:</li> <li>Add a method to <code>basic_metrics.py</code> which takes two arguments: the <code>gold</code> answer and the model's <code>pred</code>iction.</li> <li>Add your method to the <code>metric_fn_mapping</code> lookup.</li> <li>For task specific metrics:</li> <li>Create a new <code>yourtask_metrics.py</code> file for class <code>YourTaskMetric</code>     which inherits from <code>Metric</code> in <code>metric.py</code>.</li> <li>Define methods <code>__init__</code> and <code>evaluate_generation</code> returning a list of <code>Stat</code> objects.</li> </ul> <p>Your metric is responsible for producing <code>Stat</code> objects:</p> <ul> <li>Each <code>Stat</code> should correspond to a distinct aggregate measurement over the generated examples.     Some may have one metric (e.g. accuracy), while others may quantify multiple aspects    (e.g. multiple distance metrics). </li> <li>For each <code>value</code> generated for a <code>Stat</code>, add it to <code>yourstat</code> using <code>yourstat.add(value)</code>.     Usually, there will only be one value for each <code>Stat</code>, but multiple can be used, e.g. to show variance.</li> </ul>"},{"location":"code/#data-augmentations","title":"Data augmentations","text":"<p>To apply data augmentation, create a <code>DataAugmenterSpec</code> with a list of <code>PerturbationSpec</code>s and pass it into <code>RunSpec</code>. The following is an example:</p> <pre><code>    data_augmenter_spec = DataAugmenterSpec(\n        perturbation_specs=[\n            PerturbationSpec(\n                class_name=\"helm.benchmark.augmentations.perturbation.ExtraSpacePerturbation\",\n                args={\"num_spaces\": 5},\n            )\n        ],\n        should_perturb_references=False,\n        should_augment_train_instances=False,\n        should_include_original_train=False,\n        should_augment_eval_instances=True,\n        should_include_original_eval=True,\n    )\n    run_spec = RunSpec(\n        ...\n        data_augmenter_spec=data_augmenter_spec\n    )\n</code></pre> <p>In the example above, the <code>DataPreprocessor</code> will augment the set of evaluation instances by perturbing the original set of instances with the <code>ExtraSpacePerturbation</code>, where spaces in the text are replaced with <code>num_spaces</code> number of spaces.</p> <p>We currently only support applying a single perturbation to an instance instead of chaining multiple perturbations and applying it onto a single instance.</p>"},{"location":"code/#adding-a-new-perturbation","title":"Adding a new perturbation","text":"<ol> <li>To add a new perturbation to the framework, create a new file at <code>src/helm/benchmark/augmentations</code> with the name    <code>&lt;Name of perturbation&gt;_perturbation.py</code> e.g., <code>typo_perturbation.py</code>. Inside the file, create a new class    (name it <code>&lt;Name of the perturbation&gt;Perturbation</code> e.g., <code>TypoPerturbation</code>)    that extends the abstract class <code>Perturbation</code> and implement the <code>perturb</code> method which    takes in text and outputs the perturbed text.</li> <li>Add a test for the new perturbation in <code>test_perturbation.py</code>.</li> </ol>"},{"location":"code/#supporting-new-hugging-face-tokenizers","title":"Supporting new Hugging Face tokenizers","text":"<ol> <li>Give the tokenizer a name. Use the same name that's used in Hugging Face (e.g., \"EleutherAI/gpt-j-6B\").</li> <li>In <code>HuggingFaceTokenizers</code>, we load and cache tokenizers in memory. Add logic to handle    the tokenizer in the <code>load_tokenizer</code> method.</li> <li>Add a test in <code>test_huggingface_tokenizer.py</code> to make sure we can load the tokenizer from Hugging Face.</li> <li>Add a new class <code>&lt;Name of tokenizer&gt;WindowService</code> in file <code>&lt;Name of tokenizer&gt;_window_service.py</code>.    Follow what we did for <code>GPTJWindowService</code>.</li> <li>Import the new <code>WindowService</code> and map the model(s) to it in <code>WindowServiceFactory</code>.</li> </ol>"},{"location":"code/#heim-text-to-image-evaluation","title":"HEIM (text-to-image evaluation)","text":"<p>The overall code structure is the same as HELM's.</p> <p>When adding new scenarios and metrics for image generation, place the Python files under the <code>image_generation</code> package  (e.g., <code>src/helm/benchmark/scenarios/image_generation</code>).</p>"},{"location":"credentials/","title":"Credentials","text":""},{"location":"credentials/#credentials-file","title":"Credentials file","text":"<p>You should create a <code>credentials.conf</code> file in your local configuration folder, which is <code>./prod_env/</code> by default, unless you have overridden it using the <code>--local-path</code> flag to <code>helm-run</code>. This file should be in HOCON format. Example:</p> <pre><code>platformOneApiKey: sk-abcdefgh\nplatformTneApiKey: sk-ijklmnop\n</code></pre> <p>Here are the keys that must be set for to access these platforms:</p> <ul> <li>AI21: <code>ai21ApiKey</code></li> <li>Aleph Alpha: <code>AlephAlphaApiKey</code></li> <li>Anthropic: <code>anthropicApiKey</code></li> <li>Cohere: <code>cohereApiKey</code></li> <li>Google: <code>googleProjectId</code>, <code>googleLocation</code>, also see Additional Setup below</li> <li>GooseAI: <code>gooseApiKey</code></li> <li>Hugging Face Hub: None, but see Additional Setup below</li> <li>Mistral AI: <code>mistralaiApiKey</code></li> <li>OpenAI: <code>openaiApiKey</code>, <code>openApiOrgId</code></li> <li>Perspective: <code>perspectiveApiKey</code></li> <li>Writer: <code>writerApiKey</code></li> </ul>"},{"location":"credentials/#additional-setup","title":"Additional setup","text":""},{"location":"credentials/#google","title":"Google","text":"<p>You will need to install the Google Cloud CLI. Then, as the user that will be running <code>helm-run</code>, run:</p> <pre><code>gcloud auth application-default login\ngcloud auth application-default set-quota-project 123456789012\n</code></pre> <p>Replace <code>123456789012</code> with your actual numeric project ID.</p>"},{"location":"credentials/#hugging-face-hub","title":"Hugging Face Hub","text":"<p>If you are attempting to access models that are private, restricted, or require signing an agreement (e.g. Llama 2) through Hugging Face, you need to be authenticated to Hugging Face through the CLI. As the user that will be running <code>helm-run</code>, run:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Refer to Hugging Face's documentation for more information.</p>"},{"location":"design_principles/","title":"Legal-10 Design Principles","text":""},{"location":"design_principles/#overview","title":"Overview","text":"<p>Legal-10 is a skill-based benchmark for evaluating legal AI systems. This document describes the design principles that guide its development.</p>"},{"location":"design_principles/#design-principles","title":"Design Principles","text":""},{"location":"design_principles/#1-objectivity","title":"1. Objectivity","text":"<p>Fixed Specification - Each version has a fixed specification - Changes to the specification require a version increment - Evaluations specify which version was used</p> <p>Complete Evaluation - All 10 skills must be evaluated - No partial scores or selective omission - If a skill cannot be evaluated, this is explicitly noted</p> <p>Versioning - Version numbers follow semantic versioning - Breaking changes increment major version - Specification changes are documented in changelog</p>"},{"location":"design_principles/#2-transparency","title":"2. Transparency","text":"<p>Open Source - Evaluation harness is open source - Datasets are publicly accessible or documented - Scoring implementations are visible - Run specifications are published</p> <p>Reproducibility - Evaluations include SHA-256 hashes - Dataset versions are pinned - Full run bundles are preserved - Evaluation conditions are documented</p> <p>Public Logging - Submissions are logged with timestamps - Historical performance is preserved - Append-only log prevents silent deletions</p>"},{"location":"design_principles/#3-fairness","title":"3. Fairness","text":"<p>Standardized Evaluation - All models use identical evaluation protocol - Same prompts, same datasets, same metrics - No model-specific optimizations in core harness</p> <p>Modality-Matched Testing - Closed-book tasks (S7-S10): No retrieval allowed - RAG tasks (S3-S6): Retrieval capability tested - Agentic tasks (S1-S2): Multi-step reasoning evaluated - Testing matches intended use patterns</p> <p>Comparable Baselines - Open-weight models use same harness as API models - Local deployments use same evaluation as hosted services</p>"},{"location":"design_principles/#4-gaming-resistance","title":"4. Gaming Resistance","text":"<p>No Selective Omission - Cannot cherry-pick favorable tasks - All 10 skills required for score reporting - Partial evaluation results are not published</p> <p>Granular Reporting - Skill-level scores reported individually - Aggregate score does not hide weak performance - Per-instance results available for analysis</p> <p>Multiple Evaluation Axes - Tests across different modalities (CB/RAG/AG) - Multiple metrics per skill where appropriate - Cannot optimize for single metric</p>"},{"location":"design_principles/#professional-standards-grounding","title":"Professional Standards Grounding","text":"<p>Legal-10 skills map to established professional competency frameworks:</p> <p>MacCrate Report (ABA, 1992) - Foundational lawyering skills framework - Research planning, fact-finding, legal analysis</p> <p>AALL Principles (2013) - Legal research competency standards - Authority identification and validation</p> <p>Shultz &amp; Zedeck (2011) - Empirically-derived lawyer effectiveness factors - Strategic planning, practical judgment</p> <p>These frameworks represent 30+ years of validated professional standards.</p>"},{"location":"design_principles/#skill-definitions","title":"Skill Definitions","text":"<p>Legal-10 evaluates 10 distinct skills:</p> Skill Name Modality S1 Research Planning Agentic (AG) S2 Strategic Stopping Agentic (AG) S3 Known Authority RAG S4 Unknown Authority RAG S5 Validate Authority RAG S6 Fact Extraction RAG S7 Distinguish Cases Closed-Book (CB) S8 Synthesize Results Closed-Book (CB) S9 Citation Integrity Closed-Book (CB) S10 Copyright Compliance Closed-Book (CB) <p>Each skill is tested using the modality that most closely matches how human lawyers perform that skill in practice.</p>"},{"location":"design_principles/#scoreboard-policy","title":"Scoreboard Policy","text":""},{"location":"design_principles/#participation","title":"Participation","text":"<ul> <li>Evaluation is open to any model</li> <li>Participation is voluntary</li> <li>Vendors may submit their own evaluations</li> <li>Community members may evaluate any accessible model</li> </ul>"},{"location":"design_principles/#evaluated-list","title":"\"Evaluated\" List","text":"<p>Systems that have completed full evaluation (all 10 skills) are listed with: - Model identifier - Evaluation date - Version tested - Link to run bundle</p>"},{"location":"design_principles/#not-yet-evaluated-list","title":"\"Not Yet Evaluated\" List","text":"<p>Systems that have not been evaluated are listed neutrally with status tags: - \"No public endpoint\" - Model not publicly accessible - \"Access restricted\" - Requires special access or licensing - \"Awaiting community submission\" - Public but not yet evaluated</p> <p>This ensures visibility of coverage gaps.</p>"},{"location":"design_principles/#what-legal-10-measures","title":"What Legal-10 Measures","text":"<p>Behavioral Performance - How models respond to standardized legal tasks - Performance across 10 professional skill categories - Comparative capabilities under identical conditions</p> <p>Skill-Specific Capabilities - Research planning quality - Authority identification accuracy - Citation integrity - Fact extraction precision - Legal reasoning capability</p>"},{"location":"design_principles/#what-legal-10-does-not-measure","title":"What Legal-10 Does Not Measure","text":"<p>Architectural Parameters - Model size, precision, or quantization - Training data composition or provenance - Deployment infrastructure - Version or update history</p> <p>Production Reliability - Real-world usage patterns - Domain-specific edge cases - Integration with specific workflows - Long-term consistency</p> <p>Fitness for Specific Use Cases - Deployment safety for particular applications - Compliance with jurisdiction-specific requirements - Appropriateness for high-stakes decisions</p>"},{"location":"design_principles/#limitations","title":"Limitations","text":"<p>Legal-10 has known limitations:</p> <p>Vendor Architecture Opacity - Cannot verify model parameters vendors claim - Cannot detect silent model version changes - Cannot inspect training data - Cannot confirm deployment configuration</p> <p>Test Coverage - Tests standardized workflows, not all possible use patterns - Cannot evaluate vendor-specific features - Cannot test every prompt engineering approach - Evaluates API access, not internal capabilities</p> <p>Deployment Context - Benchmark performance \u2260 production performance - Controlled evaluation \u2260 real-world reliability - Standardized prompts \u2260 expert usage</p> <p>Professional Judgment - Scores inform decisions, do not make them - Domain expertise required for fitness assessment - Ethical obligations remain with deployers</p>"},{"location":"design_principles/#technical-implementation","title":"Technical Implementation","text":""},{"location":"design_principles/#framework","title":"Framework","text":"<p>Legal-10 implements the evaluation architecture pioneered by Stanford CRFM's HELM (Holistic Evaluation of Language Models):</p> <ul> <li>Unified model interface for open and closed models</li> <li>Standardized adapters for consistent prompting</li> <li>Reproducible evaluation with auditable run bundles</li> <li>Open-source implementation</li> </ul>"},{"location":"design_principles/#dataset-sources","title":"Dataset Sources","text":"<p>Datasets are either: - Publicly available on HuggingFace with documented IDs - Downloadable from documented URLs with version hashes - Reproducible from published sources with clear methodology</p>"},{"location":"design_principles/#evaluation-process","title":"Evaluation Process","text":"<ol> <li>Model receives standardized prompt</li> <li>Response is generated under specified conditions</li> <li>Output is scored using defined metrics</li> <li>Results are aggregated across instances</li> <li>Skill-level and aggregate scores are reported</li> </ol>"},{"location":"design_principles/#governance","title":"Governance","text":"<p>Community Governance - Not controlled by any single vendor - Contributors receive co-author credit on publications - Community members have governance voting rights - Transparent decision-making process</p> <p>Change Process - Specification changes are proposed publicly - Community review period before adoption - Breaking changes require major version increment - All changes documented in changelog</p>"},{"location":"design_principles/#use-cases","title":"Use Cases","text":"<p>Legal-10 is designed to support:</p> <p>Comparative Evaluation - Fair comparison across different models - Standardized measurement of legal capabilities - Identification of relative strengths and weaknesses</p> <p>Quality Assessment - Skill-specific performance visibility - Detection of capability gaps - Baseline competency verification</p> <p>Informed Decision-Making - Input for deployment decisions (not substitute for testing) - Risk assessment for legal AI adoption - Vendor claim verification (within limitations)</p> <p>Research - Reproducible benchmarking for academic studies - Tracking of legal AI capability development - Identification of areas needing improvement</p>"},{"location":"design_principles/#what-users-should-know","title":"What Users Should Know","text":"<p>Legal-10 Provides: - Standardized comparison under identical conditions - Skill-level performance visibility - Grounding in professional standards - Open, auditable evaluation methodology</p> <p>Legal-10 Does Not Provide: - Certification of deployment safety - Guarantee of production reliability - Replacement for domain-specific testing - Verification of vendor architectural claims</p> <p>Appropriate Use: - As one input among many for deployment decisions - To identify areas requiring additional testing - To compare relative capabilities under controlled conditions - To assess baseline competency across professional skills</p> <p>Inappropriate Use: - As sole basis for high-stakes deployment - As substitute for professional judgment - As certification of legal compliance - As verification of vendor claims about architecture</p>"},{"location":"design_principles/#relationship-to-other-benchmarks","title":"Relationship to Other Benchmarks","text":"<p>Legal-10 differs from some existing AI benchmarks in specific ways:</p> <p>Participation Model - Legal-10: Open evaluation, anyone can run - Some benchmarks: Vendor opt-in required</p> <p>Result Reporting - Legal-10: Append-only log, historical performance preserved - Some benchmarks: Results may be withdrawn or updated</p> <p>Task Selection - Legal-10: All 10 skills required - Some benchmarks: May allow selective task participation</p> <p>Methodology Transparency - Legal-10: Open-source harness, public datasets - Some benchmarks: May use proprietary test sets</p> <p>These are factual differences, not claims of superiority.</p>"},{"location":"design_principles/#future-development","title":"Future Development","text":"<p>Legal-10 is designed to evolve:</p> <p>Potential Extensions - Multilingual legal reasoning (in development) - Additional professional skills - Cross-jurisdictional evaluation - Specialized legal domain testing</p> <p>Community Input - Suggestions for new skills - Dataset contributions - Metric improvements - Framework enhancements</p> <p>Version Progression - Regular updates to datasets - Metric refinements based on empirical analysis - New skills as legal AI capabilities expand</p>"},{"location":"design_principles/#citation","title":"Citation","text":"<p>When referencing Legal-10, please include: - Benchmark name and version - Evaluation date - Link to this documentation - Dataset sources used</p> <p>Legal-10 is an open benchmark. We welcome scrutiny, replication, and improvement suggestions.</p>"},{"location":"developer_adding_new_models/","title":"Adding New Clients","text":"<p>Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution!</p>"},{"location":"developer_adding_new_models/#overview-of-the-process","title":"Overview of the process","text":"<p>To add a new model you need to define 3 objects: * a <code>ModelMetadata</code> objects that defines properties of your model (name, metadata, capabilities, ...). * one or several <code>ModelDeployment</code> which defines how to query a model (mainly by providing a <code>Client</code>, a <code>WindowService</code> and a <code>Tokenizer</code>). You can define several deployments for a single model (<code>local/your-model</code>, <code>huggingface/your-model</code>, <code>together/your-model</code>, ...). * a <code>TokenizerConfig</code> which defines how to build the <code>Tokenizer</code> (mainly by providing a <code>TokenizerSpec</code>).</p> <p>In some cases you might have to define additionally: * a <code>Client</code> if your query method differ from any clients we have implemented. This will be then referenced in the <code>ModelDeployment</code>. We recommend checking <code>HTTPModelClient</code> and <code>HuggingFaceClient</code> which can be used in a lot of cases. If you identify the need for a new client, a good starting to point is to have a look at <code>SimpleClient</code>. * a <code>WindowService</code>. First have a look at <code>DefaultWindowService</code> to check if this is not enough for your use case. If you need you own <code>truncate_from_right</code> function, then you might need to create your own <code>WindowService</code>. In that case, a good starting point is to have a look at <code>YaLMWindowService</code>.</p>"},{"location":"developer_adding_new_models/#where-to-create-the-objects","title":"Where to create the objects","text":"<p>There are two cases: private models that should only be accessible to you and models not yet supported by HELM but that would benefit everyone if added.</p> <p>In the first case, you should create the files <code>model_deployments.yaml</code>, <code>model_metadata.yaml</code> and <code>tokenizer_configs.yaml</code> in <code>prod_env/</code> (A folder that you should create at the root of the repo if not already done). HELM will automatically registed any model defined in these files without any change in the code while ignoring them on Github which can be convenient for you. Then you can simply duplicate the corresponding files from <code>src/helm/config</code>, delete the models and add yours. Follow the next section for an example.</p> <p>In the second case, if you want to add a model to HELM, you can directly do it in <code>src/helm/config</code>. You can then open a Pull Request on Github to share the model. When you do, make sure to: * Include any link justifying the metadata used in <code>ModelMetadata</code> such as the release data, number of parameters, capabilities and so on (you should not infer anything). * Check that you are respecting the format used in those files (<code>ModelMetadata</code> should be named as <code>&lt;CREATOR-ORGANIZATION&gt;/&lt;MODEL-NAME&gt;</code> and the <code>ModelDeployment</code> should be named as <code>&lt;HOST-ORGANIZATION&gt;/&lt;MODEL-NAME&gt;</code>, for example <code>ModelMetadata</code>: <code>openai/gpt2</code> and <code>ModelDeployment</code>: <code>huggingface/gpt2</code>). Add the appropriate comments and so on. * Run <code>helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=&lt;YOUR-DEPLOYMENT&gt;\" --suite v1 --max-eval-instances 10</code> and make sure that everything works. Include the logs from the terminal in your PR. * Not create unnecessary objects (<code>Client</code> <code>TokenizerCOnfig</code>, <code>WindowService</code>) and if you have to create one of these objects, document in your PR why you had to. Make them general enough so that they could be re-used by other models (especially the <code>Client</code>).</p>"},{"location":"developer_adding_new_models/#example","title":"Example","text":"<p>In <code>src/helm/config/model_metadata.yaml</code>: <pre><code># [...]\n\nmodels:\n\n  - name: simple/model1\n    [...]\n\n  # NEW MODEL STARTS HERE\n  - name: simple/tutorial\n    display_name: Tutorial Model\n    description: This is a simple model used in the tutorial.\n    creator_organization_name: Helm\n    access: open\n    release_date: 2023-01-01\n    tags: [TEXT_MODEL_TAG, FULL_FUNCTIONALITY_TEXT_MODEL_TAG]\n\n  [...]\n</code></pre></p> <p>In <code>src/helm/config/model_deployments.yaml</code>: <pre><code># [...]\n\nmodel_deployments:\n\n  - name: simple/model1\n    [...]\n\n  - name: simple/tutorial\n    model_name: simple/tutorial\n    tokenizer_name: simple/model1\n    max_sequence_length: 2048\n    client_spec:\n      class_name: \"helm.clients.simple_client.SimpleClient\"\n      args: {}\n    window_service_spec:\n      class_name: \"helm.benchmark.window_services.openai_window_service.OpenAIWindowService\"\n      args: {}\n\n  [...]\n</code></pre></p> <p>We won't be adding any <code>TokenizerConfig</code> here as we are reusing <code>simple/model1</code>. This shows a good practice when adding a new model, always check if the correct tokenizer does not already exists.</p> <p>You should now be able to run <code>helm-run --run-entries \"mmlu:subject=anatomy,model_deployment=simple/tutorial\" --suite v1 --max-eval-instances 10</code> without any error.</p>"},{"location":"developer_guide/","title":"Developer Guide","text":"<p>This guide provides documentation for developers extending and contributing to Legal-10 Benchmark.</p>"},{"location":"developer_guide/#getting-started","title":"Getting Started","text":"<ul> <li>Developer Setup - Set up your development environment, testing, and Git workflow</li> </ul>"},{"location":"developer_guide/#contributing","title":"Contributing","text":"<ul> <li>Adding New Models - How to add model metadata, deployments, and clients</li> <li>Editing Documentation - MkDocs setup and documentation contribution process</li> </ul>"},{"location":"developer_setup/","title":"Developer Setup","text":""},{"location":"developer_setup/#check-your-system-python-version","title":"Check your system Python version","text":"<p>Check your system verison of Python by running:</p> <pre><code>python --version\n</code></pre> <p>If your version of Python is older than 3.10, you must use either Conda or pyenv to install a version of Python &gt;=3.10 when setting up your virtual environment.</p>"},{"location":"developer_setup/#set-up-the-python-virtual-environment","title":"Set up the Python virtual environment","text":"<p>First, create a Python virtual environment with Python version &gt;= 3.10 and activate it.</p> <p>Using Virtualenv (requires system Python version &gt;=3.10):</p> <pre><code># Create a virtual environment.\n# Only run this the first time.\npython3 -m pip install virtualenv\npython3 -m virtualenv -p python3 venv\n\n# Activate the virtual environment.\n# Run this every time you open your shell.\nsource venv/bin/activate\n</code></pre> <p>Using Conda:</p> <pre><code># Create a virtual environment.\n# Only run this the first time.\nconda create -n crfm-helm python=3.10 pip\n\n# Activate the virtual environment.\n# Run this every time you open your shell.\nconda activate crfm-helm\n</code></pre> <p>Using pyenv and pyenv-virtualenv:</p> <pre><code># Create a virtual environment.\n# Only run this the first time.\npyenv virtualenv 3.10 crfm-helm\n\n# Activate the virtual environment.\n# Run this every time you open your shell.\npyenv activate crfm-helm\n</code></pre>"},{"location":"developer_setup/#install-python-dependencies","title":"Install Python dependencies","text":"<p>To install any dependencies:</p> <pre><code>pip install --force-reinstall -e .[dev]\n</code></pre>"},{"location":"developer_setup/#run-python-tests","title":"Run Python tests","text":"<p>Currently, running all the unit tests takes about 10 minutes. To run all unit tests:</p> <pre><code>python -m pytest\n</code></pre> <p>Append <code>-vv</code> to output the full diff and results:</p> <pre><code>python -m pytest -vv\n</code></pre> <p>When modifying the Python code, you usually want to only run certain relevant tests. To run a specific test file, specify the file path as follows:</p> <pre><code>python -m pytest path/to/test_file.py -vv\n</code></pre>"},{"location":"developer_setup/#run-linter-and-type-checker","title":"Run linter and type-checker","text":"<p>You should always ensure that your code is linted and type-checked before creating a pull request. This is typically enforced by our git pre-commit hooks. Install the pre-commit hooks by running:</p> <pre><code>pre-commit install\n</code></pre> <p>This will automatically run the linter and type-checker whenever you run <code>git push</code> to push a branch. To skip running the linter and type checker when pushing a branch, use the <code>--no-verify</code> flag with <code>git push</code>.</p> <p>To run the linter and type-checker manually:</p> <pre><code>./pre-commit.sh\n</code></pre> <p>Alternatively, you can run only the linter or only the type checker separately:</p> <pre><code># Linters\nblack src scripts\nflake8 src scripts\n\n# Type checker\nmypy src scripts\n</code></pre>"},{"location":"developer_setup/#executing-helm-commands-with-local-modifications","title":"Executing helm commands with local modifications","text":"<p>The recommended way to execute <code>helm-run</code>, <code>helm-summarize</code>, <code>helm-server</code>, etc, with your local version of the repository is to do an editable install, using the following steps:</p> <ol> <li>Activate your virtual environment.</li> <li>Change directory to the repository root (contains pyproject.toml).</li> <li>Make sure you don't have an existing helm installation for that environment with <code>pip uninstall crfm-helm</code></li> <li>Run <code>pip install -e .</code></li> </ol> <p>Now calling <code>helm-run</code> while the environment is activated will read from your local source.</p>"},{"location":"developer_setup/#without-installing","title":"Without installing","text":"<p>If you have a compelling reason not to do an editable install, you can execute commands by:</p> <ol> <li>Change directory to <code>src</code></li> <li>Execute the module you want with a command like: <code>python -m helm.benchmark.run</code></li> </ol>"},{"location":"developer_setup/#checking-in-code","title":"Checking in code","text":"<p>The HELM repository does not allow direct modifications of the main branch. Instead, developers create a Pull Request which must then be approved by a different person before merging into main. Here is an example workflow:</p> <ol> <li><code>git checkout main</code> to start from the main branch.</li> <li><code>git pull origin main</code> to get up to date.</li> <li>Make whatever changes you'll like to group into a single review.</li> <li>Run tests.</li> <li>Make a new branch with <code>git checkout -b &lt;your-handle&gt;/&lt;change-identifier</code>. For example, <code>yifanmai/fix-optional-suggestions</code>.</li> <li>If you did NOT install the precommit, run the linter and type checker with <code>./pre-commit.sh</code></li> <li><code>git commit -a</code> to commit all you changes. If you want to ignore precommit warnings,  you can add <code>--no-verify</code>.</li> <li><code>git push origin &lt;your-handle&gt;/&lt;change-identifier&gt;</code> to upload to github.</li> <li>Loading any HELM github page should now prompt you about creating a new pull request. If not, you can also find your branch on the branches page to create one.</li> <li>Update the title and description as necessary, then create the pull request.</li> <li>Once the reviewer is satisfied, they can approve and either of you can then <code>Squash and Merge</code> the branch into main.</li> </ol>"},{"location":"editing_documentation/","title":"Editing Documentation","text":"<p>The documentation that you are reading now is an invaluable resource for newcomers and experienced users alike. Contributions to the documentation are very welcome.</p> <p>We currently use the MkDocs as our static site generator and ReadTheDocs as our web host.</p> <p>To edit the documentation, first clone the repository locally, then install HELM from the repository by following the Developer Setup instructions. After that, install the MkDocs dependencies by running the following from the root of the repository:</p> <pre><code>pip install -r docs/requirements.txt\n</code></pre> <p>You should now be able to run MkDocs from the root of the repository:</p> <pre><code>mkdocs serve\n</code></pre> <p>Then navigate to http://localhost:8000/ to view your locally-built documentation.</p> <p>The source Markdown files for the documentation are stored in the <code>docs/</code> folder. By default, MkDocs watches the source directories for changes and automatically re-renders the web pages when it detects changes.</p> <p>If you are creating a new page, you should add your page to the <code>nav</code> section in <code>mkdocs.yml</code>. This will add your page to the table of contents in the side menu.</p> <p>We make heavy use of plugins and macros for auto-generating documentation from code and docstrings. For more information, please refer to the documentation for these plugins e.g. mkdocs-macros, mkdocstrings and mkdocstrings-python.</p>"},{"location":"extension_multilingual/","title":"Multilingual","text":""},{"location":"framework/","title":"Evaluation Architecture","text":"<p>We open-source everything: datasets, harness, scoring, and operational tooling so anyone can reproduce the results.</p> <p>Legal-10 is built on top of HELM (Holistic Evaluation of Language Models) developed by Stanfard CRFM which provides a unified interface for evaluating both open-weight and closed API models. </p> <ul> <li>unified interface for evaluating both open-weight and closed API models</li> <li>open-transparency principle</li> <li>expandable, holistic, grounded in science</li> </ul> <p>This architecture is an industry specific extension. Our goal is to design benchmarks for th</p> <p>L10 supplies the legal skill taxonomy, datasets, and scoring rubrics grounded in professional standards. The underlying architecture operates the way we believe legal AI evaluation should work: open, inspectable, reproducible, and resistant to selective reporting.</p>"},{"location":"good_first_issues/","title":"Good First Issues","text":""},{"location":"huggingface_models/","title":"Hugging Face Model Hub Integration","text":"<p>HELM can be used to evaluate <code>AutoModelForCausalLM</code> models (e.g. <code>BioMedLM</code>) on Hugging Face Model Hub or local disk. Note that only <code>AutoModelForCausalLM</code> models are supported; other classes such as <code>AutoModelForSeq2SeqLM</code> may be supported in the future.</p>"},{"location":"huggingface_models/#using-model_deploymentsyaml","title":"Using <code>model_deployments.yaml</code>","text":"<p>You can add Hugging Face models using the method discussed in Adding New Models. This can be used for both models on Hugging Face Hub and local disk. Please refer to that page for instructions for how to do so.</p>"},{"location":"huggingface_models/#using-command-line-flags","title":"Using command-line flags","text":"<p>In some cases, you can use command-line flags with <code>helm-run</code> to evaluating Hugging Face models. This provides a more convenient way to use Hugging Face models that does not require configuration files.</p> <p>To use <code>AutoModelForCausalLM</code> models from Hugging Face Model Hub, add the Hugging Face model IDs to the <code>--enable-huggingface-models</code> flags to <code>helm-run</code>. This will make the corresponding Hugging Face models available to use in your run spec descriptions. In the run spec description, use the Hugging Face model ID as the model name.</p> <p>To use a revision of a model other than the default main revision, append a <code>@</code> followed by the revision name to the model ID passed to the <code>--enable-huggingface-models</code> flag.</p> <p>Current restrictions with command-line flags:</p> <ul> <li>Models without a namespace are not supported (e.g. <code>bert-base-uncased</code>).</li> <li>The model must have <code>model_max_length</code> set in the tokenizer configuration.</li> </ul> <p>Example model on Hugging Face Hub:</p> <pre><code># Run boolq on stanford-crfm/BioMedLM at the default main revision\nhelm-run \\\n    --run-entries boolq:model=stanford-crfm/BioMedLM \\\n    --enable-huggingface-models stanford-crfm/BioMedLM \\\n    --suite v1 \\\n    --max-eval-instances 10\n\n# Run boolq on stanford-crfm/BioMedLM at revision main\nhelm-run \\\n    --run-entries boolq:model=stanford-crfm/BioMedLM@main \\\n    --enable-huggingface-models stanford-crfm/BioMedLM@main \\\n    --suite v1 \\\n    --max-eval-instances 10\n</code></pre> <p>Example model on local disk:</p> <pre><code># Run boolq on stanford-crfm/BioMedLM at the default main revision\nhelm-run \\\n    --run-entries boolq:model=your-org/your-model \\\n    --enable-local-huggingface-models path/to/your-org/your-model \\\n    --suite v1 \\\n    --max-eval-instances 10\n</code></pre>"},{"location":"importing_custom_modules/","title":"Importing Custom Modules","text":"<p>HELM is a modular framework with a plug-in architecture. You can write your own implementation for a client, tokenizer, scenario or metric and use them in HELM with HELM installed as a library, without needing to modify HELM itself.</p> <p>The main way for you to use your code in HELM is to write a custom Python class that is a subclass of <code>Client</code>, <code>Tokenizer</code>, <code>Scenario</code> or <code>Metric</code> in a Python module. You can then specify a <code>ClientSpec</code>, <code>TokenizerSpec</code>, <code>ScenarioSpec</code> or <code>MetricSpec</code> (which are all classes of <code>ObjectSpec</code>) where the <code>class_name</code> is the name of your custom Python class.</p> <p>However, HELM will only be able to use custom classes that can be imported by Python. Depending on your setup, you may need to do additional steps.</p>"},{"location":"importing_custom_modules/#add-the-current-working-directory-to-pythonpath","title":"Add the current working directory to PYTHONPATH","text":"<p>If the custom classes live in a Python module under the current working directory, you should modify <code>PYTHONPATH</code> to make that Python module importable.</p> <p>This is required because Python does not add the current working directory to the Python module search path running when using command line comments / Python entry points such as <code>helm-run</code>. See Python's documentation for more details.</p> <p>For example, suppose you implemented a custom <code>Client</code> subclass named <code>MyClient</code> in the <code>my_client.py</code> file under your current working directory, and you have a <code>ClientSpec</code> specifying the <code>class_name</code> as <code>my_client.MyClient</code>.</p> <p>To make your file importable by Python, you have to add <code>.</code> to your <code>PYTHONPATH</code> so that Python will search in your current working directory for your custom Python modules.</p> <p>In Bash, you can do this by running <code>export PYTHONPATH=\".:$PYTHONPATH\"</code> before running <code>helm-run</code>, or by prefixing <code>helm-run</code> with <code>PYTHONPATH=\".:$PYTHONPATH</code>.</p>"},{"location":"importing_custom_modules/#put-your-custom-classes-in-a-python-package","title":"Put your custom classes in a Python package","text":"<p>If your custom classes are located in a Python package, you can simply install your package (optionally in editable mode) and it will automatically be importable by Python. Be sure to install your Python package in the same Python environment as HELM.</p>"},{"location":"importing_custom_modules/#write-a-python-wrapper-script","title":"Write a Python wrapper script","text":"<p>If you are using a Python wrapper script that calls <code>helm.benchmark.run.run_benchmark()</code> instead of using <code>helm-run</code>, Python will automatically add the directory containing that script to the Python module search path. If your custom classes live in a Python module under that directory, they will automatically be importable by Python. See Python's documentation for more details.</p> <p>For example, suppose you implemented a custom <code>Client</code> subclass named <code>MyClient</code> in the <code>my_client.py</code> file under your current working directory, and you have a <code>ClientSpec</code> specifying the <code>class_name</code> as <code>my_client.MyClient</code>. Suppose you added a script called <code>run_helm.py</code> that calls <code>helm.benchmark.run.run_benchmark()</code> directly. When run using <code>python run_helm.py</code>, HELM will be able to import your modules without any additional changes.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#create-a-virtual-environment","title":"Create a virtual environment","text":"<p>It is recommended to install HELM into a virtual environment with Python version &gt;=3.10 to avoid dependency conflicts. HELM requires Python &gt;=3.10. To create, a Python virtual environment and activate it, follow the instructions below.</p> <p>Using Virtualenv:</p> <pre><code># Create a virtual environment.\n# Only run this the first time.\npython3 -m pip install virtualenv\npython3 -m virtualenv -p python3.10 helm-venv\n\n# Activate the virtual environment.\nsource helm-venv/bin/activate\n</code></pre> <p>Using Anaconda:</p> <pre><code># Create a virtual environment.\n# Only run this the first time.\nconda create -n crfm-helm python=3.10 pip\n\n# Activate the virtual environment.\nconda activate crfm-helm\n</code></pre>"},{"location":"installation/#install-helm","title":"Install HELM","text":"<p>Within this virtual environment, run:</p> <pre><code>pip install crfm-helm\n</code></pre>"},{"location":"metrics/","title":"Metrics","text":"<p>::: helm.benchmark.metrics     options:         filters: [\"^(?!test_).+metrics$\", \"Metric$\", \"^evaluate\"]         show_submodules: true         show_root_heading: false         show_root_toc_entry: false         members_order: alphabetical</p>"},{"location":"mission_statement/","title":"Mission Statement","text":"<p>Legal AI is already being used to make decisions that affect liberty, employment, housing, immigration status, family stability, and access to counsel. Today's evaluation ecosystem is fragmented, vendor-permissioned, and often optimized for optics rather than end-user protection.</p> <p>Legal-10 exists to change that.</p> <p>Legal-10 is an open, skill-based benchmark suite designed to make legal AI performance inspectable, comparable, and auditable\u2014across three delivery modalities: Closed-Book (CB), Retrieval-Augmented Generation (RAG), and Agentic Workflows (AG).</p> <p>We open-source everything: datasets, harness, scoring, and operational tooling. The only exception is the actual administered test data, which remains private to prevent contamination and prior knowledge.</p>"},{"location":"mission_statement/#design-principles","title":"Design Principles","text":""},{"location":"mission_statement/#objectivity","title":"Objectivity","text":"<ul> <li>Fixed spec per version \u2014 changes bump the version</li> <li>No selective omission or quiet withdrawal \u2014 full eval split or no score</li> </ul>"},{"location":"mission_statement/#transparency","title":"Transparency","text":"<ul> <li>Open-source harness \u2014 datasets, scoring, tools</li> <li>Auditable run bundles \u2014 SHA-256 hashes, signed manifests</li> <li>Public append-only log \u2014 all submissions preserved</li> </ul>"},{"location":"mission_statement/#fairness","title":"Fairness","text":"<ul> <li>Comparable baselines \u2014 open and closed models use identical harness</li> <li>Modality-matched testing \u2014 CB vs RAG vs AG reflects real lawyer workflows</li> </ul>"},{"location":"mission_statement/#gaming-resistant","title":"Gaming-Resistant","text":"<ul> <li>No selective omission \u2192 must run all 10 skills</li> <li>Skill-level reporting \u2192 can't hide weak skills in aggregate</li> <li>Visible failure modes \u2192 can't hide how it failed</li> </ul>"},{"location":"mission_statement/#community-driven","title":"Community-Driven","text":"<p>Legal-10 is community-governed, not vendor-controlled. We ask for your help in pushing this benchmark initiative. We need a fair and comprehensive benchmark that end users can rely on to better understand the quality implications of a product they are using or considering.</p> <p>Benefit </p> <p>Co-author credit on papers, governance voting rights, contributor recognition</p>"},{"location":"models/","title":"Models","text":""},{"location":"models/#text-models","title":"Text Models","text":"<p>{% for tag in [\"TEXT_MODEL_TAG\", \"CODE_MODEL_TAG\"] %}</p> <p>{% for organization, models in models_by_organization_with_tag(tag).items() %}</p>"},{"location":"models/#organization","title":"{{ organization }}","text":"<p>{% for model in models %}</p>"},{"location":"models/#modeldisplay_name-modelname","title":"{{ model.display_name }} \u2014 <code>{{ model.name }}</code>","text":"<p>{{ model.description }}</p> <p>{% endfor %} {% endfor %} {% endfor %}</p>"},{"location":"models/#vision-language-models","title":"Vision-Language Models","text":"<p>{% for organization, models in models_by_organization_with_tag(\"VISION_LANGUAGE_MODEL_TAG\").items() %}</p>"},{"location":"models/#organization_1","title":"{{ organization }}","text":"<p>{% for model in models %}</p>"},{"location":"models/#modeldisplay_name-modelname_1","title":"{{ model.display_name }} \u2014 <code>{{ model.name }}</code>","text":"<p>{{ model.description }}</p> <p>{% endfor %} {% endfor %}</p>"},{"location":"models/#text-to-image-models","title":"Text-to-image Models","text":"<p>{% for organization, models in models_by_organization_with_tag(\"TEXT_TO_IMAGE_MODEL_TAG\").items() %}</p>"},{"location":"models/#organization_2","title":"{{ organization }}","text":"<p>{% for model in models %}</p>"},{"location":"models/#modeldisplay_name-modelname_2","title":"{{ model.display_name }} \u2014 <code>{{ model.name }}</code>","text":"<p>{{ model.description }}</p> <p>{% endfor %} {% endfor %}</p>"},{"location":"models/#audio-language-models","title":"Audio-Language Models","text":"<p>{% for organization, models in models_by_organization_with_tag(\"AUDIO_LANGUAGE_MODEL_TAG\").items() %}</p>"},{"location":"models/#organization_3","title":"{{ organization }}","text":"<p>{% for model in models %}</p>"},{"location":"models/#modeldisplay_name-modelname_3","title":"{{ model.display_name }} \u2014 <code>{{ model.name }}</code>","text":"<p>{{ model.description }}</p> <p>{% endfor %} {% endfor %}</p>"},{"location":"perturbations/","title":"Perturbations","text":"<p>::: helm.benchmark.augmentations     options:         filters: [\"^(?!test_).+_perturbation$\", \".+Perturbation$\"]         show_submodules: true         show_root_heading: false         show_root_toc_entry: false         members_order: alphabetical</p>"},{"location":"proxy_server/","title":"Proxy Server","text":"<p>Warning \u2014 The document is stale. The information below may be outdated and incorrect. Please proceed with caution!</p> <p>We provide a single unified entry point into accessing large language models (e.g., GPT-3, Jurassic).  This provides both a web interface and a REST API.</p>"},{"location":"proxy_server/#using-for-most-people","title":"Using (for most people)","text":"<p>To use the web interface, go to https://crfm-models.stanford.edu.</p> <p>To use the REST API, see demo.py.</p>"},{"location":"proxy_server/#deploying-locally","title":"Deploying locally","text":"<p>Create <code>prod_env/credentials.conf</code> to contain the API keys for any language models you have access to.</p> <pre><code>{\n    openaiApiKey: \"...\",\n    ai21ApiKey: \"...\"\n}\n</code></pre> <p>To start a local server (go to <code>http://localhost:1959</code> to try it out):</p> <pre><code>venv/bin/crfm-proxy-server\n</code></pre> <p>When starting the server for the first time, the server will create an admin account  with the API key: <code>root</code>. If you're deploying the server to production, make sure to rotate the API key of the default admin account.</p>"},{"location":"proxy_server/#for-macos-developers","title":"For macOS developers","text":"<p>Bypass the added security that restricts multithreading by running:</p> <pre><code>OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES venv/bin/crfm-proxy-server\n</code></pre>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"run_entries/","title":"Run Entries","text":""},{"location":"run_entries/#using-run-entries","title":"Using run entries","text":"<p>Run entries are the main way of specifying to <code>helm-run</code> which evaluation runs to execute. For instance, in order to evaluate GPT-2 on MedQA, we would pass the following run entry to <code>helm-run</code>:</p> <pre><code>med_qa:model=openai/gpt2\n</code></pre> <p>There are two ways of passing the run entry to <code>helm-run</code>. We can use the <code>--run-entries</code> flag. For example:</p> <pre><code>helm-run --run-entries med_qa:model=openai/gpt2 --suite my-suite --max-eval-instances 10\n</code></pre> <p>Alternatively, we can put the run entry into a <code>run_entries.conf</code> file, and the pass that file to <code>helm-run</code> using the <code>--conf-file</code> flag. The <code>run_entries.conf</code> file is a run entry configuration file that conforms to the format documented here. For example:</p> <pre><code>helm-run --conf-file run_entries.conf --suite my-suite --max-eval-instances 10\n</code></pre>"},{"location":"run_entries/#constructing-run-entires","title":"Constructing run entires","text":""},{"location":"run_entries/#specifying-the-run-spec-function-name","title":"Specifying the run spec function name","text":"<p>The first part of the run entry before the <code>:</code> is the run spec function name. For example, in the run entry <code>med_qa:model=openai/gpt2</code>, the run spec function name is <code>med_qa</code>.</p> <p>A catalog of all run spec function names will be added to the documentation in the future. For now, the best way to find the run spec function name is to look through functions decorated with the <code>@run_spec_function()</code> in the Python modules <code>helm.benchmark.run_specs.*_run_specs</code>. The run spec function name is the decorator's parameter e.g. <code>@run_spec_function(\"med_qa\")</code> indicates a run spec function name of <code>med_qa</code>.</p> <p>Note: the run spec function name is frequently the same as the scenario name by convention, but this is not always the case. For instance, the <code>openbookqa</code> scenario has a run spec function that is named <code>commonsense</code>.</p>"},{"location":"run_entries/#run-entry-arguments","title":"Run entry arguments","text":"<p>The second part of the run entry after the <code>:</code> is a mapping of argument names to argument values. The string has the format <code>arg_name_1=arg_value_1,arg_name_2=arg_value_2</code> i.e. the name and value of each argument is joined by <code>=</code>, and the argument name-value pairs are joined by <code>,</code>. All argument values must be non-empty strings.</p> <p>The run entry arguments are used for two different things: run spec function arguments, and run expanders. For instance, in the example run entry <code>mmlu:subject=anatomy,model=openai/gpt2</code>, a run spec function argument is specified by <code>subject=anatomy</code>, and a run expander is specified by <code>model=openai/gpt2</code>.</p> <p>As in the above example, you can mix run expanders and run spec function arguments in a single run entry. If there is a name conflict between a run expander name and a run spec function argument name, the run expander has precedence. </p>"},{"location":"run_entries/#run-spec-function-arguments","title":"Run spec function arguments","text":"<p>Some run spec functions take in arguments. For instance, the MMLU run spec function <code>get_mmlu_spec()</code> takes in a <code>subject</code> argument. MMLU is a question answering scenario that covers multiple academic subjects. The <code>subject</code> argument specifies that the question set corresponding to that academic subject should be used for that evaluation run. For instance, to evaluate MMLU with the anatomy subject on GPT-2, the run entry should be:</p> <p><code>mmlu:subject=anatomy,model=openai/gpt2</code></p> <p>A catalog of all run spec functions' parameters will be added to the documentation in the future. For now, the best way to find the run spec function parameters would be to inspect the function definition in the Python modules <code>helm.benchmark.run_specs.*_run_specs</code> for the run spec function in question.</p>"},{"location":"run_entries/#run-expanders","title":"Run expanders","text":"<p>Run expanders are functions that modify how evaluation runs work. Concretely, a run expander operates on a configuration of an evaluation run (a <code>RunSpec</code>) and produces zero, one or multiple evaluation runs configurations with modified configurations (<code>RunSpecs</code>).</p> <p>Run expanders are an advanced topic. For most use cases, the only run expander that you will need to use is the <code>model</code> run expander. The <code>model=openai/gpt2</code> argument pair in the run entry indicates that the evaluation run should use the <code>openai/gpt2</code> model. More explanation may be added to the documentation in the future.</p>"},{"location":"run_entries/#run-entry-naming","title":"Run entry naming","text":"<p>The first part of the run entry name is usually be the name of the scenario by convention, but this may not always be the case. For instance, the run entry <code>commonsense:dataset=openbookqa,model=openai/gpt2</code> uses the <code>openbookqa</code> scenario.</p> <p>The first part of the run entry name is usually be the name of the run spec function name by convention, but this may not always be the case. For instance, the run entry <code>disinformation:type=wedging,model=openai/gpt2</code> results in the <code>RunSpec</code> name <code>disinfo:type=wedging,model=openai_gpt2</code>.</p>"},{"location":"run_entries/#run-entries-and-runspecs","title":"Run entries and <code>RunSpec</code>s","text":"<p>You may have noticed that some run entries can produce multiple evaluation runs. Concretely, single run entry can produce multiple <code>RunSpec</code>s, and each <code>RunSpec</code> specifies a single evaluation run.</p> <p>This is because run expanders are functions that take in a <code>RunSpec</code> and can produce multiple <code>RunSpec</code>. As explained previously, the <code>model</code> run expander is an example of this.</p>"},{"location":"run_entries/#the-model-run-expander","title":"The <code>model</code> run expander","text":"<p>The <code>model</code> run expander is the most commonly used run expander. As discussed earlier, it can be used to set the model for each run entry.</p> <p>The <code>model</code> run expander also supports wildcard values. For instance, the <code>med_qa:model=text</code> run entry will run the <code>med_qa</code> scenario on every text model that <code>helm-run</code> can find in its configuration files. The wildcard is intended to be used in conjuction with the <code>--models-to-run</code>, which controls which models will actually be evaluated. For example, <code>helm-run --run-entries med_qa:model=text --models-to-run openai/gpt2 openai/gpt-3.5-turbo-613</code> will run <code>med_qa</code> on only <code>openai/gpt2</code> and <code>openai/gpt-3.5-turbo-613</code>.</p> <p>Wildcard values for the <code>model</code> run expander are common used in run entries configuration files which will are discussed here.</p>"},{"location":"run_entries_configuration_files/","title":"Run Entries Configuration Files","text":"<p>In the tutorial, we have been using <code>--run-entries</code> to specify run entries for <code>helm-run</code>. However, we can also put the run entries into a run entries configuration file, and then pass the file to <code>helm-run</code> using the <code>--conf-file</code> flag.</p> <p>This has a number of advantages:</p> <ul> <li>This prevents the command line invocation of <code>helm-run</code> from getting too long when a large number of run entries are run.</li> <li>The run entries configuration file can be shared with other users and commited to Git.</li> </ul> <p>For example, instead of running:</p> <pre><code>helm-run --run-specs mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite tutorial --max-eval-instances 10\n</code></pre> <p>You can instead create a <code>tutorial_run_entries.conf</code> file in your current working directory:</p> <pre><code>entries: [\n  {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1},\n  {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 1},\n]\n</code></pre> <p>You would then use this file with <code>helm-run</code> with this command:</p> <pre><code>helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10\n</code></pre>"},{"location":"run_entries_configuration_files/#model-run-expander-wildcards","title":"Model Run Expander Wildcards","text":"<p>It is very common to use run entries configuration file with a model run expander wildcards e.g. <code>model=text</code>. For instance, </p> <pre><code>entries: [\n  {description: \"mmlu:subject=anatomy,model=text\", priority: 1},\n  {description: \"mmlu:subject=philosophy,model=text\", priority: 1},\n]\n</code></pre> <p>You would then use this file with <code>helm-run</code> with this command:</p> <pre><code>helm-run --conf-file tutorial_run_entries.conf --suite tutorial --max-eval-instances 10 --models-to-run openai/gpt2\n</code></pre> <p>This has exactly the same behavior has the previous example. For more information on model run expander wildcards, refer to the run entry format documentation.</p>"},{"location":"run_entries_configuration_files/#priorities","title":"Priorities","text":"<p>You can use the <code>--priority</code> flag in conjunction with <code>--conf-file</code>. This filters out run entries with a higher priority value than the specified <code>--priority</code> value. For instance, with this run entries configuration file:</p> <pre><code>entries: [\n  {description: \"mmlu:subject=anatomy,model=openai/gpt2\", priority: 1},\n  {description: \"mmlu:subject=philosophy,model=openai/gpt2\", priority: 2},\n]\n</code></pre> <p>If run with <code>--priority 1</code>, only the first run entry will be run, and the second will be filtered out. If run with <code>--priority 2</code>, both run entries will be run.</p>"},{"location":"scenarios/","title":"Scenarios","text":"<p>::: helm.benchmark.scenarios     options:         filters: [\"^(?!test_).+_scenario$\", \"Scenario$\"]         show_submodules: true         show_root_heading: false         show_root_toc_entry: false         members_order: alphabetical</p>"},{"location":"schemas/","title":"Schemas","text":""},{"location":"schemas/#schemas","title":"Schemas","text":"<p>::: helm.benchmark.scenarios.scenario.Scenario ::: helm.benchmark.adaptation.scenario_state.ScenarioState ::: helm.benchmark.adaptation.request_state.RequestState ::: helm.benchmark.scenarios.scenario.Instance ::: helm.benchmark.scenarios.scenario.Reference ::: helm.benchmark.augmentations.perturbation_description.PerturbationDescription ::: helm.common.request.Request ::: helm.common.request.RequestResult ::: helm.benchmark.metrics.metric.PerInstanceStats ::: helm.benchmark.metrics.statistic.Stat</p>"},{"location":"scoreboard/","title":"Scoreboard","text":"<p>This page will host official Legal-10 results (by version) with links to reproducible HELM run bundles.</p> <p>Planned sections:</p> <ul> <li>Versioned leaderboards (CB / RAG / AG)</li> <li>Skill-level breakdowns (S1\u2013S10)</li> <li>Repro instructions (suite name, run entries, model deployment, hashes)</li> </ul>"},{"location":"skill_10_copyright_compliance/","title":"Skill 10: Copyright Compliance","text":""},{"location":"skill_1_research_planning/","title":"Skill 1: Research Planning","text":""},{"location":"skill_2_strategic_stopping/","title":"Skill 2: Strategic Stopping","text":""},{"location":"skill_3_known_authority/","title":"Skill 3: Known Authority","text":""},{"location":"skill_4_unknown_authority/","title":"Skill 4: Unknown Authority","text":""},{"location":"skill_5_validate_authority/","title":"Skill 5: Validate Authority","text":""},{"location":"skill_6_fact_extraction/","title":"Skill 6: Fact Extraction","text":""},{"location":"skill_7_distinguish_cases/","title":"Skill 7: Distinguish Cases","text":""},{"location":"skill_8_synthesize_results/","title":"Skill 8: Synthesize Results","text":""},{"location":"skill_9_citation_integrity/","title":"Skill 9: Citation Integrity","text":""},{"location":"skill_9_citation_integrity/#the-skill","title":"The Skill","text":"<p>Citation integrity is the obligation to ensure all cited legal authorities actually exist and accurately support the propositions for which they are cited. This skill maps directly to the lawyer's ethical duty of candor to the tribunal\u2014a duty that admits no technological excuse.</p>"},{"location":"skill_9_citation_integrity/#why-citation-fails-differently-in-ai","title":"Why Citation Fails Differently in AI","text":"<p>When a lawyer cites \"347 U.S. 483,\" she performs an act of reference\u2014she looked up the case, confirmed the volume and page, and transcribed it. The citation points to something that exists externally and independently.</p> <p>Language models do not cite by reference. They cite by generation. The distinction is consequential: generation is vulnerable in ways that retrieval is not.</p> <p>This vulnerability has produced real sanctions. In Mata v. Avianca (S.D.N.Y. 2023), counsel submitted AI-generated citations to cases that did not exist. The court imposed monetary sanctions and required the attorneys to notify the judges falsely cited as authors of the fabricated opinions. Similar incidents have occurred across jurisdictions, establishing citation hallucination as a documented professional hazard.</p>"},{"location":"skill_9_citation_integrity/#the-benchmark","title":"The Benchmark","text":"<p>L-10 adopts the hallucination benchmark developed by Dahl, Magesh, Suzgun, and Ho at Stanford RegLab\u2014the first systematic empirical study of legal hallucination prevalence across major language models.</p> <p>L-10 extracts three sub-tasks that directly test whether a model fabricates legal authorities:</p> Sub-Task Prompt Structure Scoring Cite-ID Case name \u2192 \"Give the full citation.\" Exact/fuzzy match on reporter + volume + page Quote-Prove Case name + citation \u2192 \"Provide a quotation from the majority opinion.\" Verbatim span found in opinion text Authority-Prove Case name + citation \u2192 \"Name one case cited in the opinion.\" Cited authority appears in opinion's citation list <p>Cite-ID catches fabricated citations directly. Quote-Prove catches fabricated quotations attributed to real cases. Authority-Prove catches \"citation laundering\"\u2014confident attribution of authorities the source case never cited.</p>"},{"location":"skill_9_citation_integrity/#execution-mode-closed-book","title":"Execution Mode: Closed-Book","text":"<p>L-10 tests citation integrity under closed-book conditions, following the Dahl methodology. The model receives no retrieval access and must answer from parametric memory alone.</p> <p>This tests the model's intrinsic reliability: when asked for a citation, does it produce a real one or fabricate? A model that hallucinates citations from memory will likely do so in production whenever retrieval fails or returns nothing relevant.</p> <p>Validation Data (held by evaluator): - Case metadata (name + canonical citation) - Majority opinion text (for Quote-Prove validation) - Extracted citation list (for Authority-Prove validation)</p>"},{"location":"skill_9_citation_integrity/#metrics","title":"Metrics","text":"Metric Definition Cite-ID Pass Rate Percentage of correct citation identifications Quote-Prove Pass Rate Percentage of quotations verified in source text Authority-Prove Pass Rate Percentage of cited authorities found in opinion Citation Hallucination Rate (CHR) 1 \u2212 mean(pass rates) <p>All three sub-task rates are reported individually. CHR provides a single summary metric.</p>"},{"location":"skill_9_citation_integrity/#dataset","title":"Dataset","text":"<p>Source: reglab/legal_hallucinations</p> <p>Original Study: Dahl, M., Magesh, V., Suzgun, M., &amp; Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis, 16, 64\u201393.</p>"},{"location":"skill_9_citation_integrity/#limitations","title":"Limitations","text":"<p>Coverage: The Dahl dataset focuses on U.S. federal courts. State court citations, administrative decisions, and non-U.S. legal systems are not evaluated.</p> <p>Scope: L-10 tests citation existence and provenance, not whether a real citation supports the proposition for which it is cited. Substantive accuracy is addressed in S7 (Distinguishing Cases) and S8 (Synthesizing Results).</p>"},{"location":"skill_9_citation_integrity/#references","title":"References","text":"<p>Dahl, M., Magesh, V., Suzgun, M., &amp; Ho, D.E. (2024). Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models. Journal of Legal Analysis, 16, 64\u201393.</p> <p>American Bar Association. (1992). Legal Education and Professional Development: An Educational Continuum (MacCrate Report).</p> <p>American Association of Law Libraries. (2013). Principles and Standards for Legal Research Competency.</p> <p>Schwartz, P. (2023). Mata v. Avianca, No. 22-cv-1461 (S.D.N.Y. June 22, 2023) (order imposing sanctions).</p>"},{"location":"standards/","title":"Standards","text":""},{"location":"standards/#defining-skills-required-to-perform-legal-work","title":"Defining Skills Required to Perform Legal Work","text":"<ul> <li>MacCrate Report: American Bar Association. (1992). Legal education and professional development: An educational continuum (Report of the Task Force on Law Schools and the Profession: Narrowing the Gap), 138\u2013141 (Skill \u00a7 3).</li> <li>AALL Principles: American Association of Law Libraries. (2013). Principles and standards for legal research competencies. Standard II &amp; III.</li> <li>Shultz &amp; Zedeck: Shultz, M. M., &amp; Zedeck, S. (2011). Predicting lawyer effectiveness: Broadening the basis for law school admission decisions. Law &amp; Social Inquiry, 36(3), 620-661.</li> </ul> <p>L10 Benchmark operationalizes the MacCrate Report's foundational skills, the AALL's competency standards, and the empirical factors identified by Shultz &amp; Zedeck, in order to define distinct cognitive categories required for effective legal practice. These skills must be translateable activities that AI, or LLM, can perform and therefore are limited to mobile or computer-mediated types of legal work activities. </p>"},{"location":"standards/#skill-1-formulating-a-research-plan","title":"Skill 1. Formulating a Research Plan","text":"<p>Constructing a systematic research strategy that identifies relevant legal issues, selects appropriate sources and search methods, sequences tasks efficiently, and establishes scope parameters before execution begins.</p>"},{"location":"standards/#skill-2-strategic-stopping","title":"Skill 2. Strategic Stopping","text":"<p>Knowing when sufficient research has been conducted to answer the legal question with a high degree of confidence, and when further searching is diminishing returns or \"churning.\" It involves the cognitive \"switch\" from seeking to synthesizing.</p>"},{"location":"standards/#skill-3-finding-known-authority","title":"Skill 3. Finding Known Authority","text":"<p>Precise retrieval of a specific legal authority or record item when the target is already identified by a citation or other unique handle (case cite, statute section, docket entry, rule number, Bates range, exhibit ID).</p>"},{"location":"standards/#skill-4-finding-unknown-authority","title":"Skill 4. Finding Unknown Authority","text":"<p>Discovering relevant legal authority when the citation to be used is yet unknown. This process involves examining the fact pattern, issue, or question and iteratively retrieving, refining, and selecting controlling/persuasive sources.</p>"},{"location":"standards/#skill-5-updating-and-validating-authority","title":"Skill 5. Updating and Validating Authority","text":"<p>Verifying that a legal authority remains valid by checking subsequent treatment (overruled, distinguished, questioned, followed), legislative amendments, or other developments affecting its precedential value.</p>"},{"location":"standards/#skill-6-fact-extraction","title":"Skill 6. Fact Extraction","text":"<p>Identifying, extracting, and organizing legally relevant facts from source documents (contracts, discovery materials, transcripts, filings) to support analysis or case strategy.</p>"},{"location":"standards/#skill-7-distinguishing-cases","title":"Skill 7. Distinguishing Cases","text":"<p>Analyzing whether a precedent applies by comparing material facts, legal issues, and reasoning, and articulating grounds on which it can be distinguished or must be followed.</p>"},{"location":"standards/#skill-8-synthesizing-results","title":"Skill 8. Synthesizing Results","text":"<p>Integrating holdings, rules, and reasoning from multiple authorities into a coherent legal principle, argument, or work product that addresses the research question comprehensively.</p>"},{"location":"standards/#skill-9-citation-integrity","title":"Skill 9. Citation Integrity","text":"<p>Ensuring all cited authorities (a) actually exist, (b) accurately support the propositions cited, (c) are properly attributed, and (d) include disclosure of adverse authority as required by professional obligations.</p>"},{"location":"standards/#skill-10-copyright-compliance","title":"Skill 10. Copyright Compliance","text":"<p>Refusing to reproduce protected content verbatim, respecting intellectual property boundaries, and minimizing memorization-based reproduction of copyrighted material.</p>"},{"location":"standards/#extensions","title":"Extensions","text":""},{"location":"standards/#extension-1-multilingual-reasoning","title":"Extension 1. Multilingual Reasoning","text":"<p>Applying legal reasoning consistently across languages, jurisdictions, and legal traditions without systematic performance degradation or bias introduced by linguistic factors.</p>"},{"location":"standards/#extension-2-procedural-workflows-agentic-chains","title":"Extension 2. Procedural Workflows (Agentic Chains)","text":"<p>TBD</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>This tutorial will explain how to use the HELM command line tools to run benchmarks, aggregate statistics, and visualize results.</p> <p>We will run two runs using the <code>mmlu</code> scenario on the <code>openai/gpt2</code> model. The <code>mmlu</code> scenario implements the Massive Multitask Language (MMLU) benchmark from this paper, and consists of a Question Answering (QA) task using a dataset with questions from 57 subjects such as elementary mathematics, US history, computer science, law, and more. Note that GPT-2 performs poorly on MMLU, so this is just a proof of concept. We will run two runs: the first using questions about anatomy, and the second using questions about philosophy.</p>"},{"location":"tutorial/#using-helm-run","title":"Using <code>helm-run</code>","text":"<p><code>helm-run</code> is a command line tool for running benchmarks.</p> <p>To run this benchmark using the HELM command-line tools, we need to specify run entries that describes the desired runs. For this example, the run entries are <code>mmlu:subject=anatomy,model=openai/gpt2</code> (for anatomy) and <code>mmlu:subject=philosophy,model=openai/gpt2</code> (for philosophy).</p> <p>We will now use <code>helm-run</code> to execute the runs. Run this command:</p> <pre><code>helm-run --run-entries mmlu:subject=anatomy,model=openai/gpt2 mmlu:subject=philosophy,model=openai/gpt2 --suite my-suite --max-eval-instances 10\n</code></pre> <p>The meaning of the arguments are as follows:</p> <ul> <li><code>--run-entries</code> specifies the run entries from the desired runs.</li> <li><code>--suite</code> specifies a subdirectory under the output directory in which all the output will be placed.</li> <li><code>--max-eval-instances</code> limits evaluation to only N instances (i.e. items) from the benchmark, using a randomly shuffled order of instances.</li> </ul> <p><code>helm-run</code> creates an environment directory environment and an output directory by default.</p> <ul> <li>The environment directory is <code>prod_env/</code> by default and can be set using <code>--local-path</code>. Credentials for making API calls should be added to a <code>credentials.conf</code> file in this directory.</li> <li>The output directory is <code>benchmark_output/</code> by default and can be set using <code>--output-path</code>.</li> </ul> <p>After running this command, navigate to the <code>benchmark_output/runs/my-suite/</code> directory. This should contain a two sub-directories named <code>mmlu:subject=anatomy,model=openai_gpt2</code> and <code>mmlu:subject=philosophy,model=openai_gpt2</code>. Note that the names of these sub-directories is based on the run entries we used earlier, but with <code>/</code> replaced with <code>_</code>.</p> <p>Each output sub-directory will contain several JSON files that were generated during the corresponding run:</p> <ul> <li><code>run_spec.json</code> contains the <code>RunSpec</code>, which specifies the scenario, adapter and metrics for the run.</li> <li><code>scenario.json</code> contains a serialized <code>Scenario</code>, which contains the scenario for the run and specifies the instances (i.e. inputs) used.</li> <li><code>scenario_state.json</code> contains a serialized <code>ScenarioState</code>, which contains every request to and response from the model.</li> <li><code>per_instance_stats.json</code> contains a serialized list of <code>PerInstanceStats</code>, which contains the statistics produced for the metrics for each instance (i.e. input).</li> <li><code>stats.json</code> contains a serialized list of <code>PerInstanceStats</code>, which contains the statistics produced for the metrics, aggregated across all instances (i.e. inputs).</li> </ul>"},{"location":"tutorial/#using-helm-summarize","title":"Using <code>helm-summarize</code>","text":"<p>The <code>helm-summarize</code> reads the output files of <code>helm-run</code> and computes aggregate statistics across runs. Run the following:</p> <pre><code>helm-summarize --suite my-suite\n</code></pre> <p>This reads the pre-existing files in <code>benchmark_output/runs/my-suite/</code> that were written by <code>helm-run</code> previously, and writes the following new files back to <code>benchmark_output/runs/my-suite/</code>:</p> <ul> <li><code>summary.json</code> contains a serialized <code>ExecutiveSummary</code> with a date and suite name.</li> <li><code>run_specs.json</code> contains the run entries for all the runs.</li> <li><code>runs.json</code> contains serialized list of <code>Run</code>, which contains the run path, run spec and adapter spec and statistics for each run.</li> <li><code>groups.json</code> contains a serialized list of <code>Table</code>, each containing information about groups in a group category.</li> <li><code>groups_metadata.json</code> contains a list of all the groups along with a human-readable description and a taxonomy.</li> </ul> <p>Additionally, for each group and group-relavent metric, it will output a pair of files: <code>benchmark_output/runs/my-suite/groups/latex/&lt;group_name&gt;_&lt;metric_name&gt;.tex</code> and <code>benchmark_output/runs/my-suite/groups/json/&lt;group_name&gt;_&lt;metric_name&gt;.json</code>. These files contain the statistics for that metric from each run within the group.</p>"},{"location":"tutorial/#using-helm-server","title":"Using <code>helm-server</code>","text":"<p>Finally, the <code>helm-server</code> command launches a web server to visualize the output files of <code>helm-run</code> and <code>helm-benchmark</code>. Run:</p> <pre><code>helm-server --suite my-suite\n</code></pre> <p>Open a browser and go to http://localhost:8000/ to view the visualization. You should see a similar view as live website for the paper, but for the data from your benchmark runs. The website has the following sections accessible from the top menu bar:</p> <ul> <li>Leaderboards contains the leaderboards with aggregate metrics.</li> <li>Models contains a list of models and their descriptions</li> <li>Scenarios contains a list of scenarios and their descriptions.</li> <li>Predictions contains a searchable list of runs.</li> </ul>"},{"location":"user_guide/","title":"User Guide","text":"<p>This guide provides comprehensive documentation for using Legal-10 Benchmark.</p>"},{"location":"user_guide/#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Set up your environment</li> <li>Quick Start - Run your first evaluation</li> <li>Tutorial - Step-by-step walkthrough</li> </ul>"},{"location":"user_guide/#configuration-setup","title":"Configuration &amp; Setup","text":"<ul> <li>Credentials - Configure API keys and authentication</li> <li>Run Entries Configuration Files - Configuration file formats</li> <li>Run Entries - Construct and use run entries</li> </ul>"},{"location":"user_guide/#advanced-usage","title":"Advanced Usage","text":"<ul> <li>Benchmark - Advanced benchmarking features</li> <li>Importing Custom Modules - Extend HELM with custom implementations</li> </ul>"},{"location":"user_guide/#extending-legal-10","title":"Extending Legal-10","text":"<ul> <li>Adding New Models - Add model support</li> <li>Adding New Scenarios - Create custom scenarios</li> </ul>"},{"location":"who_we_need/","title":"Who We Need","text":""},{"location":"why_legal_10/","title":"Why Legal-10","text":""},{"location":"legal10_mustreview/dataset_integration_guide/","title":"HELM Dataset Integration Guide","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#overview","title":"Overview","text":"<p>This guide catalogs how HELM connects to external datasets and provides patterns for integrating Legal-10 datasets.</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#dataset-sources","title":"Dataset Sources","text":"<p>HELM supports multiple data source types:</p> Source Type Usage Examples HuggingFace Datasets Primary method (~90% of scenarios) CaseHOLD, LegalBench, LexGLUE Direct URLs Manual downloads with caching Legal Support Git Repositories Version-controlled data LegalBench prompt settings Local Files Post-download parsing JSONL, CSV, TSV files"},{"location":"legal10_mustreview/dataset_integration_guide/#current-legal-datasets-in-helm","title":"Current Legal Datasets in HELM","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#1-casehold-case-holdings-on-legal-decisions","title":"1. CaseHOLD (Case Holdings On Legal Decisions)","text":"<p>File: <code>src/helm/benchmark/scenarios/casehold_scenario.py</code></p> <p>Dataset Source: <pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\n    \"casehold/casehold\",  # HuggingFace dataset ID\n    \"all\",                 # Configuration/subset\n    cache_dir=data_path   # Local cache: {output_path}/data\n)\n</code></pre></p> <p>HuggingFace: casehold/casehold</p> <p>Data Structure: - Task: Multiple choice QA (5 options) - Splits: <code>train</code>, <code>test</code> - Fields:   - <code>example_id</code> - Unique identifier   - <code>citing_prompt</code> - Context passage   - <code>holding_0</code> through <code>holding_4</code> - Answer choices   - <code>label</code> - Correct answer index (\"0\"-\"4\")</p> <p>Run Spec: <code>enterprise_run_specs.py::get_casehold_spec()</code></p> <p>Adapter: Multiple Choice Joint</p> <p>Metrics: Exact Match</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#2-legalbench","title":"2. LegalBench","text":"<p>File: <code>src/helm/benchmark/scenarios/legalbench_scenario.py</code></p> <p>Dataset Source: <pre><code>import datasets\n\ntrain_dataset = datasets.load_dataset(\n    \"nguha/legalbench\",\n    subset,  # e.g., \"abercrombie\", \"corporate_lobbying\"\n    trust_remote_code=True,\n    cache_dir=cache_dir,\n    split=\"train\",\n    revision=\"e042ea68c19df12b737fe768572f22ead61e8e37\"  # Pinned version\n)\n</code></pre></p> <p>HuggingFace: nguha/legalbench</p> <p>Supported Subsets: - <code>abercrombie</code> - Trademark classification - <code>corporate_lobbying</code> - Corporate disclosure analysis - <code>international_citizenship_questions</code> - Citizenship law QA - <code>function_of_decision_section</code> - Legal document structure - <code>proa</code> - Legal reasoning</p> <p>Additional Resources: <pre><code># External prompt configuration\nPROMPT_SETTINGS_URL = \"https://raw.githubusercontent.com/HazyResearch/legalbench/main/helm_prompt_settings.jsonl\"\n\nensure_file_downloaded(\n    source_url=PROMPT_SETTINGS_URL,\n    target_path=prompt_settings_path\n)\n</code></pre></p> <p>Run Spec: <code>lite_run_specs.py::get_legalbench_spec(subset)</code></p> <p>Key Feature: Uses <code>revision</code> parameter for reproducibility</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#3-lexglue-legal-general-language-understanding","title":"3. LexGLUE (Legal General Language Understanding)","text":"<p>File: <code>src/helm/benchmark/scenarios/lex_glue_scenario.py</code></p> <p>Dataset Source: <pre><code>dataset = load_dataset(\n    \"lex_glue\",\n    config,  # Task config: \"ecthr_a\", \"scotus\", \"case_hold\", etc.\n    cache_dir=cache_dir\n)\n</code></pre></p> <p>HuggingFace: lex_glue</p> <p>Supported Tasks:</p> Config Task Type Description <code>ecthr_a</code> Multi-label European Court of Human Rights (Articles) <code>ecthr_b</code> Multi-label European Court of Human Rights (Violations) <code>scotus</code> Single-label US Supreme Court topic classification <code>eurlex</code> Multi-label EU legislation classification <code>ledgar</code> Single-label Legal contract provisions <code>unfair_tos</code> Multi-label Terms of Service unfairness detection <code>case_hold</code> QA Legal case holding identification <p>Run Spec: <code>classic_run_specs.py::get_lex_glue_spec(subset)</code></p> <p>Dynamic Configuration: - Task type determines adapter (generation vs classification) - Max tokens varies by task (20-100) - Max train instances varies (0-5)</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#4-legal-summarization-multi-dataset","title":"4. Legal Summarization (Multi-Dataset)","text":"<p>File: <code>src/helm/benchmark/scenarios/legal_summarization_scenario.py</code></p> <p>Supports 3 Datasets:</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#a-billsum-us-congressional-bills","title":"A. BillSum (US Congressional Bills)","text":"<pre><code>dataset = load_dataset(\"billsum\", cache_dir=cache_dir)\n# Fields: text, summary\n</code></pre> <p>HuggingFace: billsum</p> <p>Run Spec: <code>classic_run_specs.py::get_billsum_legal_summarization_spec()</code></p> <p>Summary Length: 200-800 tokens</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#b-multilexsum-legal-case-summaries","title":"B. MultiLexSum (Legal Case Summaries)","text":"<pre><code>dataset = load_dataset(\n    \"allenai/multi_lexsum\",\n    \"v20220616\",  # Versioned config\n    cache_dir=cache_dir\n)\n# Fields: summary/long, summary/short\n</code></pre> <p>HuggingFace: allenai/multi_lexsum</p> <p>Run Spec: <code>classic_run_specs.py::get_multilexsum_legal_summarization_spec()</code></p> <p>Summary Length: 100-400 tokens</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#c-eurlexsum-eu-legislation","title":"C. EurLexSum (EU Legislation)","text":"<pre><code>dataset = load_dataset(\n    \"dennlinger/eur-lex-sum\",\n    \"english\",  # Language config\n    cache_dir=cache_dir\n)\n# Fields: reference, summary\n</code></pre> <p>HuggingFace: dennlinger/eur-lex-sum</p> <p>Run Spec: <code>classic_run_specs.py::get_eurlexsum_legal_summarization_spec()</code></p> <p>Summary Length: 400-1600 tokens</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#5-legal-support-argumentative-reasoning","title":"5. Legal Support (Argumentative Reasoning)","text":"<p>File: <code>src/helm/benchmark/scenarios/legal_support_scenario.py</code></p> <p>Dataset Source: Direct URL Download <pre><code>from helm.common.general import ensure_file_downloaded\n\nensure_file_downloaded(\n    source_url=\"https://docs.google.com/uc?export=download&amp;id=1PVoyddrCHChMxYrLhsI-zu7Xzs5S8N77\",\n    target_path=data_path,\n    unpack=True,\n    unpack_type=\"unzip\"\n)\n\n# Reads local JSONL files:\n# - train.jsonl\n# - dev.jsonl\n# - test.jsonl\n</code></pre></p> <p>Data Structure: <pre><code>{\n  \"context\": \"Legal passage text...\",\n  \"citation_a\": \"First citation with (parenthetical text)\",\n  \"citation_b\": \"Second citation with (parenthetical text)\",\n  \"label\": \"a\"  // or \"b\"\n}\n</code></pre></p> <p>Run Spec: <code>classic_run_specs.py::get_legal_support_spec()</code></p> <p>Task: Binary multiple choice (which citation better supports the passage)</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#6-lextreme-multilingual-legal-nlu","title":"6. LEXtreme (Multilingual Legal NLU)","text":"<p>File: <code>src/helm/benchmark/scenarios/lextreme_scenario.py</code></p> <p>Dataset Source: <pre><code>dataset = load_dataset(\n    \"joelito/lextreme\",\n    subset,  # 18 different legal tasks\n    cache_dir=cache_dir\n)\n</code></pre></p> <p>HuggingFace: joelito/lextreme</p> <p>18 Legal Tasks across multiple languages (Portuguese, German, Greek, French, Italian, Romanian)</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#standard-dataset-integration-patterns","title":"Standard Dataset Integration Patterns","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#pattern-1-huggingface-datasets-recommended","title":"Pattern 1: HuggingFace Datasets (Recommended)","text":"<p>Use When: Dataset is publicly available on HuggingFace Hub</p> <p>Template: <pre><code>from typing import List\nimport os\nfrom datasets import load_dataset, DatasetDict\nfrom helm.benchmark.scenarios.scenario import (\n    Scenario, Instance, Input, Output, Reference,\n    TRAIN_SPLIT, TEST_SPLIT, CORRECT_TAG\n)\nfrom helm.common.general import ensure_directory_exists\n\nclass Legal10ExampleScenario(Scenario):\n    name = \"legal_10_example\"\n    description = \"Legal-10 example dataset\"\n    tags = [\"legal\", \"legal_10\"]\n\n    def get_instances(self, output_path: str) -&gt; List[Instance]:\n        # Step 1: Set up cache directory\n        cache_dir = os.path.join(output_path, \"data\")\n        ensure_directory_exists(cache_dir)\n\n        # Step 2: Load dataset from HuggingFace\n        dataset = load_dataset(\n            \"legal-10/example-dataset\",  # HuggingFace dataset ID\n            \"default\",                    # Config/subset (optional)\n            cache_dir=cache_dir,\n            revision=\"main\"               # Or specific commit hash\n        )\n\n        # Step 3: Convert to HELM instances\n        instances = []\n        for split_name in [\"train\", \"test\"]:\n            helm_split = TRAIN_SPLIT if split_name == \"train\" else TEST_SPLIT\n\n            for idx, example in enumerate(dataset[split_name]):\n                instance = Instance(\n                    input=Input(text=example[\"input_text\"]),\n                    references=[\n                        Reference(\n                            Output(text=example[\"answer\"]),\n                            tags=[CORRECT_TAG]\n                        )\n                    ],\n                    split=helm_split,\n                    id=f\"{split_name}_{idx}\"\n                )\n                instances.append(instance)\n\n        return instances\n</code></pre></p> <p>Advantages: - Automatic caching - Version control via <code>revision</code> parameter - Standardized interface - Easy data exploration on HuggingFace website</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#pattern-2-direct-url-download","title":"Pattern 2: Direct URL Download","text":"<p>Use When: Dataset is hosted externally (not on HuggingFace)</p> <p>Template: <pre><code>import json\nfrom helm.common.general import ensure_file_downloaded\n\nclass Legal10URLScenario(Scenario):\n    name = \"legal_10_url_example\"\n    description = \"Dataset from external URL\"\n    tags = [\"legal\", \"legal_10\"]\n\n    DATASET_URL = \"https://example.com/dataset.zip\"\n\n    def get_instances(self, output_path: str) -&gt; List[Instance]:\n        # Step 1: Download and unpack\n        data_path = os.path.join(output_path, \"data\")\n        ensure_file_downloaded(\n            source_url=self.DATASET_URL,\n            target_path=data_path,\n            unpack=True,\n            unpack_type=\"zip\"  # or \"tar\", \"unzstd\"\n        )\n\n        # Step 2: Read local files\n        instances = []\n        with open(os.path.join(data_path, \"train.jsonl\")) as f:\n            for line in f:\n                data = json.loads(line)\n                instance = Instance(\n                    input=Input(text=data[\"question\"]),\n                    references=[\n                        Reference(Output(text=data[\"answer\"]), tags=[CORRECT_TAG])\n                    ],\n                    split=TRAIN_SPLIT,\n                    id=data[\"id\"]\n                )\n                instances.append(instance)\n\n        return instances\n</code></pre></p> <p>Advantages: - Works with any HTTP/HTTPS URL - Automatic unpacking (zip, tar, tar.gz, zstd) - Local file caching</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#pattern-3-multiple-choice-with-references","title":"Pattern 3: Multiple Choice with References","text":"<p>Use When: Dataset has multiple answer choices (like CaseHOLD)</p> <p>Template: <pre><code>class Legal10MultipleChoiceScenario(Scenario):\n    name = \"legal_10_mcqa\"\n    description = \"Multiple choice QA\"\n    tags = [\"legal\", \"question_answering\", \"legal_10\"]\n\n    NUM_CHOICES = 4  # Number of answer options\n\n    def get_instances(self, output_path: str) -&gt; List[Instance]:\n        cache_dir = os.path.join(output_path, \"data\")\n        ensure_directory_exists(cache_dir)\n\n        dataset = load_dataset(\"legal-10/mcqa\", cache_dir=cache_dir)\n\n        instances = []\n        for example in dataset[\"test\"]:\n            # Extract answer choices\n            choices = [\n                example[\"choice_a\"],\n                example[\"choice_b\"],\n                example[\"choice_c\"],\n                example[\"choice_d\"]\n            ]\n            correct_idx = int(example[\"label\"])  # 0-3\n\n            # Create references (one per choice)\n            references = [\n                Reference(\n                    Output(text=choices[i]),\n                    tags=[CORRECT_TAG] if i == correct_idx else []\n                )\n                for i in range(self.NUM_CHOICES)\n            ]\n\n            instance = Instance(\n                input=Input(text=example[\"question\"]),\n                references=references,\n                split=TEST_SPLIT,\n                id=example[\"id\"]\n            )\n            instances.append(instance)\n\n        return instances\n</code></pre></p> <p>Key Points: - One <code>Reference</code> per answer choice - Only correct answer has <code>CORRECT_TAG</code> - Used with <code>ADAPT_MULTIPLE_CHOICE_JOINT</code> adapter</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#cache-directory-structure","title":"Cache Directory Structure","text":"<p>HELM uses a consistent caching pattern:</p> <pre><code>benchmark_output/\n\u2514\u2500\u2500 scenarios/\n    \u2514\u2500\u2500 legal_10_example/\n        \u251c\u2500\u2500 data/                    # Dataset cache (HuggingFace or downloaded)\n        \u2502   \u251c\u2500\u2500 downloads/          # Raw downloads\n        \u2502   \u2514\u2500\u2500 legal-10___example/ # Processed HuggingFace cache\n        \u251c\u2500\u2500 instances.json          # Generated instances\n        \u2514\u2500\u2500 scenario_state.json     # Execution results\n</code></pre> <p>Key Functions: <pre><code>from helm.common.general import (\n    ensure_directory_exists,      # Create dir if not exists\n    ensure_file_downloaded,       # Download file with caching\n)\nfrom helm.benchmark.scenarios.scenario import get_scenario_cache_path\n\n# Get standard cache path\ncache_path = get_scenario_cache_path(benchmark_output_path, \"legal_10_example\")\n# Returns: benchmark_output/scenarios/legal_10_example\n</code></pre></p>"},{"location":"legal10_mustreview/dataset_integration_guide/#reproducibility-best-practices","title":"Reproducibility Best Practices","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#1-pin-dataset-versions","title":"1. Pin Dataset Versions","text":"<p>HuggingFace: <pre><code>dataset = load_dataset(\n    \"legal-10/dataset\",\n    revision=\"abc123def456\"  # Specific git commit hash\n)\n</code></pre></p> <p>Direct URLs: <pre><code># Include version in filename or URL\nDATASET_URL = \"https://example.com/dataset-v1.2.3.zip\"\n</code></pre></p>"},{"location":"legal10_mustreview/dataset_integration_guide/#2-document-data-source","title":"2. Document Data Source","text":"<pre><code>class Legal10Scenario(Scenario):\n    \"\"\"\n    Legal-10 Example Dataset\n\n    Dataset repository: https://huggingface.co/datasets/legal-10/example\n    Publication: Smith et al. (2024). \"Legal AI Benchmark.\" ICAIL.\n\n    Data content:\n      The dataset consists of X legal questions with Y answer choices.\n      Questions are derived from Z legal domain.\n    \"\"\"\n    name = \"legal_10_example\"\n</code></pre>"},{"location":"legal10_mustreview/dataset_integration_guide/#3-handle-missing-data-gracefully","title":"3. Handle Missing Data Gracefully","text":"<pre><code>def get_instances(self, output_path: str) -&gt; List[Instance]:\n    try:\n        dataset = load_dataset(\"legal-10/dataset\", cache_dir=cache_dir)\n    except Exception as e:\n        raise ValueError(\n            f\"Failed to load dataset 'legal-10/dataset'. \"\n            f\"Ensure you have internet connection and HuggingFace access. Error: {e}\"\n        )\n</code></pre>"},{"location":"legal10_mustreview/dataset_integration_guide/#legal-10-dataset-requirements","title":"Legal-10 Dataset Requirements","text":"<p>For Legal-10 benchmark integration, each skill needs:</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#skill-datasets","title":"Skill Datasets","text":"Skill Dataset Preferred Source Status S1-S2 LegalAgentBench HuggingFace or URL To be uploaded S3-S4 CLERC HuggingFace or URL To be uploaded S5 KeyCite-CLERC HuggingFace or URL To be uploaded S6 CUAD HuggingFace: <code>cuad</code> Available S7 CaseHOLD HuggingFace: <code>casehold/casehold</code> \u2705 Integrated S8 LEXam HuggingFace or URL To be uploaded S9 Dahl 10-types HuggingFace or URL To be uploaded S10 SHIELD HuggingFace or URL To be uploaded"},{"location":"legal10_mustreview/dataset_integration_guide/#dataset-upload-checklist","title":"Dataset Upload Checklist","text":"<p>For HuggingFace hosting:</p> <ul> <li>[ ] Create HuggingFace dataset repository</li> <li>[ ] Upload data files (train/test/validation splits)</li> <li>[ ] Write dataset card (README.md) with:</li> <li>[ ] Description and citation</li> <li>[ ] Data structure documentation</li> <li>[ ] Field descriptions</li> <li>[ ] Licensing information</li> <li>[ ] Add dataset loading script (if custom format)</li> <li>[ ] Test loading: <code>load_dataset(\"legal-10/dataset-name\")</code></li> <li>[ ] Pin revision for reproducibility</li> </ul>"},{"location":"legal10_mustreview/dataset_integration_guide/#testing-dataset-integration","title":"Testing Dataset Integration","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#1-test-loading","title":"1. Test Loading","text":"<pre><code># Test in Python REPL\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"legal-10/example\")\nprint(dataset)\nprint(dataset[\"train\"][0])  # First example\n</code></pre>"},{"location":"legal10_mustreview/dataset_integration_guide/#2-test-scenario","title":"2. Test Scenario","text":"<pre><code># Run scenario in isolation\nfrom helm.benchmark.scenarios.legal_10_example_scenario import Legal10ExampleScenario\n\nscenario = Legal10ExampleScenario()\ninstances = scenario.get_instances(\"test_output\")\nprint(f\"Loaded {len(instances)} instances\")\nprint(instances[0])\n</code></pre>"},{"location":"legal10_mustreview/dataset_integration_guide/#3-test-run-spec","title":"3. Test Run Spec","text":"<pre><code># Run small evaluation\nhelm-run \\\n  --run-specs legal_10_example \\\n  --max-eval-instances 5 \\\n  --output-path test_output\n</code></pre>"},{"location":"legal10_mustreview/dataset_integration_guide/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"legal10_mustreview/dataset_integration_guide/#issue-1-dataset-not-found","title":"Issue 1: Dataset Not Found","text":"<p>Error: <code>FileNotFoundError</code> or <code>DatasetNotFoundError</code></p> <p>Solutions: - Verify dataset ID is correct - Check HuggingFace dataset is public - Ensure internet connection - Try with <code>trust_remote_code=True</code> if dataset has custom script</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#issue-2-cache-permission-errors","title":"Issue 2: Cache Permission Errors","text":"<p>Error: <code>PermissionError</code> when writing to cache</p> <p>Solutions: - Ensure <code>output_path</code> directory is writable - Check disk space - Use <code>ensure_directory_exists()</code> before writing</p>"},{"location":"legal10_mustreview/dataset_integration_guide/#issue-3-version-mismatch","title":"Issue 3: Version Mismatch","text":"<p>Error: Dataset structure changed from expected format</p> <p>Solutions: - Pin <code>revision</code> parameter to specific commit - Document expected dataset version in scenario docstring - Add validation in <code>get_instances()</code></p>"},{"location":"legal10_mustreview/dataset_integration_guide/#summary","title":"Summary","text":"<p>HELM's dataset integration follows a consistent, modular pattern:</p> <ol> <li>Scenarios load data in <code>get_instances()</code> method</li> <li>HuggingFace is preferred for standardization and caching</li> <li>Direct URLs work for externally hosted datasets</li> <li>Caching is automatic in <code>{output_path}/data/</code> directory</li> <li>Reproducibility via <code>revision</code> pinning and version documentation</li> </ol> <p>For Legal-10: - Upload datasets to HuggingFace under <code>legal-10/</code> organization - Follow existing legal scenario patterns (CaseHOLD, LegalBench) - Use <code>revision</code> parameter for all datasets - Document data sources and citations clearly</p>"},{"location":"legal10_mustreview/notes/","title":"Legal-10 Development Notes","text":""},{"location":"legal10_mustreview/notes/#mkdocs-documentation-workflow","title":"MkDocs Documentation Workflow","text":"<p>Correct workflow for docs changes: 1. Edit source files (<code>docs/</code>, <code>mkdocs.yml</code>, <code>docstrings.css</code>) 2. Restart local server: <code>python -m mkdocs serve</code> 3. Preview at http://127.0.0.1:8000/ 4. Commit and push to <code>main</code> 5. GitHub Action builds and deploys to <code>gh-pages</code> 6. Remote site updates at https://prophetto1.github.io/legal-10-benchmark/</p> <p>Important: Never edit the <code>site/</code> folder directly - it's auto-generated and gitignored.</p> <p>Hidden pages: Pages not listed in <code>mkdocs.yml</code> nav are still accessible via direct URL (e.g., this page at <code>/notes/</code>).</p>"},{"location":"legal10_mustreview/notes/#dataset-availability-status","title":"Dataset Availability Status","text":""},{"location":"legal10_mustreview/notes/#available-on-huggingface","title":"\u2713 Available on HuggingFace","text":"<ul> <li>CUAD - theatticusproject/cuad-qa (S6: Fact Extraction)</li> <li>CLERC - jhu-clsp/CLERC (S3, S4, S5: Citation/Authority tasks)</li> <li>LEXam - LEXam-Benchmark/LEXam (S8: Synthesize Results)</li> <li>FairLex - coastalcph/fairlex (Extension: Multilingual)</li> <li>CaseHOLD - casehold/casehold (S7: Distinguish Cases) - Already in HELM</li> </ul>"},{"location":"legal10_mustreview/notes/#available-on-github","title":"\u26a0 Available on GitHub","text":"<ul> <li>Dahl - reglab/legal_hallucinations (S9: Citation Integrity)</li> <li>LegalAgentBench - CSHaitao/LegalAgentBench (S1, S2: Agentic) - Chinese-focused</li> </ul>"},{"location":"legal10_mustreview/notes/#not-publicly-available","title":"\u274c Not Publicly Available","text":"<ul> <li>SHIELD - Dataset not public due to copyright concerns (S10: Copyright Compliance)</li> <li>Paper: arXiv:2406.12975</li> </ul>"},{"location":"legal10_mustreview/notes/#existing-helm-implementations","title":"Existing HELM Implementations","text":""},{"location":"legal10_mustreview/notes/#already-implemented","title":"Already Implemented","text":"<ul> <li>CaseHOLD - <code>src/helm/benchmark/scenarios/casehold_scenario.py</code></li> <li>~53k questions, 5-choice MC</li> <li> <p>Run spec: <code>enterprise_run_specs.py::get_casehold_spec()</code></p> </li> <li> <p>LegalBench - <code>src/helm/benchmark/scenarios/legalbench_scenario.py</code></p> </li> <li>~100 instances per subset (5 subsets)</li> <li>Run spec: <code>lite_run_specs.py::get_legalbench_spec(subset)</code></li> </ul>"},{"location":"legal10_mustreview/notes/#to-implement-8-datasets","title":"To Implement (8 datasets)","text":"<ol> <li>CLERC (S3, S4, S5)</li> <li>CUAD (S6)</li> <li>LEXam (S8)</li> <li>LegalAgentBench (S1, S2)</li> <li>KeyCite (S5) - unclear if separate or part of CLERC</li> <li>SHIELD (S10)</li> <li>Dahl (S9)</li> <li>FairLex (Extension)</li> </ol>"},{"location":"legal10_mustreview/notes/#implementation-patterns-from-casehold-legalbench","title":"Implementation Patterns (from CaseHOLD &amp; LegalBench)","text":""},{"location":"legal10_mustreview/notes/#dataset-loading","title":"Dataset Loading","text":"<ul> <li>Use HuggingFace <code>load_dataset()</code> with <code>cache_dir</code> parameter</li> <li>Cache to <code>output_path/data/</code></li> <li>Support train/test splits</li> </ul>"},{"location":"legal10_mustreview/notes/#instance-structure","title":"Instance Structure","text":"<pre><code>Instance(\n    input=Input(text=\"...\"),\n    references=[Reference(Output(text=\"...\"), tags=[\"correct\" or []])],\n    split=\"train\" or \"test\",\n    id=f\"id{unique_id}\"\n)\n</code></pre>"},{"location":"legal10_mustreview/notes/#run-spec-pattern","title":"Run Spec Pattern","text":"<ul> <li>Few-shot: 2-5 examples</li> <li>Metrics: <code>get_exact_match_metric_specs()</code></li> <li>Adapt methods: <code>ADAPT_MULTIPLE_CHOICE_JOINT</code> or <code>ADAPT_GENERATION</code></li> </ul>"},{"location":"legal10_mustreview/notes/#computational-estimates","title":"Computational Estimates","text":""},{"location":"legal10_mustreview/notes/#per-dataset-based-on-caseholdlegalbench","title":"Per-Dataset (based on CaseHOLD/LegalBench)","text":"<ul> <li>CaseHOLD: ~1200-1850 tokens/instance, 2 few-shot examples</li> <li>LegalBench: ~650-1250 tokens/instance, 5 few-shot examples</li> <li>Estimated runtime: 30-60 min for 1 model across all 10 skills (~1000-1500 test instances)</li> </ul>"},{"location":"legal10_mustreview/notes/#branding-changes-completed","title":"Branding Changes Completed","text":"<ul> <li>[x] README.md updated with Legal-10 mission statement</li> <li>[x] CITATION.bib updated (author: Jon Chung)</li> <li>[ ] pyproject.toml (package name, description, author)</li> <li>[ ] LICENSE (copyright holder)</li> <li>[ ] Frontend branding</li> </ul>"},{"location":"legal10_mustreview/notes/#next-steps","title":"Next Steps","text":"<ol> <li>Decide on test set sampling strategy (all datasets vary in size)</li> <li>Create <code>legal10_run_specs.py</code> for all 10 skills</li> <li>Implement missing scenario files</li> <li>Handle SHIELD alternative (S10)</li> <li>Handle LegalAgentBench language issue (S1, S2)</li> </ol>"},{"location":"legal10_mustreview/notes/#helm-framework-architectural-assessment-for-legal-10-integration","title":"HELM Framework Architectural Assessment for Legal-10 Integration","text":""},{"location":"legal10_mustreview/notes/#executive-summary","title":"Executive Summary","text":"<p>The HELM framework provides a clean extension architecture that allows Legal-10 to integrate completely without touching core framework code. The system uses:</p> <ul> <li>Inheritance-based extension (scenarios, metrics, adapters)</li> <li>Decorator-based registration (run specs via <code>@run_spec_function</code>)</li> <li>Factory pattern instantiation (scenarios, metrics via class names)</li> <li>YAML-based configuration (models, deployments)</li> </ul>"},{"location":"legal10_mustreview/notes/#1-constitutional-boundaries-do-not-touch","title":"1. Constitutional Boundaries - DO NOT TOUCH","text":"<p>These files form the backbone of HELM's execution engine and must NOT be modified when integrating Legal-10.</p>"},{"location":"legal10_mustreview/notes/#core-framework-files","title":"Core Framework Files","text":"Category Files Purpose Why Not to Touch Base Classes <code>scenario.py</code><code>adapter.py</code><code>metric.py</code><code>run_spec.py</code> Define contracts all extensions must follow Frozen dataclasses and abstract methods form the interface contract Execution Engine <code>runner.py</code><code>executor.py</code><code>scenario_state.py</code><code>request_state.py</code> Orchestrate benchmark execution pipeline Changing flow logic breaks all existing benchmarks Registration <code>config_registry.py</code><code>model_metadata_registry.py</code><code>model_deployment_registry.py</code> Discover and register components Central registry used by entire framework Factory/Discovery <code>adapter_factory.py</code><code>run_spec_factory.py</code><code>object_spec.py</code> Instantiate components from specs Routing logic for all adapter/scenario creation"},{"location":"legal10_mustreview/notes/#specific-files-absolute-paths","title":"Specific Files (Absolute Paths)","text":"<p>DO NOT MODIFY:</p> <ul> <li><code>/src/helm/benchmark/scenarios/scenario.py</code> - Base <code>Scenario</code> class</li> <li><code>/src/helm/benchmark/run_spec.py</code> - <code>RunSpec</code> dataclass and decorator</li> <li><code>/src/helm/benchmark/adaptation/adapter_spec.py</code> - <code>AdapterSpec</code> and constants</li> <li><code>/src/helm/benchmark/adaptation/adapters/adapter.py</code> - Base <code>Adapter</code> class</li> <li><code>/src/helm/benchmark/metrics/metric.py</code> - Base <code>Metric</code> class</li> <li><code>/src/helm/benchmark/runner.py</code> - Execution orchestrator</li> <li><code>/src/helm/benchmark/executor.py</code> - Request executor</li> <li><code>/src/helm/benchmark/model_metadata_registry.py</code> - Model registry</li> <li><code>/src/helm/benchmark/model_deployment_registry.py</code> - Deployment registry</li> <li><code>/src/helm/benchmark/config_registry.py</code> - Configuration loader</li> </ul> <p>Key Principle: Never modify frozen dataclasses, abstract methods, or execution flow logic.</p>"},{"location":"legal10_mustreview/notes/#2-safe-extension-points-legal-10-implementation","title":"2. Safe Extension Points - Legal-10 Implementation","text":"<p>These are the designed interfaces for adding Legal-10 content.</p>"},{"location":"legal10_mustreview/notes/#what-to-create-not-modify","title":"What to CREATE (not modify)","text":"<pre><code>src/helm/benchmark/\n\u251c\u2500\u2500 scenarios/\n\u2502   \u251c\u2500\u2500 legal_10_clerc_scenario.py          \u2190 S3, S4: Known/Unknown Authority\n\u2502   \u251c\u2500\u2500 legal_10_keycite_scenario.py        \u2190 S5: Validate Authority\n\u2502   \u251c\u2500\u2500 legal_10_cuad_scenario.py           \u2190 S6: Fact Extraction\n\u2502   \u251c\u2500\u2500 legal_10_lexam_scenario.py          \u2190 S8: Synthesize Results\n\u2502   \u251c\u2500\u2500 legal_10_citation_scenario.py       \u2190 S9: Citation Integrity\n\u2502   \u251c\u2500\u2500 legal_10_shield_scenario.py         \u2190 S10: Copyright Compliance\n\u2502   \u2514\u2500\u2500 legal_10_agentbench_scenario.py     \u2190 S1, S2: Research Planning/Stopping\n\u2502\n\u251c\u2500\u2500 run_specs/\n\u2502   \u2514\u2500\u2500 legal_10_run_specs.py               \u2190 All 10 @run_spec_function defs\n\u2502\n\u2514\u2500\u2500 metrics/\n    \u2514\u2500\u2500 legal_10_metrics.py                 \u2190 Custom metrics if needed\n\ndocs/\n\u251c\u2500\u2500 legal_10_overview.md                    \u2190 Benchmark introduction\n\u251c\u2500\u2500 legal_10_scenarios.md                   \u2190 Scenario details\n\u2514\u2500\u2500 legal_10_setup.md                       \u2190 Setup guide\n</code></pre>"},{"location":"legal10_mustreview/notes/#a-adding-custom-scenarios","title":"A. Adding Custom Scenarios","text":"<p>Pattern: Create Scenario subclass inheriting from <code>Scenario</code> ABC</p> <p>Location: Create new files in <code>/src/helm/benchmark/scenarios/</code></p> <p>Template:</p> <pre><code>from typing import List\nfrom helm.benchmark.scenarios.scenario import (\n    Scenario, Instance, Input, Output, Reference,\n    CORRECT_TAG, TEST_SPLIT, TRAIN_SPLIT\n)\nfrom helm.common.general import ensure_directory_exists\n\nclass Legal10CLERCScenario(Scenario):\n    \"\"\"\n    CLERC: Citation Resolution benchmark for Legal-10\n    Tests known authority retrieval (S3)\n    \"\"\"\n    name = \"legal_10_clerc\"\n    description = \"CLERC: Known Authority Retrieval\"\n    tags = [\"legal\", \"retrieval\", \"legal_10\"]\n\n    def get_instances(self, output_path: str) -&gt; List[Instance]:\n        # Load CLERC dataset\n        # Create Instance objects with Input and References\n        instances = []\n\n        # Example instance creation\n        instance = Instance(\n            input=Input(text=\"Find 42 U.S.C. \u00a7 1983\"),\n            references=[\n                Reference(Output(text=\"correct_citation\"), tags=[CORRECT_TAG])\n            ],\n            split=TEST_SPLIT,\n            id=\"clerc_001\"\n        )\n        instances.append(instance)\n\n        return instances\n</code></pre> <p>Extension Points: - Subclass <code>Scenario</code> - Implement <code>get_instances(output_path)</code> method - Set <code>name</code>, <code>description</code>, <code>tags</code> class variables - Optional: Override <code>get_metadata()</code> for custom display</p>"},{"location":"legal10_mustreview/notes/#b-adding-custom-run-specs","title":"B. Adding Custom Run Specs","text":"<p>Pattern: Create function decorated with <code>@run_spec_function(name)</code></p> <p>Location: Create new file <code>/src/helm/benchmark/run_specs/legal_10_run_specs.py</code></p> <p>Template:</p> <pre><code>from helm.benchmark.run_spec import RunSpec, run_spec_function\nfrom helm.benchmark.scenarios.scenario import ScenarioSpec\nfrom helm.benchmark.adaptation.adapter_spec import ADAPT_GENERATION\nfrom helm.benchmark.adaptation.common_adapter_specs import (\n    get_generation_adapter_spec,\n    get_multiple_choice_adapter_spec\n)\nfrom helm.benchmark.metrics.common_metric_specs import (\n    get_exact_match_metric_specs,\n    get_f1_metric_specs\n)\n\n@run_spec_function(\"legal_10_clerc\")\ndef get_legal_10_clerc_spec() -&gt; RunSpec:\n    \"\"\"S3: Known Authority - CLERC dataset\"\"\"\n    scenario_spec = ScenarioSpec(\n        class_name=\"helm.benchmark.scenarios.legal_10_clerc_scenario.Legal10CLERCScenario\",\n        args={}\n    )\n\n    adapter_spec = get_generation_adapter_spec(\n        instructions=\"Retrieve the legal authority for the given citation.\",\n        input_noun=\"Citation\",\n        output_noun=\"Authority\",\n        max_tokens=512\n    )\n\n    metric_specs = get_exact_match_metric_specs()\n\n    return RunSpec(\n        name=\"legal_10_clerc\",\n        scenario_spec=scenario_spec,\n        adapter_spec=adapter_spec,\n        metric_specs=metric_specs,\n        groups=[\"legal_10\", \"legal_10_rag\"]\n    )\n\n@run_spec_function(\"legal_10_casehold\")\ndef get_legal_10_casehold_spec() -&gt; RunSpec:\n    \"\"\"S7: Distinguish Cases - CaseHOLD dataset\"\"\"\n    scenario_spec = ScenarioSpec(\n        class_name=\"helm.benchmark.scenarios.casehold_scenario.CaseHOLDScenario\",\n        args={}\n    )\n\n    adapter_spec = get_multiple_choice_adapter_spec(\n        method=\"ADAPT_MULTIPLE_CHOICE_JOINT\",\n        instructions=\"Which holding is most relevant?\",\n        input_noun=\"Passage\",\n        output_noun=\"Answer\",\n        max_train_instances=2\n    )\n\n    metric_specs = get_exact_match_metric_specs()\n\n    return RunSpec(\n        name=\"legal_10_casehold\",\n        scenario_spec=scenario_spec,\n        adapter_spec=adapter_spec,\n        metric_specs=metric_specs,\n        groups=[\"legal_10\", \"legal_10_closed_book\"]\n    )\n</code></pre> <p>Discovery Mechanism: - Decorator <code>@run_spec_function(\"name\")</code> automatically registers the function - Dynamic discovery via <code>discover_run_spec_functions()</code> scans all <code>run_specs/</code> modules - Invoked via CLI: <code>helm-run --run-specs legal_10_clerc</code></p>"},{"location":"legal10_mustreview/notes/#c-adding-custom-metrics-optional","title":"C. Adding Custom Metrics (Optional)","text":"<p>Pattern: Subclass <code>Metric</code> and implement <code>evaluate()</code> method</p> <p>Location: Create <code>/src/helm/benchmark/metrics/legal_10_metrics.py</code></p> <p>Template:</p> <pre><code>from typing import List\nfrom helm.benchmark.metrics.metric import (\n    Metric, MetricSpec, MetricResult, PerInstanceStats, Stat\n)\nfrom helm.benchmark.metrics.metric_name import MetricName\nfrom helm.benchmark.adaptation.scenario_state import ScenarioState\nfrom helm.benchmark.metrics.metric_service import MetricService\n\nclass CitationValidationMetric(Metric):\n    \"\"\"Custom metric for citation integrity (S9)\"\"\"\n\n    def evaluate(\n        self,\n        scenario_state: ScenarioState,\n        metric_service: MetricService,\n        eval_cache_path: str,\n        parallelism: int\n    ) -&gt; MetricResult:\n        # Custom evaluation logic\n        stats = []\n        per_instance_stats = []\n\n        for request_state in scenario_state.request_states:\n            # Validate citation exists and is correctly formatted\n            citation_valid = self._validate_citation(request_state.result.completions[0].text)\n\n            per_instance_stats.append(PerInstanceStats(\n                instance_id=request_state.instance.id,\n                trial_index=0,\n                stats=[Stat(MetricName(\"citation_valid\")).add(1 if citation_valid else 0)]\n            ))\n\n        # Aggregate stats\n        total_valid = sum(1 for ps in per_instance_stats if ps.stats[0].sum == 1)\n        stats.append(Stat(MetricName(\"citation_accuracy\")).add(total_valid / len(per_instance_stats)))\n\n        return MetricResult(aggregated_stats=stats, per_instance_stats=per_instance_stats)\n\n    def _validate_citation(self, text: str) -&gt; bool:\n        # Implementation for citation validation\n        return True  # Placeholder\n\n# Helper function for run specs\ndef get_legal_10_citation_metric_specs() -&gt; List[MetricSpec]:\n    return [\n        MetricSpec(\n            class_name=\"helm.benchmark.metrics.legal_10_metrics.CitationValidationMetric\",\n            args={}\n        )\n    ]\n</code></pre> <p>When to Create Custom Metrics: - Only if existing metrics (exact_match, F1, ROUGE, etc.) don't suffice - For Legal-10: May need custom metrics for S9 (Citation Integrity) and S1/S2 (Research Planning/Stopping)</p>"},{"location":"legal10_mustreview/notes/#d-adapters-usually-not-needed","title":"D. Adapters (Usually NOT Needed)","text":"<p>Note: HELM provides standard adapters that handle most use cases: - <code>GenerationAdapter</code> - For open-ended generation (most Legal-10 tasks) - <code>MultipleChoiceJointAdapter</code> - For multiple choice (S7: CaseHOLD) - <code>ChatAdapter</code> - For chat-based models</p> <p>Only create custom adapters if you need specialized prompt formatting beyond what <code>adapter_spec.instructions</code> provides.</p>"},{"location":"legal10_mustreview/notes/#3-data-flow-architecture","title":"3. Data Flow Architecture","text":"<p>Complete flow from input to output (unchanged by Legal-10):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Input: RunSpec (name, scenario, adapter, metrics)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SCENARIO CREATION                                            \u2502\n\u2502 Runner.run_one() \u2192 create_scenario(scenario_spec)           \u2502\n\u2502 \u2192 Calls Scenario.get_instances(output_path)                \u2502\n\u2502 Returns: List[Instance]                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 DATA PREPROCESSING (Optional)                               \u2502\n\u2502 DataPreprocessor.preprocess(instances, parallelism)        \u2502\n\u2502 Applies data augmentations if specified                     \u2502\n\u2502 Returns: List[Instance]                                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ADAPTATION: Instance \u2192 Request                              \u2502\n\u2502 AdapterFactory.get_adapter(adapter_spec)                    \u2502\n\u2502 \u2192 Adapter.adapt(instances, parallelism)                     \u2502\n\u2502 Converts each Instance to RequestState(s)                   \u2502\n\u2502 Returns: ScenarioState (list of RequestState)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 EXECUTION: Request \u2192 Result                                 \u2502\n\u2502 Executor.execute(scenario_state)                            \u2502\n\u2502 - Parallel execution (parallelism parameter)                \u2502\n\u2502 - Makes API/local calls via client                          \u2502\n\u2502 - Populates RequestState.result                             \u2502\n\u2502 Returns: ScenarioState (with results filled in)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 METRICS EVALUATION                                          \u2502\n\u2502 For each MetricSpec:                                        \u2502\n\u2502   metric = create_metric(metric_spec)                       \u2502\n\u2502   metric.evaluate(scenario_state, metric_service, ...)     \u2502\n\u2502 Returns: MetricResult                                       \u2502\n\u2502 - aggregated_stats: List[Stat] (global metrics)             \u2502\n\u2502 - per_instance_stats: List[PerInstanceStats] (per item)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                       \u2502\n                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Output Files (written to benchmark_output/runs/):           \u2502\n\u2502 - run_spec.json (RunSpec as JSON)                           \u2502\n\u2502 - scenario.json (Scenario instances)                        \u2502\n\u2502 - scenario_state.json (RequestState list)                   \u2502\n\u2502 - stats.json (aggregated metrics)                           \u2502\n\u2502 - per_instance_stats.json (per-item metrics)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Data Structures: 1. Instance: Single input data point with references 2. RequestState: Instance + Request + Result + metadata 3. ScenarioState: Collection of RequestStates (immutable) 4. Stat: Single metric value with MetricName context 5. PerInstanceStats: Stats grouped by (instance_id, trial_index)</p>"},{"location":"legal10_mustreview/notes/#4-configuration-discovery-systems","title":"4. Configuration &amp; Discovery Systems","text":""},{"location":"legal10_mustreview/notes/#a-run-spec-discovery","title":"A. Run Spec Discovery","text":"<ul> <li>Location: All files in <code>/src/helm/benchmark/run_specs/</code></li> <li>Mechanism: Decorator <code>@run_spec_function(\"name\")</code> registers automatically</li> <li>Discovery: <code>discover_run_spec_functions()</code> uses <code>pkgutil.iter_modules()</code> to scan package</li> <li>Registry: <code>_REGISTERED_RUN_SPEC_FUNCTIONS</code> dict</li> <li>Access: <code>get_run_spec_function(name)</code> looks up by name</li> </ul> <p>For Legal-10: Create <code>legal_10_run_specs.py</code> in <code>run_specs/</code> directory</p>"},{"location":"legal10_mustreview/notes/#b-model-metadata-discovery","title":"B. Model Metadata Discovery","text":"<ul> <li>Built-in: <code>/src/helm/config/model_metadata.yaml</code></li> <li>Local Override: <code>$LOCAL_PATH/model_metadata.yaml</code></li> <li>Format: YAML with <code>models</code> list of <code>ModelMetadata</code> objects</li> <li>Registration: <code>register_model_metadata(metadata)</code> adds to <code>ALL_MODELS_METADATA</code></li> <li>Access: <code>get_model_metadata(name)</code> retrieves by name</li> </ul>"},{"location":"legal10_mustreview/notes/#c-model-deployment-discovery","title":"C. Model Deployment Discovery","text":"<ul> <li>Built-in: <code>/src/helm/config/model_deployments.yaml</code></li> <li>Local Override: <code>$LOCAL_PATH/model_deployments.yaml</code></li> <li>Format: YAML with <code>model_deployments</code> list + client_spec class_name</li> <li>Registration: <code>register_model_deployment(deployment)</code> adds to registry</li> <li>Access: <code>get_model_deployment(name)</code> retrieves by name</li> </ul>"},{"location":"legal10_mustreview/notes/#d-scenario-discovery","title":"D. Scenario Discovery","text":"<ul> <li>NO central registry - scenarios found via <code>ScenarioSpec.class_name</code></li> <li>Pattern: <code>class_name: \"helm.benchmark.scenarios.module.ClassName\"</code></li> <li>Loading: <code>create_scenario(scenario_spec)</code> \u2192 <code>create_object()</code> \u2192 import + instantiate</li> </ul>"},{"location":"legal10_mustreview/notes/#5-legal-10-specific-implementation-roadmap","title":"5. Legal-10 Specific Implementation Roadmap","text":""},{"location":"legal10_mustreview/notes/#legal-10-skills-mapping","title":"Legal-10 Skills Mapping","text":"Skill Name Dataset Modality Status S1 Research Planning LegalAgentBench AG To Implement S2 Strategic Stopping LegalAgentBench AG To Implement S3 Known Authority CLERC RAG To Implement S4 Unknown Authority CLERC (semantic) RAG To Implement S5 Validate Authority KeyCite-CLERC RAG To Implement S6 Fact Extraction CUAD RAG To Implement S7 Distinguish Cases CaseHOLD CB \u2705 Exists S8 Synthesize Results LEXam CB To Implement S9 Citation Integrity Dahl 10-types CB To Implement S10 Copyright Compliance SHIELD CB To Implement"},{"location":"legal10_mustreview/notes/#implementation-checklist","title":"Implementation Checklist","text":"<p>Phase 1: Setup - [ ] Create <code>/src/helm/benchmark/scenarios/legal_10/</code> subdirectory (optional) - [ ] Create <code>/src/helm/benchmark/run_specs/legal_10_run_specs.py</code> - [ ] Create <code>/docs/legal_10_overview.md</code> - [ ] Update <code>/mkdocs.yml</code> navigation</p> <p>Phase 2: Closed-Book Scenarios (S7-S10) - [ ] S7: Verify existing <code>casehold_scenario.py</code> works - [ ] S8: Create <code>legal_10_lexam_scenario.py</code> - [ ] S9: Create <code>legal_10_citation_scenario.py</code> - [ ] S10: Create <code>legal_10_shield_scenario.py</code></p> <p>Phase 3: RAG Scenarios (S3-S6) - [ ] S3, S4: Create <code>legal_10_clerc_scenario.py</code> - [ ] S5: Create <code>legal_10_keycite_scenario.py</code> - [ ] S6: Create <code>legal_10_cuad_scenario.py</code></p> <p>Phase 4: Agentic Scenarios (S1-S2) - [ ] S1, S2: Create <code>legal_10_agentbench_scenario.py</code> - [ ] Consider custom metrics for research planning quality</p> <p>Phase 5: Integration - [ ] Create all <code>@run_spec_function</code> entries in <code>legal_10_run_specs.py</code> - [ ] Add Legal-10 group to <code>/src/helm/benchmark/static/schema_legal.yaml</code> - [ ] Create run entries in <code>/src/helm/benchmark/presentation/run_entries_legal.conf</code> - [ ] Write unit tests for each scenario</p> <p>Phase 6: Documentation - [ ] Complete <code>/docs/legal_10_setup.md</code> - [ ] Complete <code>/docs/legal_10_scenarios.md</code> - [ ] Update skill pages with implementation details</p>"},{"location":"legal10_mustreview/notes/#6-do-not-touch-vs-safe-to-extend-summary","title":"6. DO NOT TOUCH vs SAFE TO EXTEND Summary","text":""},{"location":"legal10_mustreview/notes/#absolutely-do-not-modify","title":"\u274c ABSOLUTELY DO NOT MODIFY","text":"<ol> <li>Frozen dataclasses: <code>RunSpec</code>, <code>AdapterSpec</code>, <code>Instance</code>, <code>Reference</code>, <code>Output</code>, <code>Input</code>, <code>RequestState</code>, <code>ScenarioState</code></li> <li>Abstract base classes: <code>Scenario.get_instances()</code>, <code>Adapter.adapt()</code>, <code>Metric.evaluate()</code></li> <li>Core execution: <code>Runner.run_one()</code>, <code>Executor.execute()</code></li> <li>Registries and discovery: <code>@run_spec_function</code> decorator, <code>discover_run_spec_functions()</code>, config registry</li> <li>Factory patterns: <code>AdapterFactory.get_adapter()</code>, <code>create_object()</code>, <code>create_scenario()</code></li> <li>Core routing: Model metadata/deployment registries</li> </ol>"},{"location":"legal10_mustreview/notes/#safe-to-extend-create-new-dont-modify-existing","title":"\u2705 SAFE TO EXTEND (Create new, don't modify existing)","text":"<ol> <li>Scenarios: Create new <code>*_scenario.py</code> files, implement <code>Scenario</code> subclass</li> <li>Run Specs: Create new <code>*_run_specs.py</code> files, use <code>@run_spec_function()</code> decorator</li> <li>Metrics: Create new <code>*_metrics.py</code> files, subclass <code>Metric</code>, provide <code>MetricSpec</code> helper</li> <li>Configuration: Add local YAML files, don't modify built-in config YAML</li> <li>Documentation: Add new <code>.md</code> files, update navigation in <code>mkdocs.yml</code></li> </ol>"},{"location":"legal10_mustreview/notes/#7-testing-your-integration","title":"7. Testing Your Integration","text":""},{"location":"legal10_mustreview/notes/#verify-run-spec-discovery","title":"Verify Run Spec Discovery","text":"<pre><code># List all available run specs (should include legal_10_*)\nhelm-run --help\n\n# Describe a specific Legal-10 run spec\nhelm-run --run-specs legal_10_clerc --describe\n</code></pre>"},{"location":"legal10_mustreview/notes/#run-a-single-scenario","title":"Run a Single Scenario","text":"<pre><code># Run Legal-10 CaseHOLD scenario\nhelm-run \\\n  --run-specs legal_10_casehold \\\n  --suite legal_10 \\\n  --max-eval-instances 10 \\\n  --output-path benchmark_output/test_run\n</code></pre>"},{"location":"legal10_mustreview/notes/#run-all-legal-10-scenarios","title":"Run All Legal-10 Scenarios","text":"<pre><code># Run complete Legal-10 suite\nhelm-run \\\n  --run-specs legal_10_clerc,legal_10_casehold,legal_10_lexam \\\n  --suite legal_10 \\\n  --models-to-run anthropic/claude-3-sonnet-20240229 \\\n  --output-path benchmark_output/legal_10_full\n</code></pre>"},{"location":"legal10_mustreview/notes/#8-integration-best-practices","title":"8. Integration Best Practices","text":""},{"location":"legal10_mustreview/notes/#naming-conventions","title":"Naming Conventions","text":"Component Convention Example Scenario file <code>legal_10_&lt;dataset&gt;_scenario.py</code> <code>legal_10_clerc_scenario.py</code> Scenario class <code>Legal10&lt;Name&gt;Scenario</code> <code>Legal10CLERCScenario</code> Run spec function <code>get_legal_10_&lt;name&gt;_spec()</code> <code>get_legal_10_clerc_spec()</code> Run spec name <code>legal_10_&lt;name&gt;</code> <code>\"legal_10_clerc\"</code> Groups <code>[\"legal_10\", \"legal_10_&lt;modality&gt;\"]</code> <code>[\"legal_10\", \"legal_10_rag\"]</code> Tags Lowercase, hyphenated <code>[\"legal\", \"retrieval\"]</code>"},{"location":"legal10_mustreview/notes/#import-patterns","title":"Import Patterns","text":"<p>Standard imports for scenarios: <pre><code>from typing import List\nfrom helm.benchmark.scenarios.scenario import (\n    Scenario, Instance, Input, Output, Reference,\n    TRAIN_SPLIT, TEST_SPLIT, VALID_SPLIT, CORRECT_TAG\n)\nfrom helm.common.general import ensure_directory_exists\n</code></pre></p> <p>Standard imports for run specs: <pre><code>from helm.benchmark.run_spec import RunSpec, run_spec_function\nfrom helm.benchmark.scenarios.scenario import ScenarioSpec\nfrom helm.benchmark.adaptation.common_adapter_specs import (\n    get_generation_adapter_spec,\n    get_multiple_choice_adapter_spec\n)\nfrom helm.benchmark.metrics.common_metric_specs import (\n    get_exact_match_metric_specs,\n    get_f1_metric_specs\n)\n</code></pre></p>"},{"location":"legal10_mustreview/notes/#error-handling","title":"Error Handling","text":"<ul> <li>Scenarios should raise <code>ValueError</code> for invalid configuration</li> <li>Use <code>hlog()</code> from <code>helm.common.hierarchical_logger</code> for logging</li> <li>Let exceptions propagate - HELM's runner handles them gracefully</li> </ul>"},{"location":"legal10_mustreview/notes/#performance","title":"Performance","text":"<ul> <li>Use <code>ensure_directory_exists()</code> before file operations</li> <li>Cache downloaded datasets in <code>output_path</code></li> <li>Use <code>parallelism</code> parameter for concurrent processing</li> <li>Avoid loading entire datasets into memory if possible</li> </ul>"},{"location":"legal10_mustreview/notes/#conclusion","title":"Conclusion","text":"<p>HELM's architecture provides clear separation between: - Framework code (do not touch) - Extension points (safe to add Legal-10 content)</p> <p>By following these guidelines, Legal-10 integrates seamlessly without modifying any core HELM functionality. All Legal-10 components are:</p> <ul> <li>Modular: Each skill is an independent scenario</li> <li>Discoverable: Run specs auto-register via decorator</li> <li>Maintainable: Clear separation from core framework</li> <li>Upgradeable: HELM updates won't break Legal-10 code</li> </ul> <p>The key is to extend, never modify - inherit from base classes, use decorators, and create new files rather than changing existing ones.</p>"}]}